{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "metadata": {
    "g.cell_uuid": "70fc5bb1-7915-47db-83a4-910bc3087d00",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "# Homework 4 \u2014 A slightly different introduction to Deep Reinforcement Learning\n",
    "\n",
    "In this homework we will tackle an entirely new problem that will allow us to show off some reinforcement learning -- a setting where our actions *alternate* with responses from the environment, and the two influence each other."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "b84c196b-2e5d-48e5-bce4-ba2c60075c0a",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import urllib\n",
    "import urllib.request\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import torch\n",
    "\n",
    "from seq2class_previous_homeworks import StatefulTaskSetting, IncrementalScoringModel, BeamDecisionAgent, draw_tree"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "9f95bd63-c315-4976-a787-02a84cb282cb",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "## The setting: predictive keyboards\n",
    "\n",
    "You are likely familiar with modern predictive keyboards from your smartphone.  Given the text you've typed so far, the keyboard proposes a next word; the user can accept or ignore this proposal. You should have seen this as a contemporary application of *language models* in NLP in Fall, and indeed you already built an FST-powered completion and prediction tool in that class.\n",
    "\n",
    "To keep things simple, we will assume that we already have some language model over *characters*.  This gives us a conveniently small vocabulary.  It also allows us to ignore word boundaries: we will treat the space symbol as just another character. However, as we are not paying attention to words, we will no longer be proposing the next *word*.  Proposing just the single next *character* would probably not speed up typing enough, so we will propose a sequence of *several characters*.  The user can decide whether to accept this proposed substring or ignore it.  The major question will be *how many* characters to propose at once."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "4ed6ef99-9f64-469b-916e-204bc0f4627d",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## Warm-Up: predicting strings from a language model trie\n",
    "\n",
    "Here is an example: assume the user typed \"`the defor`,\" what characters should we follow up with? Since we assume we already have a language model, let's assume this is what the trie that starts here looks like (omitting all the low-probability options):"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "53f31f98-ab94-4286-a62d-3daf855ab637",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "display(HTML(draw_tree({'e/0.4': {'s/1.0': {'t/0.7': {'a/1.0': {}}}}, 'm/0.6': {'a/0.5': {'t/1.0': {'i/1.0': {}}}, 'e/0.5': {'d/1.0': {'\u2423/1.0': {}}}}})))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "e0ba2c52-8f6b-4961-bfd7-173ca83887cf",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "So the user might have wanted to type `deformed`, `deformation`, or `deforestation`. Given the very simple probabilities on the trie above, we can see what the most probable choice is if we want to propose a substring of 1, 2, 3, or 4 next characters:\n",
    "\n",
    "length | prediction\n",
    "---|---\n",
    "1 | `m`\n",
    "2 | `es`\n",
    "3 | `est`\n",
    "4 | `med_` / `mati`  (tie)\n",
    "\n",
    "We see that the \"best\" answer in general cannot be obtained by greedily choosing the best arc at each trie vertex (this should not be news to you), but will require some *planning ahead* -- depending on how many characters we want to propose. \n",
    "\n",
    "Why is this important? If the user can only accept or ignore the entire proposed substring , how long a substring should we propose? Proposing a longer substring increases our *reward* (i.e., time saved over manual typing) if the user accepts -- but it also increases the chance that the proposal is wrong, in which case the user will be forced to ignore it and type the next character themselves.\n",
    "That is the tradeoff that we will have to navigate in this assignment.\n",
    "For now, we will assume that we do indeed have access to the full trie of the language model including all probabilities -- but at the end of the assignment we will see what we can still do even if we don't have this option to plan ahead.\n",
    "\n",
    "We will give you a very simple character-level English LM over the 27 characters `a`-`z` and `\u2423` (the space character).  For simplicity, we do not include an EOS symbol.  Much of the code should be familiar to you from HW3.  Rather than make you run the training code (which takes about 20 minutes per epoch on 100MB of text, using a GPU), we will hand you the weights from our own training run.  \n",
    "\n",
    "**Notes on the English language model:**\n",
    "\n",
    "Our trained model achieves about 1.5 bits per character of cross-entropy (vs. 1.08 bits for the state-of-the-art Transformer-XL model).  Our model is weak because it uses relatively few parameters, and we only trained for 1 epoch, without any kind of regularization.\n",
    "\n",
    "You'll see that the training procedure is a little different from last homework's models: we group the training examples into batches, which allows us to parallelize SGD and improves its stability.  \n",
    "\n",
    "As in HW3, we made the model more transparent by avoiding PyTorch's `LSTM` class (which trains an entire batch of sequences on the GPU) in favor of the `LSTMCell` class (which trains only a single time step for all the sequences).  This slows down training because we have to call the GPU repeatedly, once per time step, but it makes it somewhat more convenient to examine the trie probabilities via the `LSTMCell` class as we explore reinforcement learning.  Don't do it this way in a real-world setting where speed is important.\n"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "2825f01c-cf05-461c-a938-8f5dfb5ae3c7",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, layers=3, hidden_size=512, embedding_size=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # a simple character integerizer\n",
    "        self.idx2char = vocab\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        \n",
    "        # character embedding module\n",
    "        self.embedding = torch.nn.Embedding(len(vocab), embedding_size)\n",
    "        \n",
    "        # LSTM cells to run over character embeddings\n",
    "        # Note that it may be easier to just use an torch.nn.LSTM instead of\n",
    "        # manually gluing together cells like this.\n",
    "        self.lstm_layers = [torch.nn.LSTMCell(embedding_size, hidden_size)] \\\n",
    "            + [torch.nn.LSTMCell(hidden_size, hidden_size) for _ in range(layers - 2)] \\\n",
    "            + [torch.nn.LSTMCell(hidden_size, embedding_size)]\n",
    "        for i, layer in enumerate(self.lstm_layers):\n",
    "            self.add_module(f\"lstmlayer{i}\", layer)\n",
    "\n",
    "    def _hcs_from_cidx(self, hcs, c_idx):\n",
    "        c_emb = self.embedding(c_idx)\n",
    "        nhcs = [(c_emb, None)]\n",
    "        for hc, layer in zip(hcs, self.lstm_layers):\n",
    "            nhcs.append(layer(nhcs[-1][0], hc))\n",
    "        return nhcs[1:]\n",
    "        \n",
    "    def hcs_from_context(self, context_string, hcs=None):\n",
    "        \"\"\"\n",
    "        Sets up the language model hidden state given a string prefix\n",
    "        and, optionally, the hidden state before reading in this prefix.\n",
    "        \n",
    "        If context_string is a list, this performs batched computations,\n",
    "        interpreting context_string batch-first, timestep-second.\n",
    "        \"\"\"\n",
    "        if isinstance(context_string, str):\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            # \"Transpose\" to get it timestep-first, batch-second\n",
    "            batchsize = len(context_string)\n",
    "            context_string = [tuple(cs) for cs in itertools.zip_longest(*context_string)]\n",
    "        if hcs is None:\n",
    "            # initalize the hidden state of the LSTM\n",
    "            cs = [torch.zeros(batchsize, layer.hidden_size, device = self.embedding.weight.device) for layer in self.lstm_layers]\n",
    "            hcs = [(torch.tanh(c), c) for c in cs]\n",
    "        # iterate over the string\n",
    "        for c in context_string:\n",
    "            c_idxs = torch.tensor([self.char2idx[c] for c in tuple(c)], device = self.embedding.weight.device)\n",
    "            hcs = self._hcs_from_cidx(hcs, c_idxs)\n",
    "        return hcs\n",
    "\n",
    "    def next_options(self, hcs = None, logprobs = False):\n",
    "        \"\"\"\n",
    "        Given the hidden state tuple of the LM, it returns a dictionary\n",
    "        mapping potential next characters to their probabilities.\n",
    "        \"\"\"\n",
    "        if hcs is None:\n",
    "            hcs = self.hcs_from_context(\"\")\n",
    "        probs = (hcs[-1][0] @ self.embedding.weight.t()).log_softmax(dim=-1)\n",
    "        if not logprobs:\n",
    "            probs = probs.exp()\n",
    "        l = [{c: p.item() for c, p in zip(self.idx2char, probs[b])}\n",
    "             for b in range(probs.size(0))]\n",
    "        # If no batching is used (i.e., batchsize == 1), return simple dict.\n",
    "        if len(l) == 1:\n",
    "            return l[0]\n",
    "        else:\n",
    "            return l\n",
    "    \n",
    "    def greedy_1_best(self, hcs = None):\n",
    "        \"\"\"\n",
    "        Infinite iterator returning the greedy choices of next character.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            hcs = self.hcs_from_context(\"\", hcs)  # initialize if needed\n",
    "            while True:\n",
    "                idx = torch.argmax(hcs[-1][0] @ self.embedding.weight.t())\n",
    "                yield self.idx2char[idx]\n",
    "                hcs = self._hcs_from_cidx(hcs, torch.tensor([idx], device = hcs[-1][0].device))\n",
    "\n",
    "    def greedy_sample(self, temperature = 0.5, hcs = None):\n",
    "        \"\"\"\n",
    "        Infinite iterator returning local samples of next character.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            hcs = self.hcs_from_context(\"\", hcs)  # initialize if needed\n",
    "            while True:\n",
    "                weights = (hcs[-1][0] @ self.embedding.weight.t() / temperature).exp().squeeze(0)\n",
    "                [char] = random.choices(self.idx2char, weights = weights)\n",
    "                yield char\n",
    "                hcs = self._hcs_from_cidx(hcs, torch.tensor([self.char2idx[char]], device = hcs[-1][0].device))\n",
    "    \n",
    "    def render_trie_from(self, hcs, depth=2, topk=-1, prob_gt=-1):\n",
    "        assert topk < 0 or prob_gt < 0\n",
    "        def trie_from(hcs, depth):\n",
    "            if depth == 0:\n",
    "                return {}\n",
    "            else:\n",
    "                expanded_cs = list(self.next_options(hcs).items())\n",
    "                if topk > 0:\n",
    "                    expanded_cs = sorted(expanded_cs, key=lambda x:-x[1])[:topk]\n",
    "                if prob_gt > 0:\n",
    "                    expanded_cs = [(c, p) for c, p in expanded_cs if p > prob_gt]\n",
    "                return {f\"{c}/{p:.2f}\":\n",
    "                        trie_from(self.hcs_from_context(c, hcs=hcs), depth - 1)\n",
    "                        for c, p in sorted(expanded_cs, key=lambda x: x[0])}\n",
    "        display(HTML(draw_tree(trie_from(hcs, depth))))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b7b4e478-4a59-46e8-9de0-e6edb58626f5",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# This will train the language model (don't execute this, it would take too long!):\n",
    "\n",
    "def train_lm(name, dataset, nlayers, nhid, embsize, BATCHSIZE, BPTTLENGTH):\n",
    "    STUB = f\"{name}_layers{nlayers}_hs{nhid}_emb{embsize}_bs{BATCHSIZE}_bptt{BPTTLENGTH}_adam1e-3_epochs1\"\n",
    "    start_time = time.time()\n",
    "    with open(STUB+\".log\", 'wt') as logfile:\n",
    "        print(STUB)\n",
    "        print(STUB, file=logfile)\n",
    "        vocab = \"\".join(sorted(list(set(dataset))))\n",
    "        LM = LanguageModel(vocab, layers=nlayers, hidden_size=nhid, embedding_size=embsize).cuda()\n",
    "\n",
    "        total_length = len(dataset) // BATCHSIZE\n",
    "        batches = [\n",
    "            [\n",
    "                dataset[total_length * i + BPTTLENGTH * batch_nr\n",
    "                      : total_length * i + BPTTLENGTH * (batch_nr + 1)]\n",
    "                for i in range(BATCHSIZE)\n",
    "            ]\n",
    "            for batch_nr in range(total_length // BPTTLENGTH)\n",
    "        ]\n",
    "\n",
    "        hcs = LM.hcs_from_context([\"\"] * BATCHSIZE)\n",
    "        optimizer = torch.optim.Adam(LM.parameters(), lr=1e-3)\n",
    "\n",
    "        nll_sum = 0\n",
    "        for i_batch, batch in enumerate(batches):\n",
    "            hcs = [(h.detach(), c.detach()) for (h, c) in hcs]\n",
    "            nll = 0\n",
    "            for i in range(BPTTLENGTH):\n",
    "                # These chars at this timestep\n",
    "                charidxs = torch.tensor([LM.char2idx[batch[r][i]] for r in range(BATCHSIZE)], device = LM.embedding.weight.device)\n",
    "                # Predict\n",
    "                lps = (hcs[-1][0] @ LM.embedding.weight.t()).log_softmax(dim=-1)\n",
    "                nll += -lps.gather(dim=-1, index=charidxs.unsqueeze(-1)).sum()\n",
    "                # Next timestep\n",
    "                hcs = LM._hcs_from_cidx(hcs, charidxs)\n",
    "            # Calculate gradients\n",
    "            optimizer.zero_grad()\n",
    "            nll.backward()\n",
    "            # Debug output\n",
    "            nll_sum += nll.detach().item()\n",
    "            if i_batch % 50 == 0:\n",
    "                bpc = (nll_sum / ((50 if i_batch > 0 else 1) * BATCHSIZE * BPTTLENGTH)) / math.log(2)\n",
    "                nll_sum = 0\n",
    "                greedy = ''.join(itertools.islice(LM.greedy_1_best(LM.hcs_from_context(\"\")), 70))\n",
    "                sample = ''.join(itertools.islice(LM.greedy_sample(LM.hcs_from_context(\"\")), 70))\n",
    "                print(f\"{i_batch:5}/{len(batches)} {bpc:9.2f} -> {greedy} ~~ {sample}\")\n",
    "                print(f\"{i_batch:5}/{len(batches)} {bpc:9.2f} -> {greedy} ~~ {sample}\", file=logfile)\n",
    "            # Apply gradients\n",
    "            optimizer.step()\n",
    "\n",
    "        torch.save(LM.state_dict(), STUB+\".statedict.pt\")\n",
    "        print(\"Trained in\", time.time() - start_time, \"seconds.\")\n",
    "        print(\"Trained in\", time.time() - start_time, \"seconds.\", file=logfile)\n",
    "        start_time = time.time()\n",
    "        _ = ''.join(itertools.islice(LanguageModel.greedy_sample(LM), 1000))\n",
    "        print(\"Sampled 1000 chars in\", time.time() - start_time, \"seconds.\")\n",
    "        print(\"Sampled 1000 chars in\", time.time() - start_time, \"seconds.\", file=logfile)\n",
    "\n",
    "should_we_retrain_the_language_model = \"no\"\n",
    "\n",
    "if should_we_retrain_the_language_model == \"hell yeah\":\n",
    "    if not os.path.isfile(\"text8.zip\"):\n",
    "        import urllib\n",
    "        urllib.request.urlretrieve(\"http://mattmahoney.net/dc/text8.zip\", \"text8.zip\")\n",
    "    if not os.path.isfile(\"text8\"):\n",
    "        import zipfile\n",
    "        zip_ref = zipfile.ZipFile(\"text8.zip\", 'r')\n",
    "        zip_ref.extractall(\".\")\n",
    "        zip_ref.close()\n",
    "\n",
    "    with open(\"text8\", 'r') as f:\n",
    "        text8 = f.read()\n",
    "    \n",
    "    train_lm(\"text8\", text8, nlayers = 2, nhid = 512, embsize = 16, BATCHSIZE = 80, BPTTLENGTH = 150)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "9ff22891-8e0f-42aa-a5f1-68a88918b345",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Just to illustrate how this class is supposed to work, let's run it on our example above:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "a85c8e8c-fd3c-42eb-9536-b02a4b1694be",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Load our pretrained models\n",
    "LM = LanguageModel(\"abcdefghijklmnopqrstuvwxzy\u2423\", layers=3, hidden_size=512, embedding_size=16).cpu()\n",
    "LM.load_state_dict(torch.load(\"text8.lm.statedict.pt\", map_location='cpu'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "68d090e8-ad6b-4f07-8450-dd5e010654f8",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# A sample from the LM (at local temperature T=0.5)\n",
    "print(''.join(itertools.islice(LM.greedy_sample(temperature=0.5), 100)))\n",
    "\n",
    "# Using no batches\n",
    "LM.next_options(LM.hcs_from_context(\"the\u2423defor\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "dafd8252-fc2d-4ae4-a2eb-311b7996091c",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Let's see what the model's trie looks like for our example context given above! We'll render it only to depth 4 and omit all expansions with probability $p(x_{i+1}|\\mathbf{x}_{<i}) \\le .18$ to keep things readable:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "07a55621-a878-4052-9faa-10de176a6497",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "the_defor = LM.hcs_from_context(\"the\u2423defor\")\n",
    "LM.render_trie_from(hcs=the_defor, depth=4, prob_gt=.18)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7ee10393-10fb-4877-b8ce-9434c35e04b5",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Notice that even with the threshold, this LSTM allows some words like \"defores\" (not actually English) and \"deformer\" that we didn't anticipate.  Now, given this trie, let's try to reproduce a table like the one given above that shows the *single most likely prediction* for each length:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "c655fef1-8043-4e92-8f94-50fd56b20fbe",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def brute_force_get_best_string(lm, hcs, length):\n",
    "    \"\"\"\n",
    "    Returns the string and its log-probability.\n",
    "    You can solve this by recursion using `lm.next_options(..., logprobs=True)`.\n",
    "    \"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "d0b02a86-f6b5-4b90-9a38-2d8f37e94fb1",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "for l in range(1, 3):\n",
    "    s, lp = brute_force_get_best_string(LM, the_defor, l)\n",
    "    print(f\"{l:2}: {s:5} (p={math.exp(lp)})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "a6ed4218-32af-423c-9dd4-5a7284756de9",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "You can try counting up to lengths larger than 2, but you should quickly see that even a length-3 string takes too long to brute-force maximize. But you have already learned a technique to perform (approximate) maximization in these locally normalized models: beam search!\n",
    "Let's try to apply that here!\n",
    "\n",
    "First we need to recast our problem of finding the best string in the framework of `TaskSetting`s and `ProbabilityModel`s, specifically, the `StatefulTaskSetting` and `IncrementalScoringModel`, as we did in the last homework:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "bee66a64-c0a0-4109-8758-324bc5bd4f6c",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class PredictStringTask(StatefulTaskSetting):\n",
    "    \"\"\"\n",
    "    The task predicts strings of a certain length.\n",
    "    The internal \"state\" will only be the length of the string that is\n",
    "    still to be generated (because this is sufficient for the structural zeros).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, length):\n",
    "        super().__init__()\n",
    "        self.vocab = tuple(vocab)\n",
    "        self.length = length\n",
    "    \n",
    "    def initial_taskstate(self, *, xx):\n",
    "        return self.length\n",
    "    \n",
    "    def next_taskstate(self, *, xx, a, taskstate):\n",
    "        return taskstate - 1\n",
    "    \n",
    "    def iterate_y(self, *, xx, oo=None, yy_prefix):\n",
    "        assert oo is None # let's not deal with that here\n",
    "        if yy_prefix > 0:\n",
    "            yield from self.vocab\n",
    "        else:\n",
    "            yield None\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b1a2c059-0fd0-4b44-88a6-4b5d64e4b5e6",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# It yields all sequences, as expected:\n",
    "list(PredictStringTask(\"XY\", 2).iterate_aa(xx=None))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "5c639f63-42dc-4e29-a6dd-e6a600b3b2ef",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class LMScorer(IncrementalScoringModel, torch.nn.Module):\n",
    "\n",
    "    def __init__(self, task, lm):\n",
    "        # Always initialize the PyTorch module first, so the registration hooks work!\n",
    "        torch.nn.Module.__init__(self)\n",
    "        super().__init__(task)\n",
    "        self.lm = lm\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"\n",
    "        Since all the magic happens in `self.lm`, the `LanguageModel`, nothing here.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def initial_modelstate(self, *, xx):\n",
    "        \"\"\"\n",
    "        Initialize the LM hidden states with `xx`, our preceding context.\n",
    "        We will also store the current predictions in the model state to share\n",
    "        them across different to-be-probed actions.\n",
    "        \"\"\"\n",
    "        hcs = self.lm.hcs_from_context(xx)\n",
    "        preds = self.lm.next_options(hcs, logprobs=True)\n",
    "        return (hcs, preds)\n",
    "    \n",
    "    def score_a_s(self, *, xx, a, taskstate, modelstate):\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "9f9ba17a-d487-4c84-9f58-9ad0712e01f6",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Let's see what predictions we would make for lengths 1, 2, 3, ..., 10 (and whether they're consistent with the predictions we made earlier):"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "26dd0089-9572-4e47-8a79-1f4e283a47ee",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def best_string(self, *, prefix, length, beam_size=5):\n",
    "    scorer = LMScorer(PredictStringTask(self.idx2char, length), self)\n",
    "    agent = BeamDecisionAgent(scorer, beam_size=beam_size)\n",
    "    return ''.join(agent.decision(xx=prefix))\n",
    "\n",
    "# Patch it into the language model\n",
    "LanguageModel.best_string = best_string\n",
    "\n",
    "print(\"Greedy search on our old example\")\n",
    "for l in range(1, 10):\n",
    "    s = LM.best_string(prefix=\"this\u2423is\", length=l, beam_size=1)\n",
    "    print(f\"{l:2}: {s:5}\")\n",
    "print(\"Greedy search on a new example\")\n",
    "for l in range(1, 10):\n",
    "    s = LM.best_string(prefix=\"this\u2423is\", length=l, beam_size=1)\n",
    "    print(f\"{l:2}: {s:5}\")\n",
    "print(\"Beam search (beam size 10) on the same example\")\n",
    "for l in range(1, 10):\n",
    "    s = LM.best_string(prefix=\"this\u2423is\", length=l, beam_size=10)\n",
    "    print(f\"{l:2}: {s:5}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "5879d5f4-15aa-4f8c-9ccb-88cc8227f29c",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## The task\n",
    "\n",
    "Now let's try to formalize the overarching task.  Suppose our user is partway through entering the sentence $\\mathbf{w} = $`sequence\u2423modeling\u2423is\u2423the\u2423best`.   A dialogue might go like this:\n",
    "\n",
    "observed state | possible actions | agent chooses | environment responds | reward (keystrokes saved)\n",
    "-|-|-|-|-\n",
    "`sequence\u2423modeling\u2423is\u2423` (8 left) | `a` , `th` , `the` , `the\u2423` , `the\u2423s` , `the\u2423co` , `the\u2423mos` , `the\u2423most` | `the\u2423` | accept | $4-1 = 3$\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423` (4 left) | `s` , `co` , `fir` , `most` | `co` | `b` | $1-1 = 0$\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423b` (3 left) | `e` , `es` , `est` | `est`| accept | $3-1 = 2$\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423best` (0 left) | $\\Rightarrow$ This is a final state, we collect the entire reward: | &nbsp; | &nbsp; | $\\sum = 5$\n",
    "\n",
    "In principle, an agent for this task could propose any string to the user.  However, as the table above shows, our agent will be restricted to just a few possible actions: it can propose a string of length 1, 2, 3, 4, 5, ... characters, but can only propose the most probable string of that length, according to its language model.  Furthermore, we assume that we somehow know how many characters the user wants to type altogether, so if the user has typed 21 out of 25 characters, we won't propose more than 4 more characters.  (You can try thinking of a more principled solution to that issue!)\n"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "cc51ccef-b9c4-4f11-99ed-8606676b57c9",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "## The actual environment (the typist)\n",
    "\n",
    "Let's try to build an implementation for this scenario now. We will start with:\n",
    "1. A definition of the environment, or more specifically, the environment in a specific state. It will, given this state, look at the action that the agent chose, and return the reward that the agent obtains for this action, as well as a response (more on that later). This execution causes the `EnvironmentState` to change.\n",
    "2. An `RLAgent`, that takes such responses and decides on new actions."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "04ea2287-7263-4c04-8a88-299110ca82d6",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class EnvironmentState(object):\n",
    "    \"\"\"\n",
    "    Representing a specific state our environment is in for the current episode.\n",
    "    Note that we explicitly make this object stateful to highlight that we\n",
    "    cannot just take any action free of consequences.\n",
    "    \"\"\"\n",
    "        \n",
    "    def execute_action(self, *, action):\n",
    "        \"\"\"\n",
    "        Returns the reward for an action and a response that the agent can use\n",
    "        to update its belief state.\n",
    "        Both need not be deterministic from the arguments.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def evaluate_agent(self, *, agent=None, agentclass=None):\n",
    "        \"\"\"\n",
    "        Convenience method, executing an entire episode by running the agent.\n",
    "        If passed an `agent`, it will just use it, if passed an `agentclass`, it\n",
    "        will try to instantiate that class in some way and then use it.\n",
    "        Returns the total reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "0892ddff-a1e0-43ef-a4a8-644dfaeb0827",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class RLAgent(object):\n",
    "    \"\"\"\n",
    "    Note that this RLAgent is similar to the DecisionAgent, but has a slightly\n",
    "    different (i.e., much more stateful) way of working, again, to visualize the\n",
    "    commitment to actions in the RL setting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def decision(self):\n",
    "        \"\"\"\n",
    "        Makes a decision, based on its internal state -- which is very much\n",
    "        separate from the state of the environment that it is placed in.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def receive_response(self, *, reward, response):\n",
    "        \"\"\"\n",
    "        Updates the agent's internal state using the response it received from\n",
    "        the environment (a response to the agent's previous `decision()`), and,\n",
    "        if desired, the immediate reward (we will not use that until the very\n",
    "        end of this notebook though).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "a25c7e85-7166-40da-8aa9-197a500dfc21",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "In general, the agent's actions can influence the environment's responses.  That is certainly true here, since if the agent proposes a probability-$p$ sequence of 3 characters, the environment will accept that proposal with probability $p$ (if the language model is correct!), and the environment's state will change by appending those 3 characters to the string typed so far.\n",
    "\n",
    "The specific environment in our case makes many of its choices early on, before the agent has acted, so these choices are not conditioned on the agent's actions.  In particular, our environment is an idealized user who starts out by sampling a *particular* string $\\mathbf{w}$ that they want to type.  This string does not change as the agent acts.  The specific accept/reject decisions by the environment do depend on the agent's actions, but in a way that is completely determined by those actions and the previously chosen string $\\mathbf{w}$.\n",
    "\n",
    "As a result, the environment behaves in a rather orderly way, with some probabilities being coupled.  For example,\n",
    "$$p(\\text{accept} \\mid \\text{state}=\\texttt{is\u2423}, \\text{action}=\\texttt{the}) \\\\\n",
    "= p(\\text{accept} \\mid \\text{state}=\\texttt{is\u2423}, \\text{action}=\\texttt{th})\n",
    "\\cdot p(\\text{accept} \\mid \\text{state}=\\texttt{is\u2423th}, \\text{action}=\\texttt{e})$$\n",
    "where all these probabilities are given by the user's true language model (not necessarily identical to the language model assumed by the agent).\n",
    "\n",
    "In short, $\\mathbf{w}$ is part of the environment's state.  The environment then responds *deterministically* given its state and the user's action.\n",
    "\n",
    "In terms of modeling, this means that the setting is really that of a POMDP instead of an MDP: the agent does not fully observe the environment's state.  (It does not observe $\\mathbf{w}$, only the prefix that has been typed so far.)"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "51592907-2a62-47b8-9922-ba961f1ec894",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## The agent (the predictive text system)\n",
    "\n",
    "Even though the true setting is a POMDP, we will *construct* our agent using basic MDP methods.  Our simple agent will assume (incorrectly!) that the system state consists only of the part of $\\mathbf{w}$ typed so far.  Well, almost: to simplify the task, we assume that the agent also telepathically knows how many characters of $\\mathbf{w}$ are left to type, so this is part of the system state.  Examples:\n",
    "\n",
    "true state of the environment | state assumed by the agent\n",
    "-|-\n",
    "`sequence\u2423modeling\u2423is\u2423 > the\u2423best` | `sequence\u2423modeling\u2423is\u2423` (8 left)\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423 > best` | `sequence\u2423modeling\u2423is\u2423the\u2423` (4 left)\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423b > est` | `sequence\u2423modeling\u2423is\u2423the\u2423b` (3 left)\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423best` | `sequence\u2423modeling\u2423is\u2423the\u2423best` (0 left)\n",
    "\n",
    "The agent's model of the environment thus has the form $p(y \\mid s,a)$ where $s = (\\mathbf{h},k)$ is the state assumed by the agent, consisting of the string $\\mathbf{h}$ typed so far and the remaining number of characters $k$.  The action $a$ is the new substring that the agent proposes.  $y$ is the environment's response, which carries an associated reward $r(y)$.  \n",
    "\n",
    "As mentioned earlier, our agent has a small set of actions in the MDP state $s$.  Its proposed string $a$ must have length $0 < |a| \\leq k$, and $a$ must be the string of length $|a|$ that maximizes the language model probability $p(a \\mid \\mathbf{h})$.\n",
    "\n",
    "Our agent has a model of the environment, in other words, a model $p(y \\mid s,a)$.  Specifically, it assumes that $p(\\text{accept} \\mid (\\mathbf{h},k), a)$ is the probability under the given language model that a sentence starting with $\\mathbf{h}$ and continuing for $k$ more characters would indeed choose $a$ as its next $|a|$ characters.  \n",
    "\n",
    "This model might be wrong if the agent's language model is not the actual distribution from which the environment sampled $\\mathbf{w}$.  But it is also wrong because it makes an incorrect conditional independence assumption.  The environment's behavior actually depends on the full POMDP state, not merely the simplified MDP state that the agent observed and is mistakenly assuming is enough to determine the environment's behavior.  To see this, imagine that the *actual* state of the POMDP is $\\texttt{sequence\u2423modeling\u2423is\u2423 > the\u2423best}$.  Imagine that the agent first predicts $a=\\texttt{that}$: the environment (user) will reject it, typing $\\texttt{t}$ instead.  Thus $y=\\texttt{t}$, leading to an actual state of $\\texttt{sequence\u2423modeling\u2423is\u2423t > he\u2423best}$.  A proper POMDP agent would not be able to observe this state, and specifically would not know the $\\texttt{> he\u2423best}$ part -- but the POMDP agent would at least know that the POMDP state does contain some such continuation, and it would have a posterior distribution (\"belief state\") over the possible states that the POMDP *might* be in given the agent's observations.  In particular, it would know from the interactions so far that the POMDP cannot be in any state of the form of the form $\\texttt{> hat}\\ldots$, because then the user would have accepted at the previous step instead of rejecting!  Thus, it would know that proposing $a=\\texttt{hat}$ at this step would definitely lead to rejection.  In contrast, our MDP agent has a model that might very well propose $a=\\texttt{hat}$, since it incorrectly thinks that the environment's probability of accepting that proposal is simply the probability that an arbitrary copy of $\\texttt{sequence\u2423modeling\u2423is\u2423t}$ that has 7 characters left would continue with $\\texttt{hat}$ as its next 3 characters.  In truth, the environment has probability 0 of accepting that proposal."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "0f10018d-6160-4922-9d88-7ee3488bd144",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class TypistState(EnvironmentState):\n",
    "    def __init__(self, *, string, start_index, debug=False):\n",
    "        self.string = string\n",
    "        self.current_index = start_index\n",
    "        self.debug = debug\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Our state will be the already-typed part of the string.\n",
    "        \"\"\"\n",
    "        return self.string[:self.start_index]\n",
    "\n",
    "    def execute_action(self, *, action):\n",
    "        \"\"\"\n",
    "        Given the action (a proposed string) check whether it is \"correct\"\n",
    "        and reward accordingly.\n",
    "        \"\"\"\n",
    "        # What are we looking for?\n",
    "        goldanswer = self.string[self.current_index : self.current_index + len(action)]\n",
    "        # Check what happens\n",
    "        if self.debug:\n",
    "            print(\"If we have \\\"\" + self.string[:self.current_index]\n",
    "                  + \"\\\" and still need \\\"\" + self.string[self.current_index:]\n",
    "                  + \"\\\", the proposition \\\"\" + action, end='\" ')\n",
    "        if action == goldanswer:\n",
    "            # Advance the task state that far\n",
    "            self.current_index += len(action)\n",
    "            reward = len(action) - 1\n",
    "            response = action\n",
    "            if self.debug:\n",
    "                print(\"is correct!\", end=' ')\n",
    "        else:\n",
    "            # Only advance by one\n",
    "            self.current_index += 1\n",
    "            reward = 0  # 1 character - 1 keypress\n",
    "            response = goldanswer[0]\n",
    "            if self.debug:\n",
    "                print(\"is incorrect (\\\"\" + goldanswer + \"\\\" would have been correct)!\", end=' ')\n",
    "        if self.debug:\n",
    "            print(\"We get reward\", reward, \"and response \\\"\" + response + \"\\\".\")\n",
    "        return reward, response\n",
    "    \n",
    "    def evaluate_agent(self, *, agent=None, agentclass=None, return_agent=False, **kwargs):\n",
    "        assert agent is None or agentclass is None\n",
    "        if agent is None:\n",
    "            agent = agentclass(\n",
    "                prefix=self.string[:self.current_index],\n",
    "                nchars_left=len(self.string)-self.current_index,\n",
    "                **kwargs  # this will be the way to pass the LM\n",
    "            )\n",
    "            \n",
    "        total_reward = 0\n",
    "        if self.debug:\n",
    "            print(f'Starting with state \"{self.string[:self.current_index]}>{self.string[self.current_index:]}\":')\n",
    "\n",
    "        while self.current_index < len(self.string):\n",
    "            # Get action\n",
    "            action = agent.decision()\n",
    "            if self.debug:\n",
    "                print(f'Agent chooses to predict \"{action}\" (length {len(action)})!')\n",
    "            # Use reward and response\n",
    "            reward, response = self.execute_action(action=action)\n",
    "            agent.receive_response(reward, response)  # We will define this below.\n",
    "            total_reward += reward\n",
    "            if self.debug:\n",
    "                print(f'User gave reward {reward} and response \"{response}\"')\n",
    "        if self.debug:\n",
    "            print(\"Received\", total_reward, \"total reward!\")\n",
    "        return total_reward if not return_agent else (total_reward, agent)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "c42d46f7-0d60-451d-9be9-46155064769d",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Let's see what our possible actions are in the initial state of the run that we saw above, and in some possible subsequent states.  For each action, we'll also see what the environment *would* do if the agent took that action.  Of course, the agent will only get to take *one* of the actions."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "d9962eeb-36ff-443f-9d15-2dd913ef68fc",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "for nchars in range(21, 25):\n",
    "    for length in range(1, 5):\n",
    "        # Set up an environment just to where we can test the action...\n",
    "        env = TypistState(string=\"sequence\u2423modeling\u2423is\u2423the\u2423best\", start_index=nchars, debug=True)\n",
    "        action = LM.best_string(prefix=\"sequence\u2423modeling\u2423is\u2423the\u2423best\"[:nchars], length=length)\n",
    "        env.execute_action(action=action)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "6ccd1ae4-1ee6-46f9-819b-c6beaa7f4ec7",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## A first agent: greedily choosing actions\n",
    "\n",
    "A first very simple agent is going to try to greedily choose the best action. Since our reward function gives meaningful rewards at every step, it's a sensible heuristic to always pick the action that gives the best expected immediate reward.  (This is not optimal, however, because it doesn't consider how the action affects future rewards.  We'll fix that later.)\n",
    "\n",
    "In the full RL setting, the agent must learn how the environment behaves (i.e., what the rewards are and with which probabilities they occur) through trial and error.  However, in our simple scenario, we will give the agent a model that already *perfectly* describes the environment: the language model from which our user draws the sentences they want to type!\n",
    "\n",
    "Equipped with this model and knowledge of our reward function the agent can compute the *expected immediate reward* of each action $a$.   In our case, that is the reward that the agent will receive if the user accepts $a$ times the model probability that the user will accept $a$, plus a reward of 0 times the model probability that the user will not accept $a$ but instead will type the next character."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "675c8b5c-158b-4425-bdf6-81946ba114b5",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def expected_immediate_reward(*, prefix, action, agent_lm):\n",
    "    \"\"\"\n",
    "    The prefix is the string that we observed so far, the LM is the\n",
    "    (potentially inadequate) LM of the agent.\n",
    "    \"\"\"\n",
    "    # What would the probability of the string we would give be, i.e.,\n",
    "    # what is the probability that it is correct?\n",
    "    scorer = LMScorer(PredictStringTask(agent_lm.idx2char, len(action)), agent_lm)\n",
    "    prob = scorer.score_aa(xx=prefix, aa=action).exp()\n",
    "    # What would we get if it was indeed correct?\n",
    "    # We assume knowledge of the reward function here of course.\n",
    "    positive_reward = len(action) - 1\n",
    "    # What if it was wrong? Well, we should really go over all the different\n",
    "    # \"rejections\" that the user could give us, but for the task of scoring, all\n",
    "    # that matters is that we get 1 character for 1 keypress, so the reward is 0.\n",
    "    # That's why we can succcinctly say that the expected reward is...\n",
    "    return prob * positive_reward + 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "8f696538-b551-4d15-b719-29b5e44d2ce0",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Let's actually compute those expected immediate rewards. What is the immediate reward we expect from taking any action when we are in the very first stage?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "84a8ecab-e200-4636-a671-8f4f52079fa3",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "prefix = \"sequence\u2423modeling\u2423is\u2423\"\n",
    "for length in range(1, 6):\n",
    "    action = LM.best_string(prefix=prefix, length=length)\n",
    "    er = expected_immediate_reward(prefix=prefix, action=action, agent_lm=LM)\n",
    "    print(length, f\"{action:5} {er.item():.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "48a2a8da-2dd1-44ae-8778-5483804dba5e",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Unsurprisingly, predicting four characters is best: if you already have \"t\", getting \"h\", \"e\", and \"\u2423\" isn't much of a risk -- but the reward is much higher.\n",
    "Just to make sure you understand these quantities: \n",
    "\n",
    "- What is the minimum these numbers could ever be (for any action $a$ with $a \\in \\mathbb{N}$)? $\\color{red}{\\text{FILL IN}}$\n",
    "- How many actions will achieve that minimum (given the model we use here)? $\\color{red}{\\text{FILL IN}}$\n",
    "- How far do we have to look to find the action with the highest reward? $\\color{red}{\\text{FILL IN}}$\n",
    "- Assume $a=\\texttt{the\u2423}$ indeed achieves the maximum. Is this enough to tell us that we should definitely pick $a=\\texttt{the\u2423}$? Why or why not? $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "So let's try to build a greedy agent to get us good actions!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "3c926f8d-3d79-4083-9483-a38ed623aa27",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class TextProposalAgent(RLAgent):\n",
    "    \"\"\"\n",
    "    These things will be shared by all the agents we will build in the sequel.\n",
    "    \"\"\"\n",
    "    def __init__(self, lm, prefix, nchars_left):\n",
    "        self.lm = lm\n",
    "        self.prefix = prefix\n",
    "        self.prefix_hcs = lm.hcs_from_context(prefix)  # useful to be cached\n",
    "        self.nchars_left = nchars_left\n",
    "\n",
    "    def receive_response(self, reward, response):\n",
    "        self.prefix += response\n",
    "        self.prefix_hcs = self.lm.hcs_from_context(response, hcs=self.prefix_hcs)\n",
    "        self.nchars_left -= len(response)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "31b42e1b-cd59-4772-98ff-4e7465e97962",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class GreedyExpectedImmediateRewardAgent(TextProposalAgent):\n",
    "    def decision(self):\n",
    "        \"\"\"\n",
    "        Make a decision based on `self.prefix` and `self.nchars_left` using\n",
    "        `self.lm.best_string`.\n",
    "        \"\"\"\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n",
    "\n",
    "# This should say: predict 4 characters, namely \"the\u2423\"\n",
    "assert GreedyExpectedImmediateRewardAgent(lm=LM, prefix=\"sequence\u2423modeling\u2423is\u2423\", nchars_left=8).decision() == \"the\u2423\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "39377de2-0c94-468d-ae77-e7e0dd833bf8",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Now, given our Typist and our Agent, it's time to play this game!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "66baec24-14ad-4cbd-a48e-5d1659682662",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# We should get a total reward of 5 here\n",
    "string = \"sequence\u2423modeling\u2423is\u2423the\u2423best\"\n",
    "start_index = 21\n",
    "# Verbose\n",
    "assert TypistState(string=string, start_index=start_index, debug=True) \\\n",
    "        .evaluate_agent(agent=GreedyExpectedImmediateRewardAgent(\n",
    "            lm=LM,\n",
    "            prefix=string[:start_index],\n",
    "            nchars_left=len(string)-start_index\n",
    "        )) == 5\n",
    "# Or, shorter, using some magic:\n",
    "assert TypistState(string=string, start_index=start_index, debug=True) \\\n",
    "        .evaluate_agent(agentclass=GreedyExpectedImmediateRewardAgent, lm=LM) == 5"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "c56a6aff-20fe-4dab-9cbb-885b6b8edc58",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Is that good or bad? Let's compare against a baseline that chooses randomly among the possible actions (all of which are rather good since they are high-probability under the LM):"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "7345515a-175e-4ce4-a228-4ee16830dd5f",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# A random length agent\n",
    "class RandomLengthAgent(TextProposalAgent):\n",
    "    def decision(self):\n",
    "        length = random.randint(1, self.nchars_left + 1)\n",
    "        return self.lm.best_string(prefix=self.prefix, length=length)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "a6159dae-ee0f-4269-8cf0-1ab519310f52",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Run it a few times to see how well it does on average\n",
    "rs = [\n",
    "    TypistState(string=string, start_index=start_index).evaluate_agent(agentclass=RandomLengthAgent, lm=LM)\n",
    "    for _ in range(10)\n",
    "]\n",
    "print(\"Rewards:\", rs)\n",
    "print(\"Average reward:\", np.average(rs))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "1faa865a-2177-4379-aa1f-c25373b0b648",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "So our greedy agent is definitely a lot better. But let's not just evaluate this on one cherry-picked example:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "72ca2aae-359b-4ab9-b2cd-d7dd12cd16a1",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def compare_agents(agent1class, agent2class, true_lm=LM, agent_lm=LM, n_sentences=5, total_length=20, start_index=12, verbose=True):\n",
    "    \"\"\"\n",
    "    Sample some sentences from true_lm and compare the performance of\n",
    "    the passed two agents on these sentences.\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    sum_1, sum_2 = 0, 0\n",
    "    for _ in range(n_sentences):\n",
    "        # Draw a sentence from the LM (though at lower local temperature, so the agent model isn't 100% perfect)\n",
    "        sample = ''.join(itertools.islice(true_lm.greedy_sample(temperature=0.2), total_length))\n",
    "        # Test both agents\n",
    "        agent1_reward = TypistState(string=sample, start_index=start_index).evaluate_agent(agentclass=agent1class, lm=agent_lm)\n",
    "        agent2_reward = TypistState(string=sample, start_index=start_index).evaluate_agent(agentclass=agent2class, lm=agent_lm)\n",
    "        # What happened!\n",
    "        sum_1 += agent1_reward\n",
    "        sum_2 += agent2_reward\n",
    "        if verbose:\n",
    "            print(f\"On {sample[:start_index]}>{sample[start_index:]}, agent 1 got {agent1_reward}, agent 2 got {agent2_reward}.\")\n",
    "    return (sum_1, sum_2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "ea900e9a-9f6f-4143-ab2a-abc9073267c2",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time compare_agents(RandomLengthAgent, GreedyExpectedImmediateRewardAgent)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "6b2f09fb-42e6-4f1a-bfdc-573b1280817f",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "You will have noticed that this is really way too slow for us to use it in the remainder of the assignment.\n",
    "That's why we will break all our nice abstractions a little to write a single (fast) function for our agent.\n",
    "\n",
    "We will inline the beam search, but more importantly, we will *fuse* all the different searches, because the all will share the same beam search prefix!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "4e597fbe-16c7-4145-a35b-457f857851ec",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def best_strings_from_hcs(self, *, hcs, max_length, beam_size=5):\n",
    "    \"\"\"\n",
    "    Returns a list z, such that z[l] = (logprob, string, hcs) of the best\n",
    "    string of length l.\n",
    "    \"\"\"\n",
    "    beam_size = min(beam_size, len(self.idx2char))\n",
    "    queue = [(torch.tensor(0.0), \"\", hcs)]\n",
    "    returns = [queue[0]]\n",
    "    while len(queue[0][1]) < max_length:\n",
    "        next_queue = []\n",
    "        for pscore, ptaskstate, pmodelstate in queue:\n",
    "            probs = (pmodelstate[-1][0] @ self.embedding.weight.t()).log_softmax(dim=-1)\n",
    "            for nscore, idx in zip(*probs.squeeze(0).topk(beam_size)):\n",
    "                nmodelstate = self._hcs_from_cidx(pmodelstate, torch.tensor([idx]))\n",
    "                next_queue.append((nscore + pscore, ptaskstate + self.idx2char[idx], nmodelstate))\n",
    "        if len(next_queue) == 0:\n",
    "            break\n",
    "        next_queue.sort(key=lambda x: -float(x[0]))\n",
    "        queue = next_queue[:beam_size]\n",
    "        returns.append(queue[0])\n",
    "    return returns\n",
    "\n",
    "# Patch it, too, into the language model\n",
    "LanguageModel.best_strings_from_hcs = best_strings_from_hcs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "3d0a154a-567e-42d4-b33e-923ebb65500a",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Try it out:\n",
    "\n",
    "print(\"Our old function\")\n",
    "start_time = time.time()\n",
    "for _ in range(1):\n",
    "    for a in range(1, 8+1):\n",
    "        print(LM.best_string(prefix=\"sequence\u2423modeling\u2423is\u2423\", length=a))\n",
    "print(time.time() - start_time)\n",
    "\n",
    "print(\"\\nOur new function\")\n",
    "start_time = time.time()\n",
    "hcs = LM.hcs_from_context(\"sequence\u2423modeling\u2423is\u2423\")\n",
    "print('\\n'.join([x[1] for x in LM.best_strings_from_hcs(hcs=hcs, max_length=8)][1:]))\n",
    "print(time.time() - start_time)\n",
    "\n",
    "print(\"\\nOur new function, much faster\")\n",
    "start_time = time.time()\n",
    "hcs = LM.hcs_from_context(\"sequence\u2423modeling\u2423is\u2423\")\n",
    "print('\\n'.join([x[1] for x in LM.best_strings_from_hcs(hcs=hcs, max_length=50)][-10:]))\n",
    "print(time.time() - start_time)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "4f1ea436-974d-44aa-86c7-486ff81ac9da",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Much better! Let's use it to implement faster agents! The main objective here was to fuse the searches into one search that also returns partial results, but note that avoiding the boilerplate of beam search also gives the random agent a nice speedup: "
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "790d774c-ad83-44ac-a2c5-97bc764f0619",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class FastRandomLengthAgent(TextProposalAgent):\n",
    "    def decision(self):\n",
    "        length = random.randint(1, self.nchars_left + 1)\n",
    "        hyps = self.lm.best_strings_from_hcs(hcs=self.prefix_hcs, max_length=length)\n",
    "        return hyps[-1][1]\n",
    "\n",
    "class FastGreedyExpectedImmediateRewardAgent(TextProposalAgent):\n",
    "    def decision(self):\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "cac40275-895e-4620-89ff-c0d97fc2b70d",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Compare on two sentences\n",
    "for sample in [\"modeling\u2423is\u2423the\u2423best\", \"kare\u2423to\u2423from\u2423first\u2423g\"]:\n",
    "    print(\"Predict\", sample[:12], \">\", sample[12:])\n",
    "    \n",
    "    def get_reward(agentclass):\n",
    "        random.seed(0)\n",
    "        user = TypistState(string=sample, start_index=12)\n",
    "        return user.evaluate_agent(agentclass=agentclass, lm=LM)\n",
    "    # Test the random speedup\n",
    "    %time      random_reward = get_reward(                     RandomLengthAgent)\n",
    "    %time fast_random_reward = get_reward(                 FastRandomLengthAgent)\n",
    "    # Test the greedy speedup\n",
    "    %time      greedy_reward = get_reward(    GreedyExpectedImmediateRewardAgent)\n",
    "    %time fast_greedy_reward = get_reward(FastGreedyExpectedImmediateRewardAgent)\n",
    "    assert greedy_reward == fast_greedy_reward, (greedy_reward, fast_greedy_reward)\n",
    "    assert random_reward == fast_random_reward, (random_reward, fast_random_reward)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "c6b8b244-8f95-42ff-ad89-46728aa2bed6",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "By breaking our beautiful abstraction and making some strong assumptions we have made our solution about 20x faster!\n",
    "Now, let's run a little comparative study:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "49e29eef-7760-4b51-aa48-fc5a8fc2ca06",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time compare_agents(FastRandomLengthAgent, FastGreedyExpectedImmediateRewardAgent)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "2933d64b-f5a2-4eb4-bfac-b892c8a30aa7",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Down from 42s to 3 seconds -- nice! Let's try a few more:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "65909137-7cba-4e53-820f-ec6b357b545f",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time sum_random, sum_greedy = compare_agents(FastRandomLengthAgent, FastGreedyExpectedImmediateRewardAgent, n_sentences=30)\n",
    "print(\"Totals! Random:\", sum_random, \"-- Greedy:\", sum_greedy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "9ee77b8d-9632-4517-b61c-6ea49748ef94",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Interestingly, sometimes the random agent seems to outperform the greedy agent. To be fair, of course, the random agent isn't quite that random -- it still extracts the best string of a given length from the LM. A *real* random agent would be remarkably terrible:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "737fec5d-a193-4ca1-9f7d-7385dedf6384",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class TrulyRandomAgent(TextProposalAgent):\n",
    "    def decision(self):\n",
    "        length = random.randint(1, self.nchars_left)\n",
    "        # Temperature 1000 makes it practically uniform (i.e., truly random)\n",
    "        sample = LM.greedy_sample(hcs=self.prefix_hcs, temperature=1000)\n",
    "        return ''.join(itertools.islice(sample, self.nchars_left))\n",
    "\n",
    "compare_agents(TrulyRandomAgent, FastGreedyExpectedImmediateRewardAgent, n_sentences=10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "765650c5-b3ff-48de-bc9f-c03c93666229",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## Simplifying things: predict from a four-letter alphabet\n",
    "\n",
    "Because even with our speedups this is still plenty slow, we will simplify the task a little: our strings are now over an alphabet of three characters: \"x\", \"o\", and our old readability-improving friend \"\u2423\". The language model will also be much smaller and thus faster.\n",
    "\n",
    "What data are we training on? We follow established NLP practice and pretend Jason's NLP class homework 1 toy grammars are somehow meaningful.  (Established NLP practice?  Please, catch the sarcasm and read [Yoav Goldberg's great takedown](https://medium.com/@yoav.goldberg/an-adversarial-review-of-adversarial-generation-of-natural-language-409ac3378bd7) of someone outside JHU who tried to write a real paper using those grammars.)  We generate some random sentences from one of those grammars, and then replace all characters in terminal symbols with with \"x\" or \"o\" and replace spaces with \"\u2423\".  This will be our training dataset for the language model.\n",
    "\n",
    "```\n",
    "$ ./randsent holygrail.gr 350000 | tr '\\nabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ' ' xoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxox ' | head -c 10000000 | sed 's/ /\u2423/g' | gzip > xo_corpus.txt.gz\n",
    "\n",
    "$ zcat xo_corpus.txt.gz | sed 's/\u2423/\\n/g' | sort -nr | uniq -c | sort -nr\n",
    " 382815 x\n",
    " 102040 xoxox\n",
    "  92066 oxoxx\n",
    "  72006 ox\n",
    "  71656 ooxx\n",
    "  69657 oox\n",
    "  69640 xoxooxo\n",
    "  69508 xox\n",
    "  69495 ooxo\n",
    "  69394 xxxo\n",
    "  59820 xxooxo\n",
    "  59718 oxxx\n",
    "  52349 xxoxox\n",
    "  52225 ooxoxx\n",
    "  52175 oxx\n",
    "  52111 xx\n",
    "  52003 xxooxxx\n",
    "  37677 xxo\n",
    "  [...]\n",
    "```"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "5955c0ba-4e14-4063-9397-1093ffe9274f",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "retrain_small_model = False\n",
    "\n",
    "if retrain_small_model:\n",
    "    if not os.path.isfile(\"xo_corpus.txt.gz\"):\n",
    "        import urllib\n",
    "        urllib.request.urlretrieve(\"https://sjmielke.com/tmp/xo_corpus.txt.gz\", \"xo_corpus.txt.gz\")\n",
    "\n",
    "    with gzip.open(\"xo_corpus.txt.gz\", 'rt') as f:\n",
    "        xo_corpus = f.read()\n",
    "\n",
    "    train_lm(\"xo\", xo_corpus, nlayers=3, nhid=512, embsize=32, BATCHSIZE=200, BPTTLENGTH=100)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "d40afeef-1c1c-4d68-95ca-009b07396cd2",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Load the pretrained model\n",
    "XOLM = LanguageModel(\"ox\u2423\", layers=2, hidden_size=64, embedding_size=4).cpu()\n",
    "XOLM.load_state_dict(torch.load(\"xo.lm.statedict.pt\", map_location='cpu'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "10a7faf4-1c64-44a2-9455-c9bf72abd757",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "\"\".join(itertools.islice(XOLM.greedy_sample(), 100))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "3923ff36-a7a0-4fb2-b212-984bfd83fbfd",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Looks like we got the structure down (see the single \"x\"s that used to be \".\"s).\n",
    "Just to check that this was worth it, let's rerun it on our game:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "881c814e-5ddb-47f4-a545-1514ec2b93e9",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time sum_random, sum_greedy = compare_agents(\\\n",
    "    FastRandomLengthAgent,\\\n",
    "    FastGreedyExpectedImmediateRewardAgent,\\\n",
    "    true_lm=XOLM,\\\n",
    "    agent_lm=XOLM,\\\n",
    "    n_sentences=100,\\\n",
    "    verbose=False\\\n",
    ")\n",
    "print(\"Totals! Random:\", sum_random, \"-- Greedy:\", sum_greedy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "01ee7114-150d-460d-9758-4d503fefb31f",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "That speed will do -- for making everything much slower and more complicated."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "294f4b1d-e9d0-4516-84a3-4f9ca0697d1c",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## From greedy actions to planning\n",
    "\n",
    "You will have noticed that we tried to stress that this (definitely better-than-random) agent of ours is *greedy* -- and you should know that we generally think that greediness is a bad thing. So far our reward function has been \"benevolent\", making greedy actions look reasonable enough, but the issue will become more apparent when we redefine the reward slightly.  We want to help the user enter the text in as few steps as possible (remember that the user has to press one key per step).  So let's define the reward to be $-1$ on every step (so it's really a penalty).  The total reward of an episode is the same as before, except that the old reward function added the constant $|\\mathbf{w}|$ to the total reward (compare the two reward functions on an example if you want to check this).  But the reward is now distributed differently across the time steps.  With the new function, all actions have the same immediate reward, so they are all tied and the greedy agent has no way to choose!  \n",
    "\n",
    "Once again, the solution is a kind of lookahead: let's *plan* out the rest of the trajectory!  This allows us to not greedily choose the action that maximizes immediate expected reward (i.e., minimizes immediate harm under our nasty new reward function), but instead choose the action that will give us the highest expected *return* (total reward over all futher steps).  Under our new reward function, that means trying to find short trajectories, which have a less negative return.\n",
    "\n",
    "There is an obvious complication: to plan ahead, we would have to know what response the user will give to each of our actions. Since we don't know that, we have to again use the agent's language model to hypothesize what the user might say -- and then we follow each path.\n",
    "\n",
    "Take a look at this picture, showing one such *game tree* for predicting a length-3 string using the letters \"a\", \"b\", and \"c\":\n",
    "\n",
    "<img width=\"100%\" src=\"https://sjmielke.com/tmp/gametree.png\" />\n",
    "\n",
    "Each node of this tree is a *state* containing information about the string we've seen so far and the actions that took us there; the states are aligned by timestep in the to-be-predicted string. In blue we see the actions that can be taken (with the corresponding 1-best string under it), then for each action, the paths split depending on whether the user accepted the whole proposal (green arc) or rejected it (red arcs, for each possible rejection), both putting us in a new state from which we continue reasoning... until we've reached the end! As you can see, even for this little example, there are a *lot* of paths, namely as many as there are final states.\n",
    "\n",
    "How many is that in this example? $\\color{red}{\\text{FILL IN}}$ \n",
    "\n",
    "How many is that in general for predicting $n$ characters from a set of $m$ symbols? (Hint: use recursion) $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "How many paths then would we have in our old prediction task with the old language model? $\\color{red}{\\text{FILL IN}}$ \n",
    "\n",
    "You should have arrived at an answer that more or less says \"utterly infeasible\". Of course, in the spirit of learning by pain, we will still try to construct an agent, that does precisely this: expand *all* paths in the game tree -- for our new, very small task, of course."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "1ce0d2e2-eaa1-4e0c-95f1-cf6172e99e33",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## The expected return is the expected total reward: marginalizing over all paths\n",
    "\n",
    "How can we use this game tree to find the action that seems \"best\"? We are still looking to maximize reward, but this time we will not only see how much the immediate action (i.e., the immediate outgoing arc of the start state in our game tree) would give us and with which probability, but we will play each of these outcomes until the end -- only then will we know the worth of the initial action.\n",
    "\n",
    "But how do we get from knowing a completed trajectory's return and having probabilities for environment responses (our LM) to tallying up the worth of that first initial action?\n",
    "\n",
    "We should **marginalize** over environment responses, but **maximize** over agent choices.  That is because the agent's *policy* is to always choose the action it believes to be best. If we were using a stochastic policy, we would marginalize here, too."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "be8e06a7-5792-430b-a17d-d5b2808118cc",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class ExhaustivePlanningAgent(TextProposalAgent):\n",
    "    def decision(self):\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7bf9210c-9a19-41ea-86a8-735a88924745",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "sample = \"x\u2423xox\u2423oxoxxoxo\u2423xx\u2423xoxox\u2423xxooxoo\u2423x\u2423xoxooxo\u2423oxoxx\u2423ooxoxx\u2423ooxo\u2423oxxoo\u2423x\"\n",
    "\n",
    "for maxlen in range(1, 6):\n",
    "    user = TypistState(string=sample[:12+maxlen], start_index=12, debug=True)\n",
    "    %time user.evaluate_agent(agentclass=ExhaustivePlanningAgent, lm=XOLM)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "f8cd2cd5-bc73-4dc7-8eff-99a2334934ab",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "This is still embarassingly slow -- runtime scales linearly with the number of paths and that scales exponentially in the length of the to be predicted suffix:\n",
    "\n",
    "length | paths | ms\n",
    "-|-|-\n",
    "1 | 3 | 21\n",
    "2 | 19 | 36\n",
    "3 | 175 | 277\n",
    "4 | 2123 | 3720\n",
    "5 | 32043 | 66000\n",
    "6 | 579095 | 1556000\n",
    "\n",
    "How could we fix this? Of course, we could simplify our model --  and in a sense we already did that in the previous agents! The way we set up their internal belief state, our game tree is actually not a tree, but a DAG. Why? How many nodes does it have? $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "You should have come up with a number that is $O(|\\mathbf{w}|)$ -- can we get a DAG that has $O(1)$ nodes? If yes, explain how, if no, explain why. $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "Could we do it if we didn't have the number of character left as part of our state? $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "But we could also perform approximate inference - and we will do just that, specifically, we will (finally) start using terms and techniques from reinforcement learning to find our way through this enormous state space."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "6d410e25-ca4b-4c52-9c44-b932374b2417",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## Value functions judge states\n",
    "\n",
    "If we could tell how good each state was, we could call off our search very early -- in fact, we could take our GreedyAgent and judge each action not only by how much immediate reward we get, but add in how good the state that we end up in will be (in terms of the total expected reward it can lead us to).\n",
    "\n",
    "Let's make this more precise. We will try to learn a function $V$ such that $V(s)$ will be the expected total reward that we obtain by starting in state $s$ if we always choose the best action (best, according to this new function $V$). Learning this function will be an incremental process, that is, we will start with some estimate (that is most likely wrong) and then iteratively refine these estimates.\n",
    "\n",
    "How can this refinement work? For any given state $s$ we can improve $V(s)$ by recomputing it from the states that the actions you can take from $s$ lead to.\n",
    "Consider this example:\n",
    "\n",
    "<img width=\"100%\" src=\"https://sjmielke.com/tmp/state_to_state_with_model.png\" />\n",
    "\n",
    "In state $s$, we can take actions $a_1$ and $a_2$. What happens if we did that? This is where the model of reality that the agent has comes in (the one that in our example already told us how likely an acceptance for a given proposal was): we know that taking action $a_1$ can land us in $s_{1,1}$, $s_{1,2}$, or $s_{1,3}$ (with certain probabilities, say $p_{1,1}$, $p_{1,2}$, and $p_{1,3}$).\n",
    "Then we can recompute $V(s)$ using the immediate rewards $r_{*,*}$ and the value function estimates $V(s_{*,*})$!\n",
    "\n",
    "As in the search case, the expected return from state $s$ on will be defined by the *best* action (since our *policy* is to always choose the best action), where \"best\" can be conveniently defined using $V$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{expected-return}(s, a_1) &= \\mathbb{E}[r_{1,i} + \\gamma V(s_{1,i}))] = \\sum_{i=1}^3 p_{1,i} \\cdot (r_{1,i} + \\gamma V(s_{1,i})) \\\\\n",
    "    \\text{expected-return}(s, a_2) &= \\mathbb{E}[r_{1,i} + \\gamma V(s_{1,i}))] = \\sum_{i=1}^3 p_{1,i} \\cdot (r_{1,i} + \\gamma V(s_{1,i}))\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The *discount factor* $\\gamma$ (usually, a constant like $.99$) is necessary in situations where trajectories can be infinitely long (think of cycles in some state graph), so we don't just keep amassing rewards -- since in our case we have finite trajectories, we can just set $\\gamma(s) = 1$.\n",
    "\n",
    "for all non-terminal states $s$.\n",
    "\n",
    "Note that we can hardcode $V(s)$ to be $0$ for all terminal states $s$ -- if we're done, we will not get any more rewards (and why try to learn that when we already know it). *Note: in the literature, this is usually accomplished by setting $\\gamma = 0$ when it discounts the value of a terminal state (to keep $V(s)$ completely arbitrary), but we choose to do things differently to simplify the exposition.*\n",
    "\n",
    "Now, we can finally update $V(s)$ given that we know what action we would have taken in in $s$ (say WLOG that it was $a_1$ that had higher $\\text{expected-return}(s, a)$) using the Bellman equation (also called the \"Bellman backup operator\"):\n",
    "\n",
    "$$\n",
    "    V(s) \\leftarrow \\sum_{i=1}^3 p_{1,i} \\cdot (r_{1,i} + \\gamma V(s_{1,i}))) = \\mathbb{E}[r_{1,i} + \\gamma V(s_{1,i}))]\n",
    "$$"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "8124e66a-2ef8-4052-a4fa-5c48e4e1c649",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "If we iterate that process for all our states $s$, we will learn a good value function $V$ that will lead us to make *globally optimal greedy decisions*!\n",
    "Pretty cool! But wait... iterate for *all* states? Clearly that's utterly infeasible. And what should this function $V$ look like internally? A big table of all the millions of states? That can't be it... We will tackle these two problems in turn:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "5545dfb7-dfb6-4831-8c5f-919f9ffe1ed5",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "### Deep RL: approximating $V$ with a neural network\n",
    "\n",
    "The simplest option for implementing a function $V$ is a big table in memory where for each $s$ a separate value is stored.\n",
    "This setting is known as *tabular RL* (\"tabular\" is the adjectival form of \"table\").  While it gives many nice guarantees for convergence etc., it is not very practical for real-world AI settings.  The issue is that most problems have infeasibly large state spaces: not only can we not fit these giant tables in our computer memory, but we also lose out by not *sharing information* between related states.\n",
    "\n",
    "That is where \"Deep\" comes into Reinforcement Learning. Instead of having discrete entries for states $s$, let $V$ be a neural network that outputs a scalar given some vector representation of the state $s$.\n",
    "\n",
    "For our purposes this means that the agent has to encode its state (or really the belief state, since we are in a POMDP, but we will ignore that as a technicality in the sequel) into a vector somehow. Well, we are in luck: we can just use the hidden state of the pretrained language model that we are already using.\n",
    "Given this vector we could build an arbitrarily complicated neural network that outputs a scalar -- we will try for a simple linear regressor and some feedforward networks.\n",
    "\n",
    "This of course also means that the assignment given in the equation above is meaningless: we cannot just \"assign\" an output value to a neural network. We will instead minimize the squared distance between the old estimate for $V(s)$ and the one given by the right-hand side of the Bellman equation."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "0db487a8-abc2-419e-b134-ec37adc58d73",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class ValueFunctionApproximator(torch.nn.Module):\n",
    "    def __init__(self, h_dims):\n",
    "        super().__init__()\n",
    "        # The main network\n",
    "        self.linear1 = torch.nn.Linear(h_dims + 2, 16)\n",
    "        self.linear2 = torch.nn.Linear(2 * 16 + 1, 1)\n",
    "        # The more stable, since only slowly and indirectly updated target network\n",
    "        self.linear1_target = torch.nn.Linear(h_dims + 2, 16)\n",
    "        self.linear2_target = torch.nn.Linear(2 * 16 + 1, 1)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Set the target network weights to a new moving average.\n",
    "        \"\"\"\n",
    "        self.linear1_target.weight.data = 0.2 * self.linear1.weight.data + 0.8 * self.linear1_target.weight.data\n",
    "        self.linear2_target.weight.data = 0.2 * self.linear2.weight.data + 0.8 * self.linear2_target.weight.data\n",
    "        self.linear1_target.bias.data = 0.2 * self.linear1.bias.data + 0.8 * self.linear1_target.bias.data\n",
    "        self.linear2_target.bias.data = 0.2 * self.linear2.bias.data + 0.8 * self.linear2_target.bias.data\n",
    "\n",
    "    def forward(self, hcs, nchars_left, target=False):\n",
    "        \"\"\"\n",
    "        Using PyTorch syntax, we define `forward()` to give us the scalar from\n",
    "        the state representation we feed in: the LM hidden state and the number\n",
    "        of characters left.\n",
    "        The `target` parameter tells us whether to use the (moving average)\n",
    "        target network weights.\n",
    "        \"\"\"\n",
    "        if nchars_left == 0:\n",
    "            return torch.tensor(0.0)\n",
    "        lmrep = hcs[-1][0][0]\n",
    "        n = float(nchars_left)\n",
    "        inp = torch.cat([lmrep, torch.tensor([n, math.log(n)])])\n",
    "        hid = (self.linear1_target if target else self.linear1)(inp).tanh()\n",
    "        inp = torch.cat([hid, hid / n, torch.tensor([n])])\n",
    "        return (self.linear2_target if target else self.linear2)(inp).squeeze()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "243426cd-e050-4214-b08e-ff7b3d92de17",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# This is how we would use it:\n",
    "hcs, nchars_left = LM.hcs_from_context(\"sequence\u2423modeling\u2423is\u2423\"), 8\n",
    "V = ValueFunctionApproximator(LM.lstm_layers[-1].hidden_size)\n",
    "V(hcs, nchars_left)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "87f16a9a-b267-4b6d-a81d-78899d95dcba",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We slipped in another trick here.  We defined two versions of our neural network for predicting values.  The main network has layers `linear1` and `linear2`,  but we also have another network with the same topology, whose layers are called `linear1_target` and `linear2_target`. This *target network* will be updated more slowly than the main network, so that it doesn't oscillate during training.  Its weights will be a *moving average* of the weights of the main network.  The trick is to use this target network in place of the main network when we compute the right-hand side of the Bellman equation.  This can improve convergence rates.  Note that no gradients flow into the target network -- its weights will only change by manually averaging past weights of the main network."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "f54ae564-9ab3-4e59-80e4-e8eae50eb167",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# We will need the expected return for the Bellman equation and the decisions:\n",
    "def expected_returns(*, prefix_hcs, nchars_left, value_function, agent_lm, gamma=0.99, target=False):\n",
    "    \"\"\"\n",
    "    `prefix_hcs` and `nchars_left` encode our state and will be used to find\n",
    "    actions and possible environment feedback, according to the `agent_lm` LM.\n",
    "    Returns the proposals (a list of strings of length 1..nchars_left) and a 1-D\n",
    "    tensor containing the expected returns for all these action proposals\n",
    "    according to the `value_function`.\n",
    "    \"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "11dc9d29-ac43-4878-8199-b06b90d8487a",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Given such returns, the loss we want to minimize is easy to define as the\n",
    "# squared distance between the current V estimate and the right-hand side of the\n",
    "# Bellman equation, using `expected_returns`.\n",
    "def value_function_loss_for_state(*, prefix_hcs, nchars_left, value_function, agent_lm):\n",
    "    # The old value of the value function\n",
    "    lhs = value_function(prefix_hcs, nchars_left)\n",
    "    # Consult our model to get the next states and their values\n",
    "    _, returns = expected_returns(\n",
    "        prefix_hcs=prefix_hcs,\n",
    "        nchars_left=nchars_left,\n",
    "        value_function=value_function,\n",
    "        agent_lm=agent_lm,\n",
    "        target=True  # this is where we use the target network!\n",
    "    )\n",
    "    # Choose according to the argmax -- not our policy! Because that is what we\n",
    "    # want in the end! The corresponding expectated return is:\n",
    "    rhs = torch.max(returns)\n",
    "    # Return the squared distance between that and the current estimate.\n",
    "    # Note that we are NOT doing \"residual gradient learning\" here (this would\n",
    "    # mean also using the gradients of the right-hand side to update), because\n",
    "    # this will make thing often optimize towards the wrong thing. We want to\n",
    "    # update the \"past\" using the \"future\". So, we detach:\n",
    "    return torch.nn.functional.mse_loss(lhs, rhs.detach())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "a001549d-94dc-43b4-b648-249da51a0045",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# That is how we would use the function:\n",
    "value_function_loss_for_state(prefix_hcs=hcs, nchars_left=nchars_left, value_function=V, agent_lm=LM)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "37454021-1ac7-4f65-9c10-62b0ffdfe098",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "### Learning from rollouts\n",
    "\n",
    "The next question is: which states are we updating our value function (approximation) with?\n",
    "Since we can only interact with the true environment one action and response at a time, it is impossible to just reach any state we would want to (and even if that were not true, enumerating all states in our giant state space would still be absolutely infeasible).\n",
    "\n",
    "The answer here is that we will *roll out* some *exploration policy*. (So far, we have always used an `argmax` policy: choose the action that will get you the most reward -- but we will change that a little below and actually inject some randomness. Just wait a second.)\n",
    "\n",
    "Rolling out a policy in the environment, we obtain a *sample trajectory* that performs the task from start to end. Then, given this trajectory, we update $V(s)$ for all states that are on this trajectory.\n",
    "\n",
    "Note that in the update equation given above it looks a little like we are \"trying out all the actions\" -- but, like with the GreedyAgent, we are not testing all actions against the *true* environment, but only against the *model* of the environment that we have."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "5532bc54-ea3b-439e-8d95-49afc0efcea6",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class ValueFunctionExpectedReturnAgent(TextProposalAgent):\n",
    "    def __init__(self, lm, prefix, nchars_left, value_function, exploration_policy):\n",
    "        \"\"\"\n",
    "        `exploration_policy` is a function that given a tensor of returns, yields the index\n",
    "        of the action to take (usually, the best action).\n",
    "        \"\"\"\n",
    "        self.lm = lm\n",
    "        self.prefix = prefix\n",
    "        self.prefix_hcs = lm.hcs_from_context(prefix)\n",
    "        self.nchars_left = nchars_left\n",
    "        self.value_function = value_function\n",
    "        self.exploration_policy = exploration_policy\n",
    "        self.visited_cache = []\n",
    "\n",
    "    def receive_response(self, reward, response):\n",
    "        self.prefix += response\n",
    "        self.prefix_hcs = self.lm.hcs_from_context(response, hcs=self.prefix_hcs)\n",
    "        self.nchars_left -= len(response)\n",
    "\n",
    "    def decision(self):\n",
    "        # Save visited state information\n",
    "        hcs = tuple((hc[0].detach(), hc[1].detach()) for hc in self.prefix_hcs)\n",
    "        self.visited_cache.append(\n",
    "            {\n",
    "                \"prefix_hcs\": hcs,\n",
    "                \"nchars_left\": self.nchars_left\n",
    "            }\n",
    "        )\n",
    "        # Now pick the one with the highest expected return!\n",
    "        proposals, returns = expected_returns(\n",
    "            prefix_hcs=self.prefix_hcs,\n",
    "            nchars_left=self.nchars_left,\n",
    "            value_function=self.value_function,\n",
    "            agent_lm=self.lm\n",
    "        )\n",
    "        return proposals[self.exploration_policy(returns)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "5c02fcf6-8e7a-4dbb-86a6-3415974b6a33",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Try it again -- this time, everything is super-fast since we essentially make greedy decisions\n",
    "for maxlen in range(1, 10):\n",
    "    user = TypistState(string=sample[:12+maxlen], start_index=12)\n",
    "    vfa = ValueFunctionApproximator(XOLM.lstm_layers[-1].hidden_size)\n",
    "    %time user.evaluate_agent(agentclass=ValueFunctionExpectedReturnAgent, lm=XOLM, value_function=vfa, exploration_policy=lambda t: torch.argmax(t))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "c16347fb-49cd-4530-9e2f-0b2ca46e11c4",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We end up with a roughly linear dependence (R\u00b2=0.91 on our measurements up to length 100), as expected. Nearly there!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "0660cee5-858e-405a-851a-4c3019b29137",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "### We need to force exploration\n",
    "\n",
    "The way our update is defined, we will only ever visit and update the states that our value function tells us are good. So if there is a state that is good, but our current value function doesn't know that -- it may never find out!\n",
    "We therefore need to force our policy to *explore* states, even if they aren't the best according to the value function.\n",
    "\n",
    "Specifically, we will use $\\epsilon$-greedy sampling: with probability $1-\\epsilon$, take the best action, and with probability $\\epsilon$, sample uniformly from all *other* actions ($\\epsilon$ is usually set to something like $.05$)."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "9c427012-47c1-4901-bdb4-5dbb9f63e403",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def eps_greedy_policy(returns, eps=.05):\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "f2c470d6-aec3-4754-bf9e-b1d21ff121be",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "With this, we are ready to write the entire training loop:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "9391d3ff-f544-4cb9-978e-0ee7b7310d2c",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def train(*, agentclass, params_to_optimize, agent_lm, exploration_policy, loss_function, target_network_updater, sentences, log_interval, cache_gradients, **kwargs):\n",
    "    random.seed(0)\n",
    "    sum_reward, sum_loss = 0, 0\n",
    "    # Only update the desired parameters (i.e., the main network, not the target network)\n",
    "    optimizer = torch.optim.Adam(params_to_optimize, lr=1.0)\n",
    "    optimizer.zero_grad()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Run in environment\n",
    "        env = TypistState(string=sentence, start_index=0)\n",
    "        reward, agent = env.evaluate_agent(\n",
    "            agentclass=agentclass,\n",
    "            lm=agent_lm,\n",
    "            return_agent=True,\n",
    "            exploration_policy=exploration_policy,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Construct loss for function approximator\n",
    "        loss = torch.sum(\n",
    "            torch.stack(\n",
    "                [loss_function(agent_lm=agent_lm, **d, **kwargs) for d in agent.visited_cache]\n",
    "            )\n",
    "        )\n",
    "        # Output\n",
    "        sum_reward += reward\n",
    "        sum_loss += loss.item()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(\"Avg reward (using exploration policy):\", sum_reward / log_interval, \"Avg loss:\", sum_loss / log_interval)\n",
    "            print(\"Mean abs weights:\", [p.abs().mean().item() for p in params_to_optimize])\n",
    "            # print(\"VFA example:\", ' '.join([f\"{value_function(XOLM.hcs_from_context('ooxo\u2423xxoxoxxxo\u2423xxoxox\u2423xxo\u2423oxoxxoxo\u2423x'), ncl).item():.3f}\" for ncl in range(1, 10)]))\n",
    "            sum_reward, sum_loss = 0, 0\n",
    "        # Optimize/update\n",
    "        loss.backward()\n",
    "        # Now apply the gradients to the target network after n iterations\n",
    "        if (i + 1) % cache_gradients == 0:\n",
    "            optimizer.step()\n",
    "            target_network_updater()\n",
    "            optimizer.zero_grad()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "46fe8821-db8f-48eb-89b5-6676b6178204",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def train_vf(agent_lm, sentences, cache_gradients=2):\n",
    "    \"\"\"\n",
    "    You can increase `cache_gradients` for increased stability...\n",
    "    ...at the cost of slower convergence!\n",
    "    \"\"\"\n",
    "    vfa = ValueFunctionApproximator(agent_lm.lstm_layers[-1].hidden_size)\n",
    "    train(\n",
    "        agentclass=ValueFunctionExpectedReturnAgent,\n",
    "        params_to_optimize=list(vfa.linear1.parameters()) + list(vfa.linear2.parameters()),\n",
    "        agent_lm=agent_lm,\n",
    "        exploration_policy=eps_greedy_policy,\n",
    "        value_function=vfa,\n",
    "        loss_function=value_function_loss_for_state,\n",
    "        target_network_updater=vfa.update_target_network,\n",
    "        sentences=sentences,\n",
    "        log_interval=20,\n",
    "        cache_gradients=2\n",
    "    )\n",
    "    return vfa"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "695ae3c4-2d42-4093-a750-71d19d0ade08",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "First let's try to overfit to some finite set of sentences (this is gonna take a few minutes, feel free to reduce the number of times we train on that sentence -- it is set to 500 below)."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "88d79011-2723-42cd-ac4e-45b5ada5a2f7",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "testsentence = '\u2423xxo\u2423x\u2423ooxo\u2423xxxooxx\u2423oxoxx\u2423xoxo'\n",
    "vfa_overfit = train_vf(XOLM, [testsentence] * 200)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "cad62680-31f7-42e8-9c80-ff8b53a24231",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "print(\"Greedy gets:\", TypistState(string=testsentence, start_index=0).evaluate_agent(agentclass=FastGreedyExpectedImmediateRewardAgent, lm=XOLM))\n",
    "\n",
    "print(\n",
    "    \"After training, we get this reward using our argmax policy:\",\n",
    "    TypistState(string=testsentence, start_index=0).evaluate_agent(\n",
    "        agentclass=ValueFunctionExpectedReturnAgent,\n",
    "        lm=XOLM,\n",
    "        value_function=vfa_overfit,\n",
    "        exploration_policy=lambda t: torch.argmax(t)\n",
    "    )\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "ee7b8b2d-5bb7-4b94-a179-41d6e166b7a4",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Nice (although we brutally overfit)! Let's do a qualitative check whether the value function learned what we wanted it to learn:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "7f7a9615-dec3-4017-a51f-4c301b80bb72",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "intermediate_sentence = '\u2423xxo\u2423x\u2423ooxo\u2423xxxooxx'\n",
    "\n",
    "# The more characters we think are left, the higher we think the return will be:\n",
    "print(' '.join([f\"{vfa_overfit(XOLM.hcs_from_context(intermediate_sentence), ncl).item():.3f}\" for ncl in range(1, 10)]))\n",
    "\n",
    "# This is what our Bellman updates look like (with a well-trained model, both sides should be roughly equal):\n",
    "prefix_hcs = XOLM.hcs_from_context(intermediate_sentence)\n",
    "for ncl in range(1, 10):\n",
    "    _, returns = expected_returns(prefix_hcs=prefix_hcs, nchars_left=ncl, value_function=vfa_overfit, agent_lm=XOLM)\n",
    "    print(vfa_overfit(prefix_hcs, ncl).item(), \"is Bellman-updated to come closer to\", returns[torch.argmax(returns)].item())\n",
    "\n",
    "# This is how the input features are used:\n",
    "print(vfa_overfit.linear1.weight[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7be5c628-dd47-4e04-9bc8-858675b71ef4",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Before peeking ahead, should the value function *increase* or *decrease* the further we progress through the sentence/trajectory? Why? $\\color{red}{\\text{FILL IN}}$"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "e5072acd-f61b-44af-a9e7-c05ec264541a",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Let's check for our example sentence, what the value function at every position is:\n",
    "# (we should see it go down to 0)\n",
    "for i in range(len(testsentence)):\n",
    "    hcs = XOLM.hcs_from_context(testsentence[:i])\n",
    "    ncl = len(testsentence)-i\n",
    "    loss = value_function_loss_for_state(\n",
    "        prefix_hcs=hcs,\n",
    "        nchars_left=ncl,\n",
    "        value_function=vfa_overfit,\n",
    "        agent_lm=XOLM\n",
    "    )\n",
    "    print(\"After\", i, \"characters, we have V =\", vfa_overfit(hcs, ncl).item(), \"incurring a loss of\", loss.item())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "278faa8e-ff91-4981-87e1-28acea004344",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Well here's a fun oddity. Most of the states have quite decent value function judgements it seems, but the negative numbers at the end look very wrong: and indeed the second-to-final state incurs a large loss. Why didn't we learn something better?\n",
    "\n",
    "The answer is simple: we almost never visited this state, so we never updated our value function here!  This sort of thing often happens in RL (and we're not going to try to fix it).  We can see that that is true when plotting the *stationary distribution* over states: run the agent a bunch of times and figure our which states were actually visited:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "1fd52198-715c-4a8e-b991-0349896467b0",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "stationary = Counter()\n",
    "\n",
    "for _ in range(50):\n",
    "    env = TypistState(string=testsentence, start_index=0)\n",
    "    _, agent = env.evaluate_agent(\n",
    "        agentclass=ValueFunctionExpectedReturnAgent,\n",
    "        lm=XOLM,\n",
    "        return_agent=True,\n",
    "        value_function=vfa_overfit,\n",
    "        exploration_policy=eps_greedy_policy\n",
    "    )\n",
    "    for d in agent.visited_cache:\n",
    "        stationary[d[\"nchars_left\"]] += 1\n",
    "\n",
    "for i in range(len(testsentence)):\n",
    "    print(f\"The state after {i:2} characters:\", '#' * stationary[len(testsentence) - i])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "9158f589-2bac-4125-bad9-e8124cf9a3b4",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We can see that there are some states that were visited very often, and some that are visited very rarely. Why is that?\n",
    "\n",
    "$\\color{red}{\\text{FILL IN}}$"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "a1f094b6-48be-4250-99de-f564d51cb8e9",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Alright, enough with this simple overfitting. It's time to train our agent on the real data distribution: every time it enters the environment, it has to guess a new sentence! Can we still do well?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "9a721994-8a6f-42e9-8369-c7b517240291",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def sample_generator(lm, temperature=0.2):\n",
    "    while True:\n",
    "        yield ''.join(itertools.islice(lm.greedy_sample(temperature=temperature), 30))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "49caf9c0-d869-4eda-b57c-572fcf80777c",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We will only train on 500 sentences, that should make us perform *okay* -- but feel free to train for longer to see just how far this agent gets!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "40289578-3722-42ee-85cf-aea2c9c216db",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Now train on some sentences from that generator\n",
    "vfa_all = train_vf(XOLM, itertools.islice(sample_generator(XOLM), 500), cache_gradients=5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "f6e563ec-ef00-46b8-9e4b-510512a50f6c",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Looks like it trained well! Now let's compare this new great agent and our old GreedyDecisionAgent on a multitude of sentences as well:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "a444bfdd-65ad-411f-b219-5580ed56939c",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def greedy_vs_vf_generator():\n",
    "    for sentence in sample_generator(XOLM):\n",
    "        reward_greedy = TypistState(string=sentence, start_index=0).evaluate_agent(\n",
    "            agentclass=FastGreedyExpectedImmediateRewardAgent,\n",
    "            lm=XOLM   \n",
    "        )\n",
    "        reward_vf = TypistState(string=sentence, start_index=0).evaluate_agent(\n",
    "            agentclass=ValueFunctionExpectedReturnAgent,\n",
    "            lm=XOLM,\n",
    "            value_function=vfa_all,\n",
    "            exploration_policy=lambda t: torch.argmax(t)\n",
    "        )\n",
    "        if abs(reward_vf - reward_greedy) > 4:\n",
    "            print(\"Remarkable difference: on\", sentence, \"we have greedy:\", reward_greedy, \"value functions:\", reward_vf)\n",
    "        yield (reward_greedy, reward_vf)\n",
    "\n",
    "def plot_agent_difference(reward_pair_generator):\n",
    "    list1, list2, list_diff = [], [], []\n",
    "    for i, (reward1, reward2) in enumerate(itertools.islice(reward_pair_generator, 100)):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            c1 = Counter(list1)\n",
    "            c2 = Counter(list2)\n",
    "            plt.bar(range(25), [c1[i] for i in range(25)], alpha=0.3)\n",
    "            plt.bar(range(25), [c2[i] for i in range(25)], alpha=0.3)\n",
    "            plt.legend([\"agent 1\", \"agent 2\"])\n",
    "            plt.show()\n",
    "            cd = Counter(list_diff)\n",
    "            plt.bar([i - 10 for i in range(21)], [cd[i-10] for i in range(21)])\n",
    "            plt.legend([\"2 - 1\"])\n",
    "            plt.show()\n",
    "        \n",
    "        list1.append(reward1)\n",
    "        list2.append(reward2)\n",
    "        list_diff.append(reward2 - reward1)\n",
    "    print(\"\\nsums:\", sum(list1), \"vs.\", sum(list2))\n",
    "\n",
    "plot_agent_difference(greedy_vs_vf_generator())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7d0af758-a6b2-46da-b255-0a4fb8de0d17",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Looks like the value functions help just a little bit over the greedy agent! In the final section, we will then ask the natural question: if we improve that little over our simple thing, what is all that hype about? But before, some small notes on how you could make this VF training better and faster.\n",
    "\n",
    "### Possible improvements\n",
    "\n",
    "1. **Update on minibatches** As with gradient descent, relying on single samples means instability and slow convergence -- so usually we would sample many trajectories before performing an update on our value function.\n",
    "2. **Unroll the Bellman equation** Instead of taking the value of the successor states as given by $V$ at face value, we could instead try to compute them too as the expectation over these states' successors (i.e., brute-force lookahead for another step). This yield more precise results at the cost of more computation.\n",
    "3. **Sample for the Bellman equation** We have been exploiting the fact that we have a model $p$ to take an expectation over next states that our action could get to.  This expectation may be slow to compute exactly, especially when we unroll the equation to multiple timesteps as above.  So we can instead approximate the expectation by sampling.  This leads to an interesting tradeoff between unrolling depth (which helps exactness) and a smaller number of samples for each decision on this unrolled depth (if we want to keep computation constant)."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "949c7a92-97e8-46aa-a85a-b0b9c63f1ce0",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## This sucks. Why would you do RL?\n",
    "\n",
    "What we found out here is that our greedy agent was able to take very good actions -- even with our complicated value function approximation of the entire game tree, we had a hard time doing better. The reason is simple: we supplied our agent with both knowledge of the reward function and the language model that the environment actually used!\n",
    "We basically handed our agent a complete (and correct!) model of the world -- a model in which, truly, search was all that was ever needed. So, really, instead of learning something about the environment (as one usually tried in RL), we just learned to search our existing perfect model of it. The fact that there are real trajectories with real rewards didn't matter to us at all.\n",
    "\n",
    "This is a rather uncommon scenario in RL.\n",
    "We've already hinted at the fact that things wouldn't go so well if we didn't give the agent a reward function that drips out sensible feedback on each iteration -- and it should be easy to see that if the language model that the agent has and the language model that the environment uses to choose $\\mathbf{w}$ differ, our greedy agent would not get away that easily.\n",
    "\n",
    "So, in this last section we will take away this perfect model and thus the capability of the agent to *reason* about what rewards it will get for any action. It will truly have nothing but the rewards it got from its current rollout to go on. Let's see how far this will get us. Can we still do better than the random agent (which is what the greedy agent essentially would fall back to in this case)?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "e3ae89d8-384d-4159-87db-789048a0374a",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "### Learning $Q$ without modeling $p$\n",
    "\n",
    "Say that we don't know $p$.  Where does our approach break if we no longer have this explicit environment model? We can identify two places:\n",
    "\n",
    "1. During the Bellman update: we need to find out what states $s'$ an action *could get us into* to not only evaluate whether that is the one we would take but also to then look up the value of that state $s'$ in $V$. If we can't make these inferences anymore, what signal can we rely on? Only the transitions $s \\overset{a}{\\rightarrow} s'$ that we actually observed in our roll-out phase!\n",
    "*Note that had we stuck to value function learning, our $\\epsilon$-greedy exploration policy would then be \"baked into\" the estimates for $V$! That is of course undesirable, since we want to use $V$ with our optimal `argmax` policy later on -- we want the value function to tell us what expected return we would get with that optimal policy, not the suboptimal $\\epsilon$-greedy policy. The method we will introduce, Q-learning, will solve that problem nicely: with Q-learning, you can take *any* policy for exploration -- as long as it is able to reach every state, you will get the same results (in the tabular case)! (Of course, this is not to say that learning value functions using rollout trajectories only is impossible. One classic algorithm, called [TD-Lambda](https://en.wikipedia.org/wiki/Temporal_difference_learning#TD-Lambda), has sucessfully been used back in 1992 to [solve Backgammon in an early win for RL in 1992](https://en.wikipedia.org/wiki/TD-Gammon).)*\n",
    "\n",
    "2. Both in the Bellman update and during the actual exploration of course, we still need to decide what action to take! If we would stick to value functions, that would mean learning some neural net for both $V$ and some policy $\\pi$, that, given $s$, tells us which $a$ to take. Why not fuse these two into one? Observe:\n",
    "\n",
    "We will learn a *$Q$-function* that given a state *and action* tells us the expected return, i.e., instead of querying $V(s)$ for a state $s$ we will now query $Q(s, a)$ for some state-action-pair $(s, a)$.\n",
    "If, again, we would have this function, a policy is trivial to derive:\n",
    "$\\pi(s) = \\mathrm{argmax}_{a \\in \\mathcal{A}} Q(s, a)$ -- no need to query the perfect environment model anymore.\n",
    "\n",
    "Most things stay the same as before, though. We again approximate $Q$ with a neural network, we will still use the hidden states of our pretrained LM as a convenient feature representation for this function approximator, and for the eventual proposal, we will still extract the 1-best strings for the proposed length from the pretrained language model (to keep things simple for this assignment)."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "24a0f934-9bf3-4806-b1c8-054fc759bddc",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "So let's go through the only thing that really changes: the Bellman update for this $Q$-function. We already stated that we are in a situation where instead of being able to expand the game tree, we only observe one trajectory:\n",
    "\n",
    "<img width=\"100%\" src=\"https://sjmielke.com/tmp/state_to_state_without_model.png\" />\n",
    "\n",
    "The image hints at the solution: for a given state $s$ we can always define $V(s)$ using the $Q$-function: the value of the state is the maximum value you can get from taking any action $a$ in this state, where the value of taking $a$ in $s$ is precisely what $Q$ gives us. As the new Bellman update we obtain a new estimate for $Q(s, a_1)$ (because we took $a_1$):\n",
    "\n",
    "$$\n",
    "    Q(s, a_1) = r_{1,3} + \\gamma V(s_{1,3}) = r_{1,3} + \\gamma \\max_{a' \\in \\{a_{1,3,1}, a_{1,3,2}\\}} Q(s_{1,3}, a')\n",
    "$$\n",
    "\n",
    "Note that even though the equation refers to $s_{1,3}$, it should really say $s_{1,?}$ as we have no idea what other states (here, $s_{1,1}$ and $s_{1,2}$) we could have landed in! So, more simply, if we sampled $s \\overset{a}{\\rightarrow} s'$ giving reward $r$ during exploration, this is the corresponding update:\n",
    "\n",
    "$$\n",
    "    Q(s, a) = r + \\gamma \\max_{a' \\,\\text{from}\\, s'} Q(s', a')\n",
    "$$\n",
    "\n",
    "All right! Let's implement it!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "14c17d00-de8b-43c1-9302-93b7c2e39cef",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class QFunctionApproximator(torch.nn.Module):\n",
    "    def __init__(self, h_dims):\n",
    "        super().__init__()\n",
    "        # This time we will not only feed the prefix hidden state but also\n",
    "        # the hidden state of the pretrained LM after reading in our proposal\n",
    "        # -- this will serve as a representation of the action a -- as well as\n",
    "        # the length and log-length of the proposed action.\n",
    "        self.linear1 = torch.nn.Linear(h_dims + 2 + h_dims + 2, 16)\n",
    "        self.linear2 = torch.nn.Linear(2 * 16 + 1, 1)\n",
    "        # We will again use target networks for stability.\n",
    "        self.linear1_target = torch.nn.Linear(h_dims + 2 + h_dims + 2, 16)\n",
    "        self.linear2_target = torch.nn.Linear(2 * 16 + 1, 1)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Set the target network weights to a new moving average.\n",
    "        \"\"\"\n",
    "        self.linear1_target.weight.data = 0.2 * self.linear1.weight.data + 0.8 * self.linear1_target.weight.data\n",
    "        self.linear2_target.weight.data = 0.2 * self.linear2.weight.data + 0.8 * self.linear2_target.weight.data\n",
    "        self.linear1_target.bias.data = 0.2 * self.linear1.bias.data + 0.8 * self.linear1_target.bias.data\n",
    "        self.linear2_target.bias.data = 0.2 * self.linear2.bias.data + 0.8 * self.linear2_target.bias.data\n",
    "\n",
    "    def forward(self, *, hcs_state, nchars_left, hcs_proposal, proposal_length, target=False):\n",
    "        \"\"\"\n",
    "        `hcs_state`: the state of the pretrained LM after reading the prefix\n",
    "        `hcs_proposal`: ~ after also reading the proposed action\n",
    "        `proposal_length`: length of the proposed extension\n",
    "        `nchars_left` and `target` same as before.\n",
    "        \"\"\"\n",
    "        if nchars_left == 0:\n",
    "            return torch.tensor(0.0)\n",
    "        n = float(nchars_left)\n",
    "        l = float(proposal_length)\n",
    "        inp = torch.cat(\n",
    "            [\n",
    "                hcs_state[-1][0][0],\n",
    "                torch.tensor([n, math.log(n)]),\n",
    "                hcs_proposal[-1][0][0],\n",
    "                torch.tensor([l, math.log(l)])\n",
    "            ]\n",
    "        )\n",
    "        hid = (self.linear1_target if target else self.linear1)(inp).tanh()\n",
    "        inp = torch.cat([hid, hid / n, torch.tensor([n])])\n",
    "        return (self.linear2_target if target else self.linear2)(inp).squeeze()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "80f5b1c5-b512-4a3b-9dff-dcd32a4237d1",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# This is how we would use it:\n",
    "prefix_hcs, nchars_left = LM.hcs_from_context(\"sequence\u2423modeling\u2423is\u2423\"), 8\n",
    "proposal = \"the\u2423best\"\n",
    "proposal_hcs = LM.hcs_from_context(\"the\u2423best\", hcs=prefix_hcs)\n",
    "Q = QFunctionApproximator(LM.lstm_layers[-1].hidden_size)\n",
    "Q(\n",
    "    hcs_state=prefix_hcs,\n",
    "    nchars_left=nchars_left,\n",
    "    hcs_proposal=proposal_hcs,\n",
    "    proposal_length=len(proposal)\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "0e81e9fb-6e20-4a4e-9c5d-8a67ba55f930",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def q_function_loss_for_state_action_reward_state(*, hcs_state1, nchars_left1, reward, hcs_state2, nchars_left2, q_function, agent_lm, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Both `hcs_state` and `nchars_left` have versions 1 and 2 for the two states\n",
    "    we need here. Since we chose to encode the action by its final LM hidden\n",
    "    state and its length, there is no need to feed in any more information about\n",
    "    it, as the former is simple `hcs_state2` and the latter can be calculated as\n",
    "    `nchars_left1 - nchars_left2`.\n",
    "    We do, however, need to pass in the immediate reward that we received.\n",
    "    Note that the `agent_lm` is only used to calculate all possible actions from\n",
    "    state2, not to do model-based calculations!\n",
    "    \"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "9d6cce5f-29c2-44a7-aa67-2b4d52ffed50",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# That is how we would use the function:\n",
    "q_function_loss_for_state_action_reward_state(\n",
    "    hcs_state1=prefix_hcs,\n",
    "    hcs_state2=proposal_hcs,\n",
    "    nchars_left1=nchars_left,\n",
    "    nchars_left2=nchars_left - len(proposal),\n",
    "    reward=8,\n",
    "    q_function=Q,\n",
    "    agent_lm=LM\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7c2b6a4e-24b2-474f-ae4a-1656c145b270",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class QFunctionExpectedReturnAgent(TextProposalAgent):\n",
    "    def __init__(self, lm, prefix, nchars_left, q_function, exploration_policy):\n",
    "        self.lm = lm\n",
    "        self.prefix = prefix\n",
    "        self.prefix_hcs = lm.hcs_from_context(prefix)\n",
    "        self.nchars_left = nchars_left\n",
    "        self.q_function = q_function\n",
    "        self.exploration_policy = exploration_policy\n",
    "        self.visited_cache = []\n",
    "\n",
    "    def receive_response(self, reward, response):\n",
    "        \"\"\"\n",
    "        This time we can only append a completed dict to the `visisted_cache` list!\n",
    "        \"\"\"\n",
    "        # Assemble tuple\n",
    "        hcs_state1 = self.prefix_hcs\n",
    "        hcs_state2 = self.lm.hcs_from_context(response, hcs=self.prefix_hcs)\n",
    "        nchars_left1 = self.nchars_left\n",
    "        nchars_left2 = self.nchars_left - len(response)\n",
    "        # Append\n",
    "        self.visited_cache.append(\n",
    "            {\n",
    "                \"hcs_state1\": hcs_state1,\n",
    "                \"hcs_state2\": hcs_state2,\n",
    "                \"nchars_left1\": nchars_left1,\n",
    "                \"nchars_left2\": nchars_left2,\n",
    "                \"reward\": reward\n",
    "            }\n",
    "        )\n",
    "        # Update internal state\n",
    "        self.prefix += response\n",
    "        self.prefix_hcs = hcs_state2\n",
    "        self.nchars_left = nchars_left2\n",
    "\n",
    "    def decision(self):\n",
    "        \"\"\"\n",
    "        No need to append to our cache here (unlike the VF agent), but do\n",
    "        remember to use the `self.exploration_policy`!\n",
    "        \"\"\"\n",
    "        hcs = tuple((hc[0].detach(), hc[1].detach()) for hc in self.prefix_hcs)\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b77dd8ef-9f5f-48cc-9418-05bc0b1dad86",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def train_qf(agent_lm, sentences, cache_gradients=10):\n",
    "    qf = QFunctionApproximator(agent_lm.lstm_layers[-1].hidden_size)\n",
    "    train(\n",
    "        agentclass=QFunctionExpectedReturnAgent,\n",
    "        params_to_optimize=list(qf.linear1.parameters()) + list(qf.linear2.parameters()),\n",
    "        agent_lm=agent_lm,\n",
    "        exploration_policy=eps_greedy_policy,\n",
    "        q_function=qf,\n",
    "        loss_function=q_function_loss_for_state_action_reward_state,\n",
    "        target_network_updater=qf.update_target_network,\n",
    "        sentences=sentences,\n",
    "        log_interval=50,\n",
    "        cache_gradients=cache_gradients\n",
    "    )\n",
    "    return qf"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "fdb704f9-fbf9-4b33-9823-05a1dd9fe504",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Again, test if we can overfit this one sentence. This time, we will probably need a lot more samples to get a reasonable estimate because we only learn from our trajectories. In real settings, \"more samples\" means millions and billions -- our code in this notebook already takes a long time for thousands. But maybe the simplicity of our problem makes this still feasible?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "3eac7e4a-5f79-4792-9a51-d7015b6f43a5",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "testsentence = '\u2423xxo\u2423x\u2423ooxo\u2423xxxooxx\u2423oxoxx\u2423xoxo'\n",
    "qf_overfit = train_qf(XOLM, [testsentence] * 1000)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "2bef4066-384a-41ff-8b45-c36f780be6f2",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Let's check for our example sentence, what the maximum Q value at every position is:\n",
    "# (we should see it go down to 0)\n",
    "for i in range(len(testsentence)):\n",
    "    hcs = XOLM.hcs_from_context(testsentence[:i])\n",
    "    ncl = len(testsentence)-i\n",
    "    hyps = XOLM.best_strings_from_hcs(hcs=hcs, max_length=ncl)\n",
    "    maxq = max(\n",
    "        [\n",
    "            qf_overfit(\n",
    "                hcs_state=hcs,\n",
    "                nchars_left=ncl,\n",
    "                hcs_proposal=end_hcs,\n",
    "                proposal_length=action_length\n",
    "            ).item()\n",
    "            for action_length, (_, _, end_hcs) in enumerate(hyps)\n",
    "            if action_length > 0\n",
    "        ]\n",
    "    )\n",
    "    print(\"After\", i, \"characters, we have a maximum Q of\", maxq)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "de93510d-7eb2-4521-a4ed-2d1ca593fb53",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "print(\"Random gets:\", TypistState(string=testsentence, start_index=0).evaluate_agent(agentclass=FastRandomLengthAgent, lm=XOLM))\n",
    "\n",
    "print(\n",
    "    \"After training, we get this reward using our argmax policy:\",\n",
    "    TypistState(string=testsentence, start_index=0).evaluate_agent(\n",
    "        agentclass=QFunctionExpectedReturnAgent,\n",
    "        lm=XOLM,\n",
    "        q_function=qf_overfit,\n",
    "        exploration_policy=lambda t: torch.argmax(t)\n",
    "    )\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "9cb4560b-0c72-4f30-b878-a4c775c69c33",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "You might have gotten lucky and learned something or not. If not, you can restart and try again or keep reading on. Deep reinforcement learning is very unstable and unpredictable!\n",
    "\n",
    "Also for the final test, we can again try to train on the true distribution:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "aa3aafb6-1b58-475a-bb94-be6564c9f761",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "qf_all = train_qf(XOLM, itertools.islice(sample_generator(XOLM), 2000))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "e57d990a-2fcf-48e5-8271-835df702b322",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Time for the final test: how well does it perform against the random agent overall?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "8183a9ba-4933-4977-820a-7b6b1a5d2838",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def random_vs_q_generator():\n",
    "    for sentence in sample_generator(XOLM):\n",
    "        reward_random = TypistState(string=sentence, start_index=0).evaluate_agent(\n",
    "            agentclass=FastRandomLengthAgent,\n",
    "            lm=XOLM\n",
    "        )\n",
    "        reward_qf = TypistState(string=sentence, start_index=0).evaluate_agent(\n",
    "            agentclass=QFunctionExpectedReturnAgent,\n",
    "            lm=XOLM,\n",
    "            q_function=qf_all,\n",
    "            exploration_policy=lambda t: torch.argmax(t)\n",
    "        )\n",
    "        if abs(reward_qf - reward_random) > 4:\n",
    "            print(\"Remarkable difference: on\", sentence,\n",
    "                  \"we have random:\", reward_random, \"vs. Q-functions:\", reward_qf)\n",
    "        yield (reward_random, reward_qf)\n",
    "\n",
    "%matplotlib inline\n",
    "plot_agent_difference(random_vs_q_generator())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "e64e83d4-3edf-4138-992c-fe10789be359",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "You should see some very unconvincing numbers: the agent doesn't perform too well.\n",
    "Why is that, you think? $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "*Note: we don't **know** the answer to that question for sure! But you can make some educated guesses -- or find a bug and make it work for extra credit? ;)*"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "77a03090-4990-41d1-99b0-38573f75533c",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "# Congrats! You've won the game.\n",
    "\n",
    "That's it for this assignment! If you want, play around with the reinforcement agent a bit more: try other policies and see how this has a bigger effect on $V$ than on $Q$, try to use more complicated networks, try to speed things up by batching... But probably, if you are to attempt to use RL \"in the wild\", you will -- like with most things in this class -- choose to write more task-specific and optimized code, but hopefully you have gotten a little glimpse of how reinforcement learning isn't all about robots moving -- but can be instructive to think about in other settings, too!"
   ]
  }
 ]
}