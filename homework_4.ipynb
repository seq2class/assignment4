{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "metadata": {
    "g.cell_uuid": "a3485bdf-7596-4b83-ad96-68cdf3330aab",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "# Homework 4 \u2014 A slightly different introduction to Deep Reinforcement Learning\n",
    "\n",
    "In this homework we will tackle an entirely new problem that will allow us to show off some reinforcement learning -- a setting where our actions *alternate* with responses from the environment, and the two influence each other."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "274f03ef-4d7c-4573-92f9-7b1dea2e16d4",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import urllib\n",
    "import urllib.request\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import torch\n",
    "\n",
    "from seq2class_previous_homeworks import StatefulTaskSetting, IncrementalScoringModel, BeamDecisionAgent, draw_tree"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "d952d326-38d7-4a4f-9eac-0b10e5d2ec1c",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "## The setting: predictive keyboards\n",
    "\n",
    "You are likely familiar with modern predictive keyboards from your smartphone.  Given the text you've typed so far, the keyboard proposes a next word; the user can accept or ignore this proposal. You should have seen this as a contemporary application of *language models* in NLP in Fall, and indeed you already built an FST-powered completion and prediction tool in that class.\n",
    "\n",
    "To keep things simple, we will assume that we *already have* some language model.  Furthermore, it's a language model over *characters*.  This gives us a conveniently small vocabulary.  It also allows us to ignore word boundaries: we will treat the space symbol as just another character.\n",
    "\n",
    "As we are not making any use of words as units, we will no longer be proposing the next *word*.  Proposing just the *single next character* would probably not speed up typing enough, so our keyboard will propose a sequence of *several characters*.  The user can decide whether to accept this proposed substring or ignore it.  The major question will be *how many* characters to propose at once."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "f5623f92-3fe1-4630-a289-6cf2feca0aab",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## Warm-Up: predicting strings from a language model trie\n",
    "\n",
    "Here is an example: assume the user typed \"`the defor`,\" what characters should we follow up with? Since we assume we already have a language model, let's assume this is what the trie that starts here looks like (omitting all the low-probability options):"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "88b81d9f-a475-4d27-adc1-106072d5f318",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "display(HTML(draw_tree({'e/0.4': {'s/1.0': {'t/0.7': {'a/1.0': {}}}}, 'm/0.6': {'a/0.5': {'t/1.0': {'i/1.0': {}}}, 'e/0.5': {'d/1.0': {'\u2423/1.0': {}}}}})))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "5cbafc56-9005-4445-8bbf-1e026bc93168",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "So the user might have wanted to type `deformed`, `deformation`, or `deforestation`. Given the very simple probabilities on the trie above, we can see what the most probable choice is if we want to propose a substring of 1, 2, 3, or 4 next characters:\n",
    "\n",
    "length | prediction\n",
    "---|---\n",
    "1 | `m`\n",
    "2 | `es`\n",
    "3 | `est`\n",
    "4 | `med_` / `mati`  (tie)\n",
    "\n",
    "The optimal longer strings do not all start with `m`.  Thus, we see that the best prediction of length 3, for example, cannot in general be obtained by greedily choosing the best arc at each trie vertex, but will require some *planning ahead*.  (This shouldn't be surprising.)\n",
    "\n",
    "How do we choose the length?  The user can only accept or ignore the entire proposed substring. Proposing a longer substring increases our *reward* (i.e., time saved over manual typing) if the user accepts -- but it also increases the chance that the proposal is wrong, in which case the user will be forced to ignore it and type the next character themselves.\n",
    "That is the tradeoff that we will have to navigate in this assignment.\n",
    "For now, we will assume that we do indeed have access to the full trie of the language model including all probabilities -- but at the end of the assignment we will see what we can still do even if we don't have this option to plan ahead.\n",
    "\n",
    "We will give you a very simple character-level English LM over the 27 characters `a`-`z` and `\u2423` (the space character).  For simplicity, we do not include an EOS symbol.  Much of the code should be familiar to you from HW3.  Rather than make you run the training code (which takes about 20 minutes per epoch on 100MB of text, using a GPU), we will hand you the weights from our own training run.  \n",
    "\n",
    "**Notes on the English language model:**\n",
    "\n",
    "Our trained model achieves about 1.5 bits per character of cross-entropy (vs. 1.08 bits for the state-of-the-art Transformer-XL model).  Our model is weak because it uses relatively few parameters, and we only trained for 1 epoch, without any kind of regularization.\n",
    "\n",
    "You'll see that the training procedure is a little different from last homework's models: we group the training examples into batches, which allows us to parallelize SGD and improves its stability.  \n",
    "\n",
    "As in HW3, we made the model more transparent by avoiding PyTorch's `LSTM` class (which trains an entire batch of sequences on the GPU) in favor of the `LSTMCell` class (which trains only a single time step for all the sequences).  This slows down training because we have to call the GPU repeatedly, once per time step, but it makes it somewhat more convenient to examine the trie probabilities via the `LSTMCell` class as we explore reinforcement learning.  Don't do it this way in a real-world setting where speed is important.\n"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "aa9a7668-32e4-418f-8baf-6a4abe34c992",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, layers=3, hidden_size=512, embedding_size=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # a simple character integerizer\n",
    "        self.idx2char = vocab\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        \n",
    "        # character embedding module\n",
    "        self.embedding = torch.nn.Embedding(len(vocab), embedding_size)\n",
    "        \n",
    "        # LSTM cells to run over character embeddings\n",
    "        # Note that it may be easier to just use an torch.nn.LSTM instead of\n",
    "        # manually gluing together cells like this.\n",
    "        self.lstm_layers = [torch.nn.LSTMCell(embedding_size, hidden_size)] \\\n",
    "            + [torch.nn.LSTMCell(hidden_size, hidden_size) for _ in range(layers - 2)] \\\n",
    "            + [torch.nn.LSTMCell(hidden_size, embedding_size)]\n",
    "        for i, layer in enumerate(self.lstm_layers):\n",
    "            self.add_module(f\"lstmlayer{i}\", layer)\n",
    "\n",
    "    def _hcs_from_cidx(self, hcs, c_idx):\n",
    "        c_emb = self.embedding(c_idx)\n",
    "        nhcs = [(c_emb, None)]\n",
    "        for hc, layer in zip(hcs, self.lstm_layers):\n",
    "            nhcs.append(layer(nhcs[-1][0], hc))\n",
    "        return nhcs[1:]\n",
    "        \n",
    "    def hcs_from_context(self, context_string, hcs=None, get_all_hcs=False):\n",
    "        \"\"\"\n",
    "        Sets up the language model hidden state given a string prefix\n",
    "        and, optionally, the hidden state before reading in this prefix.\n",
    "        \n",
    "        If context_string is a list, this performs batched computations,\n",
    "        interpreting context_string batch-first, timestep-second.\n",
    "        \"\"\"\n",
    "        if isinstance(context_string, str):\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            # \"Transpose\" to get it timestep-first, batch-second\n",
    "            batchsize = len(context_string)\n",
    "            context_string = [tuple(cs) for cs in itertools.zip_longest(*context_string)]\n",
    "        if hcs is None:\n",
    "            # initalize the hidden state of the LSTM\n",
    "            cs = [torch.zeros(batchsize, layer.hidden_size, device = self.embedding.weight.device) for layer in self.lstm_layers]\n",
    "            hcs = [(torch.tanh(c), c) for c in cs]\n",
    "        if get_all_hcs: all_hcs = [hcs]\n",
    "        # iterate over the string\n",
    "        for c in context_string:\n",
    "            c_idxs = torch.tensor([self.char2idx[c] for c in tuple(c)], device = self.embedding.weight.device)\n",
    "            hcs = self._hcs_from_cidx(hcs, c_idxs)\n",
    "            if get_all_hcs: all_hcs.append(hcs)\n",
    "        return all_hcs if get_all_hcs else hcs\n",
    "\n",
    "    def next_options(self, hcs = None, logprobs = False):\n",
    "        \"\"\"\n",
    "        Given the hidden state tuple of the LM, it returns a dictionary\n",
    "        mapping potential next characters to their probabilities.\n",
    "        \"\"\"\n",
    "        if hcs is None:\n",
    "            hcs = self.hcs_from_context(\"\")\n",
    "        probs = (hcs[-1][0] @ self.embedding.weight.t()).log_softmax(dim=-1)\n",
    "        if not logprobs:\n",
    "            probs = probs.exp()\n",
    "        l = [{c: p.item() for c, p in zip(self.idx2char, probs[b])}\n",
    "             for b in range(probs.size(0))]\n",
    "        # If no batching is used (i.e., batchsize == 1), return simple dict.\n",
    "        if len(l) == 1:\n",
    "            return l[0]\n",
    "        else:\n",
    "            return l\n",
    "    \n",
    "    def greedy_1_best(self, hcs = None):\n",
    "        \"\"\"\n",
    "        Infinite iterator returning the greedy choices of next character.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            hcs = self.hcs_from_context(\"\", hcs)  # initialize if needed\n",
    "            while True:\n",
    "                idx = torch.argmax(hcs[-1][0] @ self.embedding.weight.t())\n",
    "                yield self.idx2char[idx]\n",
    "                hcs = self._hcs_from_cidx(hcs, torch.tensor([idx], device = hcs[-1][0].device))\n",
    "\n",
    "    def greedy_sample(self, temperature = 0.5, hcs = None):\n",
    "        \"\"\"\n",
    "        Infinite iterator returning local samples of next character.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            hcs = self.hcs_from_context(\"\", hcs)  # initialize if needed\n",
    "            while True:\n",
    "                weights = (hcs[-1][0] @ self.embedding.weight.t() / temperature).exp().squeeze(0)\n",
    "                [char] = random.choices(self.idx2char, weights = weights)\n",
    "                yield char\n",
    "                hcs = self._hcs_from_cidx(hcs, torch.tensor([self.char2idx[char]], device = hcs[-1][0].device))\n",
    "    \n",
    "    def render_trie_from(self, hcs, depth=2, topk=-1, prob_gt=-1):\n",
    "        assert topk < 0 or prob_gt < 0\n",
    "        def trie_from(hcs, depth):\n",
    "            if depth == 0:\n",
    "                return {}\n",
    "            else:\n",
    "                expanded_cs = list(self.next_options(hcs).items())\n",
    "                if topk > 0:\n",
    "                    expanded_cs = sorted(expanded_cs, key=lambda x:-x[1])[:topk]\n",
    "                if prob_gt > 0:\n",
    "                    expanded_cs = [(c, p) for c, p in expanded_cs if p > prob_gt]\n",
    "                return {f\"{c}/{p:.2f}\":\n",
    "                        trie_from(self.hcs_from_context(c, hcs=hcs), depth - 1)\n",
    "                        for c, p in sorted(expanded_cs, key=lambda x: x[0])}\n",
    "        display(HTML(draw_tree(trie_from(hcs, depth))))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "333d16a1-bcd3-4951-ab86-2eff030f706a",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# This will train the language model (don't execute this, it would take too long!):\n",
    "\n",
    "def train_lm(name, dataset, nlayers, nhid, embsize, BATCHSIZE, BPTTLENGTH):\n",
    "    STUB = f\"{name}_layers{nlayers}_hs{nhid}_emb{embsize}_bs{BATCHSIZE}_bptt{BPTTLENGTH}_adam1e-3_epochs1\"\n",
    "    start_time = time.time()\n",
    "    with open(STUB+\".log\", 'wt') as logfile:\n",
    "        print(STUB)\n",
    "        print(STUB, file=logfile)\n",
    "        vocab = \"\".join(sorted(list(set(dataset))))\n",
    "        LM = LanguageModel(vocab, layers=nlayers, hidden_size=nhid, embedding_size=embsize).cuda()\n",
    "\n",
    "        total_length = len(dataset) // BATCHSIZE\n",
    "        batches = [\n",
    "            [\n",
    "                dataset[total_length * i + BPTTLENGTH * batch_nr\n",
    "                      : total_length * i + BPTTLENGTH * (batch_nr + 1)]\n",
    "                for i in range(BATCHSIZE)\n",
    "            ]\n",
    "            for batch_nr in range(total_length // BPTTLENGTH)\n",
    "        ]\n",
    "\n",
    "        hcs = LM.hcs_from_context([\"\"] * BATCHSIZE)\n",
    "        optimizer = torch.optim.Adam(LM.parameters(), lr=1e-3)\n",
    "\n",
    "        nll_sum = 0\n",
    "        for i_batch, batch in enumerate(batches):\n",
    "            hcs = [(h.detach(), c.detach()) for (h, c) in hcs]\n",
    "            nll = 0\n",
    "            for i in range(BPTTLENGTH):\n",
    "                # These chars at this timestep\n",
    "                charidxs = torch.tensor([LM.char2idx[batch[r][i]] for r in range(BATCHSIZE)], device = LM.embedding.weight.device)\n",
    "                # Predict\n",
    "                lps = (hcs[-1][0] @ LM.embedding.weight.t()).log_softmax(dim=-1)\n",
    "                nll += -lps.gather(dim=-1, index=charidxs.unsqueeze(-1)).sum()\n",
    "                # Next timestep\n",
    "                hcs = LM._hcs_from_cidx(hcs, charidxs)\n",
    "            # Calculate gradients\n",
    "            optimizer.zero_grad()\n",
    "            nll.backward()\n",
    "            # Debug output\n",
    "            nll_sum += nll.detach().item()\n",
    "            if i_batch % 50 == 0:\n",
    "                bpc = (nll_sum / ((50 if i_batch > 0 else 1) * BATCHSIZE * BPTTLENGTH)) / math.log(2)\n",
    "                nll_sum = 0\n",
    "                greedy = ''.join(itertools.islice(LM.greedy_1_best(LM.hcs_from_context(\"\")), 70))\n",
    "                sample = ''.join(itertools.islice(LM.greedy_sample(LM.hcs_from_context(\"\")), 70))\n",
    "                print(f\"{i_batch:5}/{len(batches)} {bpc:9.2f} -> {greedy} ~~ {sample}\")\n",
    "                print(f\"{i_batch:5}/{len(batches)} {bpc:9.2f} -> {greedy} ~~ {sample}\", file=logfile)\n",
    "            # Apply gradients\n",
    "            optimizer.step()\n",
    "\n",
    "        torch.save(LM.state_dict(), STUB+\".statedict.pt\")\n",
    "        print(\"Trained in\", time.time() - start_time, \"seconds.\")\n",
    "        print(\"Trained in\", time.time() - start_time, \"seconds.\", file=logfile)\n",
    "        start_time = time.time()\n",
    "        _ = ''.join(itertools.islice(LanguageModel.greedy_sample(LM), 1000))\n",
    "        print(\"Sampled 1000 chars in\", time.time() - start_time, \"seconds.\")\n",
    "        print(\"Sampled 1000 chars in\", time.time() - start_time, \"seconds.\", file=logfile)\n",
    "\n",
    "should_we_retrain_the_language_model = \"no\"\n",
    "\n",
    "if should_we_retrain_the_language_model == \"hell yeah\":\n",
    "    if not os.path.isfile(\"text8.zip\"):\n",
    "        import urllib\n",
    "        urllib.request.urlretrieve(\"http://mattmahoney.net/dc/text8.zip\", \"text8.zip\")\n",
    "    if not os.path.isfile(\"text8\"):\n",
    "        import zipfile\n",
    "        zip_ref = zipfile.ZipFile(\"text8.zip\", 'r')\n",
    "        zip_ref.extractall(\".\")\n",
    "        zip_ref.close()\n",
    "\n",
    "    with open(\"text8\", 'r') as f:\n",
    "        text8 = f.read()\n",
    "    \n",
    "    train_lm(\"text8\", text8, nlayers = 2, nhid = 512, embsize = 16, BATCHSIZE = 80, BPTTLENGTH = 150)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "3f761e9d-0a61-4a6d-87ef-652f623d1638",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Just to illustrate how this class is supposed to work, let's run it on our example above:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "da6fe40f-69b6-4bfd-8d91-f3b249e716bc",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Load our pretrained models\n",
    "LM = LanguageModel(\"abcdefghijklmnopqrstuvwxzy\u2423\", layers=3, hidden_size=512, embedding_size=16).cpu()\n",
    "LM.load_state_dict(torch.load(\"text8.lm.statedict.pt\", map_location='cpu'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "d421eaea-5467-4852-b352-7c49fd6d0e0d",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# A sample from the LM (at local temperature T=0.5)\n",
    "print(''.join(itertools.islice(LM.greedy_sample(temperature=0.5), 100)))\n",
    "\n",
    "# Using no batches\n",
    "LM.next_options(LM.hcs_from_context(\"the\u2423defor\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "bb9e13ed-47c5-40bc-9d46-9c7be312ad77",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Let's see what the model's trie looks like for our example context given above! We'll render it only to depth 4 and omit all expansions with probability $p(x_{i+1}|\\mathbf{x}_{<i}) \\le .18$ to keep things readable:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "709664d6-7725-4709-973f-97b95cf31506",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "the_defor = LM.hcs_from_context(\"the\u2423defor\")\n",
    "LM.render_trie_from(hcs=the_defor, depth=4, prob_gt=.18)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "17e6945c-37a0-4cab-84c5-5062e8fbdbb1",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Notice that even with the threshold, this LSTM allows some words like \"defores\" (not actually English) and \"deformer\" that we didn't anticipate.  Now, given this trie, let's try to reproduce a table like the one given above that shows the *single most likely prediction* for each length:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "4ef47f89-d538-4a59-a4e4-e451a7b896da",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def brute_force_get_best_string(lm, hcs, length):\n",
    "    \"\"\"\n",
    "    Returns the string and its log-probability.\n",
    "    You can solve this by recursion using `lm.next_options(..., logprobs=True)`.\n",
    "    \"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "d9e71c9b-de66-4184-9bcd-830f7a4e2ec2",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "for l in range(1, 3):\n",
    "    s, lp = brute_force_get_best_string(LM, the_defor, l)\n",
    "    print(f\"{l:2}: {s:5} (p={math.exp(lp)})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "2bef5e80-fd66-4b43-af24-19c60006aedc",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "You can try counting up to lengths larger than 2, but you should quickly see that even a length-3 string takes too long to brute-force maximize. But you have already learned a technique to perform (approximate) maximization in these locally normalized models: beam search!\n",
    "Let's try to apply that here!\n",
    "\n",
    "First we need to recast our problem of finding the best string in the framework of `TaskSetting`s and `ProbabilityModel`s, specifically, the `StatefulTaskSetting` and `IncrementalScoringModel`, as we did in the last homework:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "dfe1b141-1b66-47fc-b454-44ae5d98a76c",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class PredictStringTask(StatefulTaskSetting):\n",
    "    \"\"\"\n",
    "    The task predicts strings of a certain length.\n",
    "    The internal \"state\" will only be the length of the string that is\n",
    "    still to be generated (because this is sufficient for the structural zeros).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, length):\n",
    "        super().__init__()\n",
    "        self.vocab = tuple(vocab)\n",
    "        self.length = length\n",
    "    \n",
    "    def initial_taskstate(self, *, xx):\n",
    "        return self.length\n",
    "    \n",
    "    def next_taskstate(self, *, xx, a, taskstate):\n",
    "        return taskstate - 1\n",
    "    \n",
    "    def iterate_y(self, *, xx, oo=None, yy_prefix):\n",
    "        assert oo is None # let's not deal with that here\n",
    "        if yy_prefix > 0:\n",
    "            yield from self.vocab\n",
    "        else:\n",
    "            yield None\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7be986d9-5ff1-4880-a2d8-5135a98d0052",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# It yields all sequences, as expected:\n",
    "list(PredictStringTask(\"XY\", 2).iterate_aa(xx=None))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b9737b43-22da-4b27-b3ac-09d3b8090762",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class LMScorer(IncrementalScoringModel, torch.nn.Module):\n",
    "\n",
    "    def __init__(self, task, lm):\n",
    "        # Always initialize the PyTorch module first, so the registration hooks work!\n",
    "        torch.nn.Module.__init__(self)\n",
    "        super().__init__(task)\n",
    "        self.lm = lm\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"\n",
    "        Since all the magic happens in `self.lm`, the `LanguageModel`, nothing here.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def initial_modelstate(self, *, xx):\n",
    "        \"\"\"\n",
    "        Initialize the LM hidden states with `xx`, our preceding context.\n",
    "        We will also store the current predictions in the model state to share\n",
    "        them across different to-be-probed actions.\n",
    "        \"\"\"\n",
    "        hcs = self.lm.hcs_from_context(xx)\n",
    "        preds = self.lm.next_options(hcs, logprobs=True)\n",
    "        return (hcs, preds)\n",
    "    \n",
    "    def score_a_s(self, *, xx, a, taskstate, modelstate):\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "3e3df285-1725-4e29-affa-53272123234c",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Let's see what predictions we would make for lengths 1, 2, 3, ..., 10 (and whether they're consistent with the predictions we made earlier):"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "1c44a8a6-2341-4ade-a8a6-a772298d8972",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def best_string(self, *, prefix, length, beam_size=5):\n",
    "    scorer = LMScorer(PredictStringTask(self.idx2char, length), self)\n",
    "    agent = BeamDecisionAgent(scorer, beam_size=beam_size)\n",
    "    return ''.join(agent.decision(xx=prefix))\n",
    "\n",
    "# Patch it into the language model\n",
    "LanguageModel.best_string = best_string\n",
    "\n",
    "print(\"Greedy search on our old example\")\n",
    "for l in range(1, 10):\n",
    "    s = LM.best_string(prefix=\"this\u2423is\", length=l, beam_size=1)\n",
    "    print(f\"{l:2}: {s:5}\")\n",
    "print(\"Greedy search on a new example\")\n",
    "for l in range(1, 10):\n",
    "    s = LM.best_string(prefix=\"this\u2423is\", length=l, beam_size=1)\n",
    "    print(f\"{l:2}: {s:5}\")\n",
    "print(\"Beam search (beam size 10) on the same example\")\n",
    "for l in range(1, 10):\n",
    "    s = LM.best_string(prefix=\"this\u2423is\", length=l, beam_size=10)\n",
    "    print(f\"{l:2}: {s:5}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "59d81e02-ba95-4016-83e2-5e36150307b9",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## The \"real\" task (which we won't be solving)\n",
    "\n",
    "Now let's try to formalize the overarching task.  Suppose our user is partway through entering the sentence $\\mathbf{w} = $`sequence\u2423modeling\u2423is\u2423the\u2423best`.   A dialogue might go like this:\n",
    "\n",
    "observed state | a few examples of possible actions | agent chooses | environment responds | reward (keystrokes saved)\n",
    "-|-|-|-|-\n",
    "`sequence\u2423modeling\u2423is\u2423` | `a` , `th` , `the` , `the\u2423` , `the\u2423s` , `the\u2423co` , `the\u2423mos` , `the\u2423most` | `the\u2423` | accept | $4-1 = 3$\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423` | `s` , `co` , `fir` , `most` | `co` | `b` | $1-1 = 0$\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423b`  | `e` , `es` , `est` | `est`| accept | $3-1 = 2$\n",
    "`sequence\u2423modeling\u2423is\u2423the\u2423best` | $\\Rightarrow$ This is a final state, so we can take no more actions and get no more reward. | &nbsp; | &nbsp; | $0$\n",
    "\n",
    "In principle, an agent for this task could suggest any string to the user; the table above shows just a few possible actions at each time step. Our total score accross all timesteps was $3 + 0 + 2 = 5$. We of course want this score to be as high as possible.\n",
    " \n",
    "Intuitively, what would a good approach to this task look like? First of all, we definitely don't want to suggest *unlikely* strings - a language model will come in handy here. Secondly, we want to suggest a lot of characters at a time, to maximize the keystrokes saved. But we need to trade this off against the chance that the user actually accepts our suggestion, so we probably don't want to suggest strings that are *too* long. \n",
    "\n",
    "### Formalizing the task: Markov Decision Processes\n",
    "\n",
    "In order to map the abstract game described above into a more formal setting, we define a couple of terms.\n",
    "\n",
    "The *state space* is the set of all possible states that the agent could be in. In this case, the state space is the set of all strings in our vocabulary, which correspond to possible values of the prefix that has already been entered. There is also an *initial state distribution*, which in our case places probability 1 on the prefix `\"\"`.\n",
    "\n",
    "The *action space* is the set of all actions that can be taken by the agent in any state. In this task, we can suggest any string to the user, so our action space is the set of all strings in our vocabulary. (Note that it's  unusual that the state space is the same as the action space, they are generally different.)\n",
    "\n",
    "The *transition function* is a function that takes in a state and an action, and outputs a sample from a distribution over states. In our case, the transition function is essentially the user of the predictive keyboard: after we take an action, suggesting some autocompletion, the user either accepts that suggestion (transitioning us to the state consisting of our old state concatenated with our suggested autocompletion), or types another character instead (transitioning us to the state consisting of our old state concatenated with one new character). Or, the user presses \"send\" on their message, transitioning us to the *terminal state*: the state in which no futher actions are possible.\n",
    "\n",
    "An *episode* is a sequence of state->action->state->action->....->terminal state.\n",
    "\n",
    "The *reward* is the score given in response to taking a particular action at a particular timestep. The *return* is the sum of all rewards in an episode. These two terms are similar, so take note of the difference! In our case, the reward is the number of characters saved by the user in response to a particular suggestion, and the return is the total number of characters saved over the whole episode.\n",
    "\n",
    "Together, the state space, initial state distribution, action space, reward function, and transition function define an *environment*. An *agent* exists in an environment and interacts with it. The agent can have many components, but one important component that every agent has is a *policy*.\n",
    "\n",
    "The *policy* is the function which maps from states to actions. Our overall task-independent goal can be stated as follows: create an agent whose policy has high expected return. In this case, we are looking to create an agent whose policy for suggesting autocompletions saves the user the most total keystrokes.\n",
    "\n",
    "## Simplifying things for the homework\n",
    "\n",
    "Unfortuately, the problem setting as described above is a bit too tough for a homework assignment. Therefore, we are going to simplify the problem in several ways (some of which are unrealistic):\n",
    "\n",
    "We assume that the user is generating their strings according to some language model **p\\***. The agent may or may not know **p\\***.\n",
    "\n",
    "For any given episode, we assume oracle knowledge of the total length of the complete string that the user wants to enter. Additionally, we \"summarize\" any given state of the environment by a single dense vector $\\mathbf{h}$, namely the hidden state of an pre-trained LSTM language model that has read the already-typed prefix associated with that state. \n",
    "\n",
    "Thus, our state space is a 2-tuple: $s = (\\mathbf{h},k) =$ (prefix_vector, nchars_left).  Our policy will choose its action based only on this 2-tuple.  We will also predict the environment's response based only on that 2-tuple.  This is a kind of conditional independence assumption.\n",
    "\n",
    "(Implementation note: in some places in the code, we prefer to view the state as the 2-tuple (prefix_string, nchars_left), instead. Therefore, we always actually pass around the 3-tuple (prefix_string, prefix_vector, nchars_left), and only ever use either one or the other of the first two.)\n",
    "\n",
    "(Including \"nchars_left\" is completely unrealistic and shouldn't really be necessary.  We included it in the state only because our pre-trained LM was not trained on a collection of actual text messages ending in EOS , but rather on a single very long string consisting of many concatenated documents.  So it unfortunately couldn't ever learn how to predict EOS: $\\mathbf{h}$ does not include *any* information about how close to the end of the user's string we are. Therefore, we cheated and augmented it with *perfect* information about this.)\n",
    "\n",
    "We also significantly reduce our action space by assuming that we have an oracle language model that we can use, in any given state, to estimate the most probable choice of the next $a$ characters, using beam search.  We now restrict our action set to $\\{ a : 1 \\leq a \\leq 10 \\}$.  In other words, there are 10 possible actions -- we must suggest the estimated most probable string of length 1, or of length 2, or ... you get the point.\n",
    "\n",
    "If the user is close to the end of the string, and the agent suggests a longer string than the remaining characters that the user wants to type, the environment will reject the suggestion. However, the environment is lenient regarding final spaces: if the suggestion is longer than the user's string, but all of the extra characters are spaces (which happens often), the user will still accept the suggestion. \n",
    "\n",
    "To start off, the reward function will be as described above: the number of characters saved by suggesting some autocompletion. Note that this means that the reward scales with the size of the suggested string, but only if the suggestion is accepted: if the suggestion is rejected, the reward is 0 regardless of what character the user types. (This reward function is pretty intuitive and easy to work with. Later on in the homework, we will also consider the case where the reward function is less \"nice\".)"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "0ea744d7-05a2-4460-ae4e-46f3c6f71317",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "## The actual environment (the typist)\n",
    "\n",
    "Let's try to build an implementation for this scenario now. We will start with:\n",
    "1. A definition of the environment, or more specifically, the environment in a specific state. It will, given this state, look at the action that the agent chose, and return the reward that the agent obtains for this action, as well as sample the output of the transition function and return the next state. (Also, it will update its own internal state to the next state.)\n",
    "2. An `RLAgent`, that represents our agent and its policy: given a state, it decides on what action to take."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "e73260c9-537c-4781-8089-f93706a31f0f",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class RLAgent(object):\n",
    "    \"\"\"\n",
    "    Note that this RLAgent is similar to the DecisionAgent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def decision(self, *, state):\n",
    "        \"\"\"\n",
    "        Makes a decision, based on the state that it sees.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def receive_response(self, *, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Updates the agent's internal state using the response it received from\n",
    "        the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "dcb2f258-bfb8-4cd0-8a31-1c8169f78d98",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class EnvironmentState(object):\n",
    "    \"\"\"\n",
    "    Representing a specific state our environment is in for the current episode.\n",
    "    Note that we explicitly make this object stateful to highlight that we\n",
    "    cannot just take any action free of consequences.\n",
    "    \"\"\"\n",
    "        \n",
    "    def execute_action(self, *, action):\n",
    "        \"\"\"\n",
    "        Returns the reward for an action and the new state that the agent\n",
    "        has entered. Also, updates the current state of the environment.\n",
    "        Needs not be deterministic.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def evaluate_agent(self, *, agent=None, agentclass=None):\n",
    "        \"\"\"\n",
    "        Convenience method, executing an entire episode by running the agent.\n",
    "        If passed an `agent`, it will just use it, if passed an `agentclass`, it\n",
    "        will try to instantiate that class in some way and then use it.\n",
    "        Returns the total reward, (aka the return).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "d682a9c7-2b86-491f-b836-6e6262cc016f",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "For implementational convenience and perhaps even realism, our environment is an idealized user who starts out by randomly sampling a *particular* string $\\mathbf{w}$ that they want to type.  This string does not change as the agent acts.  The subsequent accept/reject decisions by the environment do depend on the agent's actions, but without any further randomness - they are fully determined by the agent's actions together with the previously sampled string $\\mathbf{w}$.\n",
    "\n",
    "## The agent's (imperfect) model of the environment\n",
    "\n",
    "Since $\\mathbf{w}$ is a hidden (latent) part of the environment's true state $\\bar{s}$, the true setting is a POMDP setting.  But we will, for simplicity, construct our agent anyway as if the setting were an MDP with simpler states $s$ of the form $s = (\\mathbf{h},k) =$ (prefix_vector, nchars_left) that the agent actually observes.\n",
    "\n",
    "(What are the consequences of this simplification?  The agent doesn't know the true POMDP state  $\\bar{s}$.  So it doesn't actually have enough information to determine the true values $\\bar{Q}(\\bar{s},a)$ of the different actions, even if it had the ability to do exhaustive search over the possible future rollouts.  Instead the agent is under the mistaken impression that $s$ is all it needs to know to choose its action.  As a result, its policy can only depend on the prefix typed so far, and not on the history of actions and observables that constructed that prefix.  *Example:* Suppose the user rejects the suggestion `that` by typing `t`.  The user would now *certainly* reject `hat` as the next suggestion, but the MDP agent will fail to realize that!  In contrast, a POMDP agent would be able to update its belief about the unknown state $\\mathbf{s}$ to reflect the new evidence that $\\mathbf{w}$ (whatever it may be!) clearly wasn't supposed to continue with `that`, and hence it now does not continue with `hat` following the `t`.)\n",
    "\n",
    "For now, let's assume that our agent does not have to do any learning because it is given a pretty good model of the environment, in other words, a model $\\hat{p}(s' \\mid s,a)$ and $\\hat{r}(s, a, s')$.  Here $s'$ represents the predicted next state of the MDP,  and $\\hat{r}$ is the associated reward if that is the next state.   The action $a$ is the number of characters in the substring that the agent proposes; it will always correspond to the beam-search approximation of the maximum-likelihood string of length $a$ given context $s$, where the likelihood is computed under **p\\***.  \n",
    "\n",
    "Specifically, the agent's environment model assumes that the probability that the environment will accept the suggested string of length $a$, and append it to the current prefix, equals the probability according to a language model that the next $a$ characters following the current prefix would in fact be the suggested string.  This environment model might be bad if the agent's language model is not the actual distribution **p\\*** from which the environment sampled $\\mathbf{w}$.  But for now, let's allow the agent to use **p\\*** as its language model.\n",
    "\n",
    "In addition, let's say that the agent knows the true reward structure, so $\\hat{r}$ gives the same reward for acceptance as the real environment would."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "ba29493d-e478-45f3-82a8-6dae505c8216",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class TypistState(EnvironmentState):\n",
    "    def __init__(self, *, lm, string=None, start_index=0, total_length=10, debug=False): \n",
    "        self.lm = lm\n",
    "        self.debug = debug        \n",
    "        self.reset(string=string, start_index=start_index, total_length=total_length)\n",
    "\n",
    "    def reset(self, *, string=None, start_index=0, total_length=20):\n",
    "        \"\"\"\n",
    "        Our state will be the already-typed part of the string.\n",
    "        \"\"\"\n",
    "        if string is None:\n",
    "            self.string = ''.join(itertools.islice(self.lm.greedy_sample(temperature=0.2), total_length))\n",
    "        else:\n",
    "            self.string = string\n",
    "        self.current_index = start_index\n",
    "        self.all_hcs = self.lm.hcs_from_context(self.string, get_all_hcs=True)\n",
    "        return self.current_state\n",
    "\n",
    "    @property\n",
    "    def current_prefix(self): return self.string[:self.current_index]\n",
    "    @property\n",
    "    def current_hcs(self): return self.all_hcs[self.current_index]\n",
    "    @property\n",
    "    def current_nchars_left(self): return len(self.string) - self.current_index\n",
    "    @property\n",
    "    def current_state(self):\n",
    "        return (self.current_prefix, self.current_hcs, self.current_nchars_left)\n",
    "\n",
    "    def get_suggestion(self, *, action):\n",
    "        return self.lm.best_string(prefix=self.current_prefix, length=action)\n",
    "    \n",
    "    def successful_prediction_reward(self, action): return action - 1\n",
    "    def failed_prediction_reward(self, action): return 0\n",
    "    \n",
    "    def execute_action(self, *, action):\n",
    "        \"\"\"\n",
    "        Given the action (a proposed string) check whether it is \"correct\"\n",
    "        and reward accordingly.\n",
    "        \"\"\"\n",
    "        assert 0 < action <= 10\n",
    "        # What are we looking for?\n",
    "        goldanswer = self.string[self.current_index : self.current_index + action]\n",
    "        # What did we suggest?\n",
    "        suggestion = self.get_suggestion(action=action)\n",
    "        if action > self.current_nchars_left:\n",
    "            # crop ending spaces\n",
    "            action -= len([c for c in suggestion[self.current_nchars_left:] if c == '\u2423'])\n",
    "            suggestion = suggestion[:self.current_nchars_left] + ''.join([c for c in suggestion[self.current_nchars_left:] if c != '\u2423'])\n",
    "        # Check what happens\n",
    "        if self.debug:\n",
    "            print(\"If we have \\\"\" + self.string[:self.current_index]\n",
    "                  + \"\\\" and still need \\\"\" + self.string[self.current_index:]\n",
    "                  + \"\\\", the proposition \\\"\" + suggestion, end='\" ')\n",
    "        if suggestion == goldanswer:\n",
    "            # Advance the task state that far\n",
    "            self.current_index += action\n",
    "            reward = self.successful_prediction_reward(action)\n",
    "            if self.debug:\n",
    "                print(\"is correct!\", end=' ')\n",
    "        else:\n",
    "            # Only advance by one\n",
    "            self.current_index += 1\n",
    "            reward = self.failed_prediction_reward(action)\n",
    "            if self.debug:\n",
    "                print(\"is incorrect (\\\"\" + goldanswer + \"\\\" would have been correct)!\", end=' ')\n",
    "        if self.debug:\n",
    "            print(\"We get reward\", reward, \"and new state \\\"\" + self.current_prefix + \"\\\".\")\n",
    "        return reward, self.current_state\n",
    "    \n",
    "    def evaluate_agent(self, *, agent=None, agentclass=None, return_agent=False, reset=False, **kwargs):\n",
    "        assert agent is None or agentclass is None\n",
    "        if agent is None:\n",
    "            agent = agentclass(\n",
    "                **kwargs  # this will be the way to pass the LM\n",
    "            )\n",
    "            \n",
    "        total_reward = 0\n",
    "        if self.debug:\n",
    "            print(f'Starting with state \"{self.string[:self.current_index]}>{self.string[self.current_index:]}\":')\n",
    "\n",
    "        if reset: self.reset()\n",
    "        while self.current_index < len(self.string):\n",
    "            # Get action\n",
    "            state = self.current_state\n",
    "            action = agent.decision(state=state)\n",
    "            if self.debug:\n",
    "                print(f'Agent chooses to predict {action}!')\n",
    "            # Use reward and response\n",
    "            reward, next_state = self.execute_action(action=action)\n",
    "            agent.receive_response(state=state, action=action, reward=reward, next_state=next_state)  # We will define this below.\n",
    "            total_reward += reward\n",
    "            if self.debug:\n",
    "                print(f'User gave reward {reward} and response \"{next_state[0]}\"')\n",
    "        if self.debug:\n",
    "            print(\"Received\", total_reward, \"total reward!\")\n",
    "        return total_reward if not return_agent else (total_reward, agent)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "94a21844-57cd-4734-b611-bc20e18320d2",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Let's see what our possible actions are in the initial state of the run that we saw above, and in some possible subsequent states.  For each action, we'll also see what the environment *would* do if the agent took that action.  Of course, the agent will only get to take *one* of the actions."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "2949e206-4bf3-4daa-ba90-31596ccb928e",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "for nchars in range(21, 25):\n",
    "    for action in range(1, 5):\n",
    "        # Set up an environment just to where we can test the action...\n",
    "        env = TypistState(lm=LM, string=\"sequence\u2423modeling\u2423is\u2423the\u2423best\", start_index=nchars, debug=True)\n",
    "        env.execute_action(action=action)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "e0fa180a-e0b3-497a-916b-ceb43d72c9ea",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## A first agent: greedily choosing actions\n",
    "\n",
    "A first very simple agent is going to use its model to greedily choose the best action. Since our reward function gives meaningful rewards at every step, it's a sensible heuristic to always pick the action that gives the best expected immediate reward.  (This is not optimal, however, because it doesn't consider how the action affects the next state $s'$: getting to a good state $s'$ could set us up for big future rewards.  We'll see the impact of that later.)\n",
    "\n",
    "Equipped with this model, the agent can compute the *expected immediate reward* of each action $a$.   In our case, that is the reward that the agent will receive if the user accepts $a$ times the model probability that the user will accept $a$, plus a reward of 0 times the model probability that the user will not accept $a$ but instead will type the next character."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "94581d01-fa6e-40dd-a09b-90ae70cfc17b",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def expected_immediate_reward(*, prefix, nchars_left, action, agent_lm, success_reward_fn, fail_reward_fn):\n",
    "    \"\"\"\n",
    "    The prefix is the string that we observed so far, the LM is the\n",
    "    (potentially inadequate) LM of the agent.\n",
    "    \"\"\"\n",
    "    # Return 0 if no chars left\n",
    "    if nchars_left == 0: return torch.tensor(0)\n",
    "    # What would the probability of the string we would give be, i.e.,\n",
    "    # what is the probability that it is correct?\n",
    "    scorer = LMScorer(PredictStringTask(agent_lm.idx2char, action), agent_lm)\n",
    "    aa = agent_lm.best_string(prefix=prefix, length=action)\n",
    "    prob = scorer.score_aa(xx=prefix, aa=aa).exp()\n",
    "    # What would we get if it was indeed correct?\n",
    "    # We assume knowledge of the reward function here of course.\n",
    "    positive_reward = success_reward_fn(action)\n",
    "    negative_reward = fail_reward_fn(action)\n",
    "    return prob * positive_reward + (1 - prob) * negative_reward"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "f48bf4db-78e9-43e9-bec7-0de8dbd4c7b8",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Let's actually compute those expected immediate rewards. What is the immediate reward we expect from taking any action when we are in some state?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "9f14d0d9-30db-47f0-a662-0eecc508a1c0",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "prefix = \"sequence\u2423modeling\u2423is\u2423\"\n",
    "for action in range(1, 6):\n",
    "    er = expected_immediate_reward(prefix=prefix, nchars_left=10, action=action, agent_lm=LM,\n",
    "                                   success_reward_fn=lambda action: action-1,\n",
    "                                   fail_reward_fn=lambda action: 0)\n",
    "    print(action, f\"{er.item():.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "80d957bd-9418-40df-9585-5d1d6f54adda",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Unsurprisingly, predicting four characters is best: if you already have \"t\", getting \"h\", \"e\", and \"\u2423\" isn't much of a risk -- but the reward is much higher.\n",
    "Just to make sure you understand these quantities: \n",
    "\n",
    "- What is the minimum the reward could ever be (for any action $a \\in \\mathbb{N}$)? $\\color{red}{\\text{FILL IN}}$\n",
    "- How many actions will achieve that minimum (given the environment we use here)? $\\color{red}{\\text{FILL IN}}$\n",
    "- How far do we have to look to find the action with the highest reward? $\\color{red}{\\text{FILL IN}}$\n",
    "- Assume $a=4$ indeed achieves the maximum. Is this enough to tell us that we should definitely pick $a=4$? Why or why not? $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "So let's try to build a greedy agent to get us good actions!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "24c1d41c-0409-402d-9c18-100e5985e5bb",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class TextProposalAgent(RLAgent):\n",
    "    \"\"\"\n",
    "    These things will be shared by all the agents we will build in the sequel.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, lm=None):\n",
    "        self.lm = lm\n",
    "    def decision(self, *, state):\n",
    "        pass    \n",
    "    def receive_response(self, *, state, action, reward, next_state):\n",
    "        pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "177cd37c-2fe8-459d-925a-dfb77adf29b3",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class GreedyExpectedImmediateRewardAgent(TextProposalAgent):\n",
    "    def success_reward_fn(self, action): return action-1\n",
    "    def fail_reward_fn(self, action): return 0\n",
    "  \n",
    "    def decision(self, *, state):\n",
    "        \"\"\"\n",
    "        This function defines a policy: make a decision based on the triple `state`.\n",
    "        \"\"\"\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n",
    "\n",
    "# This should say: predict 4 characters, namely \"the\u2423\"\n",
    "env = TypistState(lm=LM, string=\"sequence\u2423modeling\u2423is\u2423the\u2423best\", start_index=21)\n",
    "assert GreedyExpectedImmediateRewardAgent(lm=LM).decision(state=env.current_state) == 4 # \"the\u2423\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "e91e30ef-c7cb-49d9-b81d-d068bf865639",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Now, given our Typist and our Agent, it's time to play this game!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "831be3c6-27ef-45da-bb00-2f6cac21d42c",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# We should get a total reward of 5 here\n",
    "string = \"sequence\u2423modeling\u2423is\u2423the\u2423best\"\n",
    "start_index = 21\n",
    "# Verbose\n",
    "assert TypistState(lm=LM, string=string, start_index=start_index, debug=True) \\\n",
    "        .evaluate_agent(agent=GreedyExpectedImmediateRewardAgent(lm=LM)) == 5\n",
    "# Or, shorter, using some magic and only getting the result without debug info\n",
    "assert TypistState(lm=LM, string=string, start_index=start_index) \\\n",
    "        .evaluate_agent(agentclass=GreedyExpectedImmediateRewardAgent, lm=LM) == 5"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "577f9de9-f7ee-42b5-bac5-c62335f04600",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Is that good or bad? Let's compare against a baseline that chooses randomly among the possible actions (all of which are rather good since they are high-probability under the LM):"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "59a94a8f-6452-446b-a6de-d1331f49260c",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# A random length agent\n",
    "class RandomLengthAgent(TextProposalAgent):\n",
    "    def decision(self, *, state):\n",
    "        action = random.randint(1, 10)\n",
    "        return action"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "052d72b8-1605-4184-ad1a-03043a7b9487",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Run it a few times to see how well it does on average\n",
    "rs = [\n",
    "    TypistState(lm=LM, string=string, start_index=start_index).evaluate_agent(agentclass=RandomLengthAgent, lm=LM)\n",
    "    for _ in range(10)\n",
    "]\n",
    "print(\"Rewards:\", rs)\n",
    "print(\"Average reward:\", np.average(rs))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "8beaabc5-98a0-4a0f-8723-8308b2beffd4",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "So our greedy agent is definitely a lot better. But let's not just evaluate this on one cherry-picked example:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "bee4269f-f151-4151-a944-a62488c10ca1",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def compare_agents(agent1class, agent2class, true_lm=LM, agent_lm=LM, n_sentences=5, total_length=20, start_index=12, verbose=True):\n",
    "    \"\"\"\n",
    "    Sample some sentences from true_lm and compare the performance of\n",
    "    the passed two agents on these sentences.\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    sum_1, sum_2 = 0, 0\n",
    "    for _ in range(n_sentences):\n",
    "        # Draw a sentence from the LM (though at lower local temperature, so the agent model isn't 100% perfect)\n",
    "        sample = ''.join(itertools.islice(true_lm.greedy_sample(temperature=0.2), total_length))\n",
    "        # Test both agents\n",
    "        agent1_reward = TypistState(lm=true_lm, string=sample, start_index=start_index).evaluate_agent(agentclass=agent1class, lm=agent_lm)\n",
    "        agent2_reward = TypistState(lm=true_lm, string=sample, start_index=start_index).evaluate_agent(agentclass=agent2class, lm=agent_lm)\n",
    "        # What happened!\n",
    "        sum_1 += agent1_reward\n",
    "        sum_2 += agent2_reward\n",
    "        if verbose:\n",
    "            print(f\"On {sample[:start_index]}>{sample[start_index:]}, agent 1 got {agent1_reward}, agent 2 got {agent2_reward}.\")\n",
    "    return (sum_1, sum_2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "43791ddb-1902-43ed-bccf-744cdf600e67",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time compare_agents(RandomLengthAgent, GreedyExpectedImmediateRewardAgent)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "2263ddbb-dd95-4458-93c4-3d5f9a53a4b5",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "You will have noticed that this is really way too slow for us to use it in the remainder of the assignment.\n",
    "That's why we will break all our nice abstractions a little to write a single (fast) function for our agent.\n",
    "\n",
    "We will inline the beam search, but more importantly, we will *fuse* all the different searches, because they all will share the same beam search prefix!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "9e5ca088-341d-4823-bb12-d2ccae118ded",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def best_strings_from_hcs(self, *, hcs, max_length, beam_size=5):\n",
    "    \"\"\"\n",
    "    Returns a list z, such that z[l] = (logprob, string, hcs) of the best\n",
    "    string of length l.\n",
    "    \"\"\"\n",
    "    beam_size = min(beam_size, len(self.idx2char))\n",
    "    queue = [(torch.tensor(0.0), \"\", hcs)]\n",
    "    returns = [queue[0]]\n",
    "    while len(queue[0][1]) < max_length:\n",
    "        next_queue = []\n",
    "        for pscore, ptaskstate, pmodelstate in queue:\n",
    "            probs = (pmodelstate[-1][0] @ self.embedding.weight.t()).log_softmax(dim=-1)\n",
    "            for nscore, idx in zip(*probs.squeeze(0).topk(beam_size)):\n",
    "                nmodelstate = self._hcs_from_cidx(pmodelstate, torch.tensor([idx]))\n",
    "                next_queue.append((nscore + pscore, ptaskstate + self.idx2char[idx], nmodelstate))\n",
    "        if len(next_queue) == 0:\n",
    "            break\n",
    "        next_queue.sort(key=lambda x: -float(x[0]))\n",
    "        queue = next_queue[:beam_size]\n",
    "        returns.append(queue[0])\n",
    "    return returns\n",
    "\n",
    "# Patch it, too, into the language model\n",
    "LanguageModel.best_strings_from_hcs = best_strings_from_hcs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "5bd66455-d0ca-45dd-a78d-8f440558796c",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Try it out:\n",
    "\n",
    "print(\"Our old function\")\n",
    "start_time = time.time()\n",
    "for _ in range(1):\n",
    "    for a in range(1, 8+1):\n",
    "        print(LM.best_string(prefix=\"sequence\u2423modeling\u2423is\u2423\", length=a))\n",
    "print(time.time() - start_time)\n",
    "\n",
    "print(\"\\nOur new function\")\n",
    "start_time = time.time()\n",
    "hcs = LM.hcs_from_context(\"sequence\u2423modeling\u2423is\u2423\")\n",
    "print('\\n'.join([x[1] for x in LM.best_strings_from_hcs(hcs=hcs, max_length=8)][1:]))\n",
    "print(time.time() - start_time)\n",
    "\n",
    "print(\"\\nOur new function, much farther into the string\")\n",
    "start_time = time.time()\n",
    "hcs = LM.hcs_from_context(\"sequence\u2423modeling\u2423is\u2423\")\n",
    "print('\\n'.join([x[1] for x in LM.best_strings_from_hcs(hcs=hcs, max_length=50)][-10:]))\n",
    "print(time.time() - start_time)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "2097c07d-a5ef-4b8a-b560-f495b3b035b6",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Much better! Let's use it to implement faster agents!\n"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "466b78e4-4d8c-4a2c-b462-0cfd0d13ae9b",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class FastGreedyExpectedImmediateRewardAgent(TextProposalAgent):\n",
    "    def success_reward_fn(self, action): return action-1\n",
    "    def fail_reward_fn(self, action): return 0  \n",
    "\n",
    "    def decision(self, *, state):\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "11e1a789-e11e-403b-af97-b985205658b5",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Compare on two sentences\n",
    "for sample in [\"modeling\u2423is\u2423the\u2423best\", \"kare\u2423to\u2423from\u2423first\u2423g\"]:\n",
    "    print(\"Predict\", sample[:12], \">\", sample[12:])\n",
    "    \n",
    "    def get_reward(agentclass):\n",
    "        random.seed(0)\n",
    "        user = TypistState(lm=LM, string=sample, start_index=12)\n",
    "        return user.evaluate_agent(agentclass=agentclass, lm=LM)\n",
    "    # Test the greedy speedup\n",
    "    %time      random_reward = get_reward(                     RandomLengthAgent)\n",
    "    %time      greedy_reward = get_reward(    GreedyExpectedImmediateRewardAgent)\n",
    "    %time fast_greedy_reward = get_reward(FastGreedyExpectedImmediateRewardAgent)\n",
    "    assert greedy_reward == fast_greedy_reward, (greedy_reward, fast_greedy_reward)\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "d4bb3db7-fe6a-4f54-bdb4-b8e5e68752cf",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "By breaking our beautiful abstraction and making some strong assumptions we have made our solution about 10x faster!\n",
    "Now, let's run a little comparative study:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "65f9d6ea-9bfe-4e57-9d62-31d976d249d8",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time compare_agents(RandomLengthAgent, FastGreedyExpectedImmediateRewardAgent)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7e753825-82ab-4b92-abc9-20b45f0208cd",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "This should be at least twice as fast -- nice! Let's try a few more:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "3654a518-1558-4dc3-a42b-58348138dd6e",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time sum_random, sum_greedy = compare_agents(RandomLengthAgent, FastGreedyExpectedImmediateRewardAgent, n_sentences=20)\n",
    "print(\"Totals! Random:\", sum_random, \"-- Greedy:\", sum_greedy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "0378c78c-0c82-4719-bf41-35d397ea037f",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Interestingly, sometimes the random agent seems to outperform the greedy agent. To be fair, of course, the random agent isn't quite that random -- it still extracts the best string of a given length from the LM. This is mostly an artifact of the limited scope of the problem that we are attempting to tackle in this assignment. In the \"real\" version of this problem, our action space would contain all strings, not just ten one-best LM samples; a random agent in that action space would be remarkably terrible. As it is, the random agent can occasionally get lucky and pick a better action sequence than our more-intelligent greedy agent."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "0f4f05fa-c06b-4c7b-8d91-f8bb43a0f993",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## Simplifying things: predict from a four-letter alphabet\n",
    "\n",
    "Because even with our speedups this is still plenty slow in a Python notebook, we will simplify the task a little: our strings are now over an alphabet of three characters: \"x\", \"o\", and our old readability-improving friend \"\u2423\". The language model will also be much smaller and thus faster.\n",
    "\n",
    "What data are we training on? We follow established NLP practice and pretend Jason's NLP class homework 1 toy grammars are somehow meaningful.  (Established NLP practice?  Please, catch the sarcasm and read [Yoav Goldberg's great takedown](https://medium.com/@yoav.goldberg/an-adversarial-review-of-adversarial-generation-of-natural-language-409ac3378bd7) of someone outside JHU who tried to write a real paper using those grammars.)  We generate some random sentences from one of those grammars, and then replace all characters in terminal symbols with with \"x\" or \"o\" and replace spaces with \"\u2423\".  This will be our training dataset for the language model.\n",
    "\n",
    "```\n",
    "$ ./randsent holygrail.gr 350000 | tr '\\nabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ' ' xoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxoxox ' | head -c 10000000 | sed 's/ /\u2423/g' | gzip > xo_corpus.txt.gz\n",
    "\n",
    "$ zcat xo_corpus.txt.gz | sed 's/\u2423/\\n/g' | sort -nr | uniq -c | sort -nr\n",
    " 382815 x\n",
    " 102040 xoxox\n",
    "  92066 oxoxx\n",
    "  72006 ox\n",
    "  71656 ooxx\n",
    "  69657 oox\n",
    "  69640 xoxooxo\n",
    "  69508 xox\n",
    "  69495 ooxo\n",
    "  69394 xxxo\n",
    "  59820 xxooxo\n",
    "  59718 oxxx\n",
    "  52349 xxoxox\n",
    "  52225 ooxoxx\n",
    "  52175 oxx\n",
    "  52111 xx\n",
    "  52003 xxooxxx\n",
    "  37677 xxo\n",
    "  [...]\n",
    "```"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "c1082c1d-a5a5-422a-90c8-0ada51e1bc76",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "retrain_small_model = False\n",
    "\n",
    "if retrain_small_model:\n",
    "    if not os.path.isfile(\"xo_corpus.txt.gz\"):\n",
    "        import urllib\n",
    "        urllib.request.urlretrieve(\"https://sjmielke.com/tmp/xo_corpus.txt.gz\", \"xo_corpus.txt.gz\")\n",
    "\n",
    "    with gzip.open(\"xo_corpus.txt.gz\", 'rt') as f:\n",
    "        xo_corpus = f.read()\n",
    "\n",
    "    train_lm(\"xo\", xo_corpus, nlayers=3, nhid=512, embsize=32, BATCHSIZE=200, BPTTLENGTH=100)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "de81cdd2-f38d-4a84-8cd3-68b1aa320202",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Load the pretrained model\n",
    "XOLM = LanguageModel(\"ox\u2423\", layers=2, hidden_size=64, embedding_size=4).cpu()\n",
    "XOLM.load_state_dict(torch.load(\"xo.lm.statedict.pt\", map_location='cpu'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "829d3cfb-867a-4c72-ba37-1a8873106723",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "sample = \"\".join(itertools.islice(XOLM.greedy_sample(), 100))\n",
    "print(sample)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "548b6d2f-f3d9-4f9d-bf70-57f6583d1229",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Looks like we got the structure down (see the single \"x\"s that used to be \".\"s).\n",
    "Just to check that this was worth it, let's rerun it on our game:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "75e5d225-f206-4d2c-8f71-f37afa113fbc",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time sum_random, sum_greedy = compare_agents(\\\n",
    "    RandomLengthAgent,\\\n",
    "    FastGreedyExpectedImmediateRewardAgent,\\\n",
    "    true_lm=XOLM,\\\n",
    "    agent_lm=XOLM,\\\n",
    "    n_sentences=100,\\\n",
    "    verbose=False\\\n",
    ")\n",
    "print(\"Totals! Random:\", sum_random, \"-- Greedy:\", sum_greedy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "21591211-fe2b-4977-abf0-006825724229",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "That speed will do -- for making everything much slower and more complicated."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "18dc5a27-4967-44c2-a563-0d7655eeeadc",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## From greedy actions to planning\n",
    "\n",
    "You will have noticed that we tried to stress that this (definitely better-than-random) agent of ours is *greedy* -- and you should know that we generally think that greediness is a bad thing. Specifically, the agent sometimes fails to take actions that have low immediate reward, but \"set up\" the agent for future success by moving it to a good state.\n",
    "\n",
    "To emphasize this, we will make a small change to the reward function of the task. Since we want to help the user enter the text in as few steps as possible (remember that the user has to press one key per step), let's define the reward to be $-1$ on every step (so it's really a penalty).  The total reward of an episode is now the negative total number of keystrokes.  This is the same as our old reward function (the number of keystrokes saved) plus a constant (namely  $|\\mathbf{w}|$).  So the *optimal* policy should be the same.  But the reward is now distributed differently across the time steps.  With the new function, all actions have the same immediate reward, so they are all tied and the *greedy* policy has no way to choose!  \n"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "057935ce-db49-4136-80e4-ce77cc05011d",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# update our environment, and our greedy agents\n",
    "TypistState.successful_prediction_reward = lambda *args:-1\n",
    "TypistState.failed_prediction_reward = lambda *args:-1\n",
    "\n",
    "class PenaltyRewardFastGreedyExpectedImmediateRewardAgent(FastGreedyExpectedImmediateRewardAgent):\n",
    "    def success_reward_fn(self, action): return -1\n",
    "    def fail_reward_fn(self, action): return -1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "2ad13401-c0ff-4e06-81ab-52666ee7023a",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "%time sum_random, sum_greedy = compare_agents(\\\n",
    "    RandomLengthAgent,\\\n",
    "    FastGreedyExpectedImmediateRewardAgent,\\\n",
    "    true_lm=XOLM,\\\n",
    "    agent_lm=XOLM,\\\n",
    "    n_sentences=100,\\\n",
    "    verbose=False\\\n",
    ")\n",
    "print(\"Totals under new reward function, with old agent! Random:\", sum_random, \"-- Greedy:\", sum_greedy)\n",
    "\n",
    "%time sum_random, sum_greedy = compare_agents(\\\n",
    "    RandomLengthAgent,\\\n",
    "    PenaltyRewardFastGreedyExpectedImmediateRewardAgent,\\\n",
    "    true_lm=XOLM,\\\n",
    "    agent_lm=XOLM,\\\n",
    "    n_sentences=100,\\\n",
    "    verbose=False\\\n",
    ")\n",
    "print(\"Totals under new reward function, with new agent! Random:\", sum_random, \"-- Greedy:\", sum_greedy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "4a5735e3-88b7-4a02-ab9b-810e996b3e0a",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We change the reward function, and suddenly, greedy looks terrible! Why? Well, under our new reward function, all the actions look the same, and the way the greedy agent is coded, it defaults to guessing the lowest-size action in the case of ties. So, the greedy agent always picks an action of size 1, and achieves the worst possible score. (Of course, we could mitigate this by changing the \"default\" behavior. But that's not really solving the issue.) How can we fix this?\n",
    "\n",
    "(Sidenote: one question you might have is, \"why not just keep the old reward function\"? Well, if the environment's reward is $r$, and the agent's policy maximizes some $\\hat{r} \\neq r$, we are taking a risk. We typically have no guarantees that maximizing $\\hat{r}$ has any good effect on $r$ at all. For this toy example, it happens to be the case that the two reward functions are equivalent, but for more complex tasks, hand-designing good reward heuristics can be tedious and difficult. We want a *generic* solution that maximizes reward in any environment, regardless of when and how rewards are doled out.)\n",
    "\n",
    "One solution to this issue is a kind of lookahead: let's *plan* out the rest of the trajectory!  This allows us to not greedily choose the action that maximizes immediate expected reward (i.e., minimizes immediate harm under our nasty new reward function), but instead choose the action that will give us the highest expected *return* (total reward over all futrher steps).  Under our new reward function, that means trying to find short trajectories, which have a less negative return.\n",
    "\n",
    "There is an obvious complication: to plan ahead, we would have to know what response the user will give to each of our actions. Since we don't know that, we have to again use the agent's language model $\\hat{p}$ to hypothesize what the user might say -- and then we follow each path.\n",
    "\n",
    "Take a look at this picture, showing one such *game tree* for predicting a length-3 string using the letters \"a\", \"b\", and \"c\":\n",
    "\n",
    "<img width=\"100%\" src=\"gametree.png\" />\n",
    "\n",
    "Each node of this tree is a *state* containing information about the string we've seen so far and the actions that took us there; the states are aligned by timestep in the to-be-predicted string. In blue we see the actions that can be taken (with the corresponding 1-best string under it), then for each action, the paths split depending on whether the user accepted the whole proposal (green arc) or rejected it (red arcs, for each possible rejection), both putting us in a new state from which we continue reasoning... until we've reached the end! As you can see, even for this little example, there are a *lot* of paths, namely as many as there are final states.\n",
    "\n",
    "How many is that in this example? $\\color{red}{\\text{FILL IN}}$ \n",
    "\n",
    "How many is that in general for predicting $n$ characters from a set of $m$ symbols? (Hint: use recursion) $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "How many paths then would we have in our old prediction task with the old language model? $\\color{red}{\\text{FILL IN}}$ \n",
    "\n",
    "You should have arrived at an answer that more or less says \"utterly infeasible\". Of course, in the spirit of learning by pain, we will still try to construct an agent, that does precisely this: expand *all* paths in the game tree -- for our new, very small task, of course."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "ffad2536-742e-4e55-8af7-954d5504aa4a",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## The expected return is the expected total reward: marginalizing over all paths\n",
    "\n",
    "How can we use this game tree to find the action that seems \"best\"? We are still looking to maximize reward, but this time we will not only see how much the immediate action (i.e., the immediate outgoing arc of the start state in our game tree) would give us and with which probability, but we will play each of these outcomes until the end -- only then will we know the worth of the initial action.\n",
    "\n",
    "But how do we get from knowing a completed trajectory's return and having probabilities for environment responses (our LM) to tallying up the worth of that first initial action?\n",
    "\n",
    "We should **marginalize** over environment responses, but **maximize** over agent choices.  That is because the agent's *policy* is to always choose the action it believes to be best. If we were using a stochastic policy, we would marginalize over the agent's choices, too."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "8db37de7-9975-4d18-a301-aba9e1db05c9",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class ExhaustivePlanningAgent(TextProposalAgent):\n",
    "    def __init__(self, lm, lookahead_nchars):\n",
    "        self.lm = lm\n",
    "        self.lookahead_nchars = lookahead_nchars\n",
    "  \n",
    "    def success_reward_fn(self, action): return -1\n",
    "    def failure_reward_fn(self, action): return -1\n",
    "  \n",
    "    def decision(self, *, state):\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "4a79dd01-68f5-4b47-bd44-eb99e6be534b",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "sample = \"x\u2423xox\u2423oxoxxoxo\u2423xx\u2423xoxox\u2423xxooxoo\u2423x\u2423xoxooxo\u2423oxoxx\u2423ooxoxx\u2423ooxo\u2423oxxoo\u2423x\"\n",
    "\n",
    "for maxlen in range(1, 6):\n",
    "    user = TypistState(lm=XOLM, string=sample[:12+maxlen], start_index=12, debug=True)\n",
    "    %time user.evaluate_agent(agentclass=ExhaustivePlanningAgent, lm=XOLM, lookahead_nchars=maxlen)\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "01b346c9-1b67-4f35-b466-3333e195b295",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "This is still embarassingly slow -- runtime scales linearly with the number of paths and that scales exponentially in the length of the to be predicted suffix:\n",
    "\n",
    "length | paths | time (ms)\n",
    "-|-|-\n",
    "1 | 3 | 21\n",
    "2 | 19 | 36\n",
    "3 | 175 | 277\n",
    "4 | 2123 | 3720\n",
    "5 | 32043 | 66000\n",
    "6 | 579095 | 1556000\n",
    "\n",
    "How could we fix this? Of course, we could simplify our model --  and in a sense we already did that in the previous agents! The way we set up their internal belief state, our game tree is actually not a tree, but a DAG. Why? How many nodes does it have? $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "You should have come up with a number that is $O(|\\mathbf{w}|)$ -- can we get a DAG that has $O(1)$ nodes? If yes, explain how, if no, explain why. $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "Could we do it if we didn't have the number of character left as part of our state? $\\color{red}{\\text{FILL IN}}$\n",
    "\n",
    "But we could also perform approximate inference - and we will do just that, specifically, we will (finally) start using techniques from reinforcement learning to find our way through this enormous state space."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "8cabdf8d-c413-4bd2-a4ac-0cab7255cd63",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## Value functions judge states\n",
    "\n",
    "If we could tell how good each state was, we could call off our search very early -- in fact, we could take our GreedyAgent and judge each action not only by how much immediate reward we get, but add in how good the state that we end up in will be. How should we judge the \"goodness\" of the next state? One useful way to evaluate a state is to ask, \"If I end up in this state, what return will I get over the rest of the episode?\"\n",
    "\n",
    "Let's make this more precise. Let's call our current policy $\\pi$, i.e. $a = \\pi(s)$. Define the function $V^\\pi$ such that $V^\\pi(s)$ is be the expected return that we obtain by starting in state $s$ if we always choose actions according to $\\pi$ in every future state. We can use a procedure called *policy iteration* to find the best policy.\n",
    "\n",
    "At a high level, here's how policy iteration works. First, start off with any $\\pi$. Compute $V^\\pi(s)$ for all states $s$. Define a new policy $\\pi'(s)$ in the following way: $\\pi(s) = $the action which results in the highest value among all actions we could take in $s$. Now, this new policy $\\pi'$ is guaranteed to score higher than the old $\\pi$! Compute $V^{\\pi'}(s)$ for this new policy, and repeat: define $\\pi''=\\cdots$.\n",
    "\n",
    "This procedure should hopefully seem very intuitive. Whenever we make a change to our policy, we switch from a worse action (where \"worse\" means our current policy gets a lower return from this action) to a better one. Since all changes to our policy result in improvements, the overall policy must improve.\n",
    "\n",
    "There's just one gap: how do we actually compute $V^\\pi$? We can learn $V^\\pi$ with an incremental process, that is, we will start with some estimate (that is most likely wrong) and then iteratively refine these estimates. How can this refinement work? Let's refer to our current estimator as just $V$. For any given state $s$ we can improve $V(s)$ by recomputing it from the states that the actions you can take from $s$ lead to.\n",
    "Consider this example:\n",
    "\n",
    "<img width=\"100%\" src=\"state_to_state_with_model.png\" />\n",
    "\n",
    "In state $s$, we can take actions $a_1$ and $a_2$. What happens if we did that? This is where the model of reality that the agent has comes in (the one that in our example already told us how likely an acceptance for a given proposal was): we know that taking action $a_1$ can land us in $s_{1,1}$, $s_{1,2}$, or $s_{1,3}$ (with certain probabilities, say $p_{1,1}$, $p_{1,2}$, and $p_{1,3}$).\n",
    "Then we can recompute $V(s)$ using the immediate rewards $r_{*,*}$ and the value function estimates $V(s_{*,*})$!\n",
    "\n",
    "As in the search case, the expected return from state $s$ on will be defined by the *best* action, since at any given step, our current policy is to always choose the best action, where \"best\" can be conveniently defined using $V$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{expected-return}(s, a_1) &= \\mathbb{E}[r_{1,i} + \\gamma V(s_{1,i}))] = \\sum_{i=1}^3 p_{1,i} \\cdot (r_{1,i} + \\gamma V(s_{1,i})) \\\\\n",
    "    \\text{expected-return}(s, a_2) &= \\mathbb{E}[r_{1,i} + \\gamma V(s_{1,i}))] = \\sum_{i=1}^3 p_{1,i} \\cdot (r_{1,i} + \\gamma V(s_{1,i}))\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The *discount factor* $\\gamma$ (usually, a constant like $.99$) is necessary in situations where trajectories can be infinitely long (think of cycles in some state graph), so we don't end up with infinite return. Since in our case we have finite trajectories, we technically can just set $\\gamma(s) = 1$ everywhere without an issue, but in deep RL, it's typical to include a gamma even on finite-horizon problems for stability reasons. So, we set $\\gamma(s) = .99$.\n",
    "\n",
    "Note that we can hardcode $V(s)$ to be $0$ for all terminal states $s$ -- if we're done, we will not get any more rewards (and why try to learn that when we already know it).  We can identify which states are terminal by the fact that nchars_left is 0.\n",
    "\n",
    "Now, we can finally update $V(s)$ given that we know what action we would have taken in in $s$. Let's say WLOG that it was $a_1$ that had highest $\\text{expected-return}(s, a)$ out of all possible actions. We can update using the Bellman equation (also called the \"Bellman backup operator\"):\n",
    "\n",
    "$$\n",
    "    V(s) \\leftarrow \\sum_{i=1}^3 p_{1,i} \\cdot (r_{1,i} + \\gamma V(s_{1,i}))) = \\mathbb{E}[r_{1,i} + \\gamma V(s_{1,i}))]\n",
    "$$"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "8c9d00c1-58cd-4fb2-b012-f6b83dd8ef64",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "If we iterate that process for all our states $s$, we will learn a good value function $V$ in all states, and acting according to the argmax of $V$ will lead us to make *globally optimal greedy decisions*! We call this the *optimal policy*, and write it as $\\pi^*$, with value function $V^*$.\n",
    "\n",
    "Pretty cool! But wait... iterate for *all* states? Clearly that's utterly infeasible, when you have a state space as large as ours. And what should this function $V$ look like internally? A big table of all the millions of states? That can't be it... We will tackle these two problems in turn:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "7c394e5a-c20e-4675-b237-87209df08ec8",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "### Deep RL: approximating $V$ with a neural network\n",
    "\n",
    "The simplest option for implementing a function $V$ is a big table in memory where for each $s$ a separate value is stored.\n",
    "This setting is known as *tabular RL* (\"tabular\" is the adjectival form of \"table\").  While it gives many nice guarantees for convergence etc., it is not very practical for real-world AI settings.  The issue is that most problems have infeasibly large state spaces: not only can we not fit these giant tables in our computer memory, but we also lose out by not *sharing information* between related states.\n",
    "\n",
    "The alternative to tabular reinforcement learning is sometimes called *function approximation RL*. Rather than a table-lookup, we have a function that maps arbitrary states to values.\n",
    "That is where \"Deep\" comes into Reinforcement Learning. We choose a specific functional form for $V$: a neural network that outputs a scalar given some vector representation of the state $s$.\n",
    "\n",
    "For our purposes this means that the agent has to encode its state into a vector somehow. Well, we are in luck: we can just use the hidden state of the pretrained language model that we are already using.\n",
    "Given this vector we could build an arbitrarily complicated neural network that outputs a scalar -- we will for now just use 2-layer feedforward networks.\n",
    "\n",
    "This of course also means that the assignment given in the equation above is meaningless: we cannot just \"assign\" an output value to a neural network. We will instead minimize the squared distance between the old estimate for $V(s)$ and the one given by the right-hand side of the Bellman equation."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "73ef3541-2e82-4e69-91ea-45d7c8dd21f2",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class ValueFunctionApproximator(torch.nn.Module):\n",
    "    def __init__(self, h_dims):\n",
    "        super().__init__()\n",
    "        # The main network\n",
    "        self.linear1 = torch.nn.Linear(h_dims+1, 16)\n",
    "        self.linear2 = torch.nn.Linear(16+1, 1)\n",
    "        self.linear2.weight.data = torch.FloatTensor(1, 16+1).uniform_(-.01, .01)\n",
    "        self.linear2.bias.data.fill_(0.00)\n",
    "        # The more stable, since only slowly and indirectly updated target network\n",
    "        self.linear1_target = torch.nn.Linear(h_dims+1, 16)\n",
    "        self.linear1_target.weight.data = self.linear1.weight.data\n",
    "        self.linear1_target.bias.data = self.linear1.bias.data\n",
    "        self.linear2_target = torch.nn.Linear(16+1, 1)\n",
    "        self.linear2_target.weight.data = self.linear2.weight.data\n",
    "        self.linear2_target.bias.data = self.linear2.bias.data\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Set the target network weights to a new moving average.\n",
    "        \"\"\"\n",
    "        self.linear1_target.weight.data = 0.05 * self.linear1.weight.data + 0.95 * self.linear1_target.weight.data\n",
    "        self.linear2_target.weight.data = 0.05 * self.linear2.weight.data + 0.95 * self.linear2_target.weight.data\n",
    "        self.linear1_target.bias.data = 0.05 * self.linear1.bias.data + 0.95 * self.linear1_target.bias.data\n",
    "        self.linear2_target.bias.data = 0.05 * self.linear2.bias.data + 0.95 * self.linear2_target.bias.data\n",
    "\n",
    "    def forward(self, hcs, nchars_left, target=False):\n",
    "        \"\"\"\n",
    "        Using PyTorch syntax, we define `forward()` to give us the scalar from\n",
    "        the state representation we feed in: the LM hidden state and the number\n",
    "        of characters left.\n",
    "        The `target` parameter tells us whether to use the (moving average)\n",
    "        target network weights.\n",
    "        \"\"\"\n",
    "        if nchars_left <= 0: return torch.tensor(0.)\n",
    "        lmrep = hcs[-1][0][0]\n",
    "        inp = torch.cat([lmrep, torch.tensor([float(nchars_left)])])\n",
    "        hid = (self.linear1_target if target else self.linear1)(inp).tanh()\n",
    "        hid = torch.cat([hid, torch.tensor([float(nchars_left)])])\n",
    "        return (self.linear2_target if target else self.linear2)(hid).squeeze()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "95ce1266-dc98-4108-98a5-965cc23ffdbe",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# This is how we would use it:\n",
    "hcs = LM.hcs_from_context(\"sequence\u2423modeling\u2423is\u2423\")\n",
    "V = ValueFunctionApproximator(LM.lstm_layers[-1].hidden_size)\n",
    "V(hcs, 5)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "4cbfd327-6534-47b0-ada0-12e7c1715d0a",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We slipped in a few tricks here.  First, note that we initialize the bias of the final layer to zero, and its weights to be very small. This ensures that the initial predictions of the value function, before any training takes place, are close to zero, which improves stability by ensuring that no big wrong values get bootstrapped into other states (remember, even though none of the predictions are correct yet, each value function serves as the \"right answer\" for the state the timestep before it!). Secondly, we defined two versions of our neural network for predicting values.  The main network has layers `linear1` and `linear2`,  but we also have another network with the same topology, whose layers are called `linear1_target` and `linear2_target`. This *target network* will be updated more slowly than the main network, so that it doesn't oscillate during training.  Its weights will be a *moving average* of the weights of the main network.  The trick is to use this target network in place of the main network when we compute the right-hand side of the Bellman equation.  This can improve convergence rates.  Note that no gradients flow into the target network -- its weights will only change by manually averaging past weights of the main network."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "a6764fe6-0ce0-4eb7-a12c-e4f5edad0b97",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def success_reward_fn(action): return -1\n",
    "def failure_reward_fn(action): return -1\n",
    "\n",
    "\n",
    "# We will need the expected return for the Bellman equation and the decisions:\n",
    "def expected_returns(*, prefix_hcs, nchars_left, value_function, agent_lm, gamma=0.99, target=False):\n",
    "    \"\"\"\n",
    "    `prefix_hcs` and `nchars_left` encode our state and will be used to find\n",
    "    actions and possible environment feedback, according to the `agent_lm` LM.\n",
    "    Returns the proposals (a list of strings of length 1..nchars_left) and a 1-D\n",
    "    tensor containing the expected returns for all these action proposals\n",
    "    according to the `value_function`.\n",
    "    \"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b5e846ed-5627-4844-a5a4-abe747c3f76b",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Given such returns, the loss we want to minimize is easy to define as the\n",
    "# squared distance between the current V estimate and the right-hand side of the\n",
    "# Bellman equation, using `expected_returns`.\n",
    "def value_function_loss_for_state(*, prefix_hcs, nchars_left, value_function, agent_lm):\n",
    "    # The old value of the value function\n",
    "    lhs = value_function(prefix_hcs, nchars_left)\n",
    "    # Consult our model to get the next states and their values\n",
    "    rhs = expected_returns(\n",
    "        prefix_hcs=prefix_hcs,\n",
    "        nchars_left=nchars_left,\n",
    "        value_function=value_function,\n",
    "        agent_lm=agent_lm,\n",
    "        target=True  # this is where we use the target network!\n",
    "    ).max()\n",
    "    # Return the squared distance between that and the current estimate.\n",
    "    # Note that we are NOT doing \"residual gradient learning\" here (this would\n",
    "    # mean also using the gradients of the right-hand side to update), because\n",
    "    # this will make thing often optimize towards the wrong thing. We want to\n",
    "    # update the \"past\" using the \"future\". So, we detach:\n",
    "    return torch.nn.functional.mse_loss(lhs, rhs.detach())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "d40a474b-fe07-4977-80dc-f3a5b5df0f79",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# That is how we would use the function:\n",
    "value_function_loss_for_state(prefix_hcs=hcs, nchars_left=3, value_function=V, agent_lm=LM)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "38ef809d-5e9a-45fd-bb0b-8ab9542d683e",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "### Learning from rollouts\n",
    "\n",
    "The next question is: which states should we actually use for our Bellman update? Ideally we could update all states, but that's impossible here.\n",
    "Since we can only interact with the true environment one action and response at a time, we can't simply update any arbitrary state: we need to actually reach it via executing actions in the environment in order to see it and update it. And even if that were not true, enumerating all states in our giant state space would still be absolutely infeasible.\n",
    "\n",
    "The answer here is that we will *roll out* some policy in the environment: take actions in response to each state, while recording all states we pass through. Rolling out a policy in the environment, we obtain a *sample trajectory* that performs the task for a full episode from start to end. Then, given this trajectory, we update $V(s)$ for all states that are on this trajectory. \n",
    "\n",
    "Note that the form of the Bellman update equation makes it look a little like we are \"trying out all the actions\" -- but, like with the GreedyAgent, we are not testing all actions against the *true* environment, but only evaluating them using *model* of the environment that we have. For now, we are assuming that our agent's model is perfect, so we can perform the update once we've sampled a state (but that assuption is a bit unrealistic in many settings, and it will be removed in the next section of this homework)."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "1417eb5a-dc20-4322-a6bd-c854f815a3bb",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class ValueFunctionExpectedReturnAgent(TextProposalAgent):\n",
    "    def __init__(self, lm, value_function, exploration_policy):\n",
    "        \"\"\"\n",
    "        `exploration_policy` is a function that given a tensor of expected returns, \n",
    "        yields the index of the action to take (usually, the best action).\n",
    "        \"\"\"\n",
    "        self.lm = lm\n",
    "        self.value_function = value_function\n",
    "        self.exploration_policy = exploration_policy\n",
    "        self.visited_cache = []\n",
    "\n",
    "    def receive_response(self, *, state, action, reward, next_state):\n",
    "        # save visited state information, to train on later\n",
    "        _, hcs, nchars_left = state\n",
    "        hcs = tuple((hc[0].detach(), hc[1].detach()) for hc in hcs)\n",
    "        self.visited_cache.append(\n",
    "            {\n",
    "                \"prefix_hcs\": hcs,\n",
    "                \"nchars_left\": nchars_left\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def decision(self, *, state):\n",
    "        # Now pick the one with the highest expected return!\n",
    "        _, hcs, nchars_left = state\n",
    "        returns = expected_returns(\n",
    "            prefix_hcs=hcs,\n",
    "            nchars_left=nchars_left,\n",
    "            value_function=self.value_function,\n",
    "            agent_lm=self.lm\n",
    "        )\n",
    "        return self.exploration_policy(returns)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "aa7fb416-05d9-4a60-8bea-c585e2670871",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Try it again -- this time, everything is super-fast since we essentially make greedy decisions\n",
    "for maxlen in range(1, 10):\n",
    "    user = TypistState(lm=XOLM, string=sample[:12+maxlen], start_index=12)\n",
    "    vfa = ValueFunctionApproximator(XOLM.lstm_layers[-1].hidden_size)\n",
    "    %time user.evaluate_agent(agentclass=ValueFunctionExpectedReturnAgent, lm=XOLM, value_function=vfa, exploration_policy=lambda t: torch.argmax(t))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "15915a71-6773-4314-84d7-0d9ccc879525",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We end up with a roughly linear dependence between sequence length and time, as expected -- as fast as we were with the greedy agent. Nearly there!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "0136a5c4-9c6c-4cb7-9934-db4f798ed933",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "### We need to force exploration\n",
    "\n",
    "What policy should we roll out in the environment? One guess might be to just use the current policy. \n",
    "But actually, it's important that we use an *exploration policy* instead: roll out some policy that we *know* to be suboptimal, in order to learn from the states that policy reaches. But, why not just use our current policy? It is the best policy we currently know about, so surely it must be the most valuable to learn about. However, if we only ever used our current policy to interact with the environment, there are many states that we would never visit. If one of those states were really good, we would never visit it to realize that it was good, so we would never know! Therefore, it's important that we use an exploration policy with a non-zero chance of visiting every possible state.\n",
    "\n",
    "Specifically, we will use as our exploration policy an $\\epsilon$-greedy version of our current policy, which is defined as follows: with probability $1-\\epsilon$, take the best action, and with probability $\\epsilon$, sample an action uniformly at random ($\\epsilon$ is usually set to something like $.05$)."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "052ce7f0-a6bd-4338-91fa-a51db7d779c2",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def eps_greedy_policy(returns, eps=.05):\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "91b9b955-1e2c-4d9c-b613-19444b264798",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "With this, we are ready to write the entire training loop:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "6abce089-454f-4fc2-b57e-77b3e83d6a95",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def train(*, agentclass, params_to_optimize, agent_lm, exploration_policy, loss_function, target_network_updater, sentences, log_interval, cache_gradients, **kwargs):\n",
    "    random.seed(0)\n",
    "    sum_reward, sum_loss = 0, 0\n",
    "    # Only update the desired parameters (i.e., the main network, not the target network)\n",
    "    optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3)\n",
    "    optimizer.zero_grad()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Run in environment\n",
    "        env = TypistState(lm=XOLM, string=sentence, start_index=0)\n",
    "        reward, agent = env.evaluate_agent(\n",
    "            agentclass=agentclass,\n",
    "            lm=agent_lm,\n",
    "            return_agent=True,\n",
    "            exploration_policy=exploration_policy,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Construct loss for function approximator\n",
    "        loss = torch.sum(\n",
    "            torch.stack(\n",
    "                [loss_function(agent_lm=agent_lm, **d, **kwargs) for d in agent.visited_cache]\n",
    "            )\n",
    "        )\n",
    "        # Output\n",
    "        sum_reward += reward\n",
    "        sum_loss += loss.item()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(\"Iter:\", i+1)\n",
    "            print(\"Avg reward (using exploration policy):\", sum_reward / log_interval, \"Avg loss:\", sum_loss / log_interval)\n",
    "            try:\n",
    "                print(\"Sample argmax policy reward:\", TypistState(lm=XOLM, string=sentence, start_index=0).evaluate_agent(\n",
    "                    agentclass=ValueFunctionExpectedReturnAgent, lm=XOLM, value_function=agent.value_function, exploration_policy=lambda t: torch.argmax(t)))\n",
    "                print(\"VFA example:\", ' '.join([f\"{ncl}:{agent.value_function(XOLM.hcs_from_context(sentence[:-ncl]), ncl).item():.3f}\" for ncl in reversed(range(1, 10))]))\n",
    "            except AttributeError:\n",
    "                print(\"Sample argmax policy reward:\", TypistState(lm=XOLM, string=sentence, start_index=0).evaluate_agent(\n",
    "                    agentclass=QFunctionExpectedReturnAgent, lm=XOLM, q_function=agent.q_function, exploration_policy=lambda t: torch.argmax(t)))\n",
    "                print(\"QFA example:\", ' '.join([f\"{ncl}:{agent.q_function(XOLM.hcs_from_context(sentence[:-ncl]), ncl).max().item():.3f}\" for ncl in reversed(range(1, 10))]))\n",
    "            print(\"Mean abs weights:\", [p.abs().mean().item() for p in params_to_optimize])\n",
    "            print()\n",
    "            sum_reward, sum_loss = 0, 0\n",
    "        # Optimize/update\n",
    "        loss.backward()\n",
    "        # Now apply the gradients to the target network after n iterations\n",
    "        if (i + 1) % cache_gradients == 0:\n",
    "            optimizer.step()\n",
    "            target_network_updater()\n",
    "            optimizer.zero_grad()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "12d6306b-79b6-4697-af54-389e60d46f92",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def train_vf(agent_lm, sentences, cache_gradients=1):\n",
    "    \"\"\"\n",
    "    You can increase `cache_gradients` for increased stability...\n",
    "    ...at the cost of slower convergence!\n",
    "    \"\"\"\n",
    "    vfa = ValueFunctionApproximator(agent_lm.lstm_layers[-1].hidden_size)\n",
    "    train(\n",
    "        agentclass=ValueFunctionExpectedReturnAgent,\n",
    "        params_to_optimize=list(vfa.linear1.parameters()) + list(vfa.linear2.parameters()),\n",
    "        agent_lm=agent_lm,\n",
    "        exploration_policy=eps_greedy_policy,\n",
    "        value_function=vfa,\n",
    "        loss_function=value_function_loss_for_state,\n",
    "        target_network_updater=vfa.update_target_network,\n",
    "        sentences=sentences,\n",
    "        log_interval=20,\n",
    "        cache_gradients=cache_gradients\n",
    "    )\n",
    "    return vfa"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "e38a3234-746c-47de-bcf0-a99f21dbe7ec",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "First let's try to overfit to some finite set of sentences. This is gonna take a few minutes. Feel free to reduce the number of times we train on that sentence (it is set to 500 below), but be warned - if you cut it off too early, you could get very bad results. To guarantee good results, you need to train a long time."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "daf3cb81-7989-4ec4-9997-f7aec7582958",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "testsentence = '\u2423xxo\u2423x\u2423ooxo\u2423xxxooxx\u2423oxoxx\u2423xoxo'\n",
    "vfa_overfit = train_vf(XOLM, [testsentence] * 500)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b18f61ba-d7f2-46f9-953e-69bdbdd6fda7",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "print(\"Random gets:\", sum([TypistState(lm=XOLM, string=testsentence, start_index=0).evaluate_agent(agentclass=RandomLengthAgent, lm=XOLM) for _ in range(10)])/10)\n",
    "print(\"Greedy gets:\", TypistState(lm=XOLM, string=testsentence, start_index=0).evaluate_agent(agentclass=FastGreedyExpectedImmediateRewardAgent, lm=XOLM))\n",
    "\n",
    "print(\n",
    "    \"After training, we get this reward using our argmax policy:\",\n",
    "    TypistState(lm=XOLM, string=testsentence, start_index=0).evaluate_agent(\n",
    "        agentclass=ValueFunctionExpectedReturnAgent,\n",
    "        lm=XOLM,\n",
    "        value_function=vfa_overfit,\n",
    "        exploration_policy=lambda t: torch.argmax(t)\n",
    "    )\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "e35f5537-43c7-4201-9894-088dd4cee5e5",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Nice (although we brutally overfit)! Let's do a qualitative check whether the value function learned what we wanted it to learn:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "6e40e230-4f36-455e-9507-30b9be93dc84",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "intermediate_sentence = '\u2423xxo\u2423x\u2423ooxo\u2423xxxooxx'\n",
    "\n",
    "# The more characters we think are left, the higher we think the return will be:\n",
    "print(' '.join([f\"{vfa_overfit(XOLM.hcs_from_context(intermediate_sentence), ncl).item():.3f}\" for ncl in reversed(range(1, 10))]))\n",
    "\n",
    "# This is what our Bellman updates look like (with a well-trained model, both sides should be roughly equal):\n",
    "prefix_hcs = XOLM.hcs_from_context(intermediate_sentence)\n",
    "for ncl in reversed(range(1, 10)):\n",
    "    returns = expected_returns(prefix_hcs=prefix_hcs, nchars_left=ncl, value_function=vfa_overfit, agent_lm=XOLM)\n",
    "    print(vfa_overfit(prefix_hcs, ncl).item(), \"is Bellman-updated to come closer to\", returns[torch.argmax(returns)].item())\n",
    "\n",
    "# This is how the input features are used:\n",
    "print(vfa_overfit.linear1.weight[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "f4fa4c05-b0e6-44a5-8cf7-fa2df190f8b1",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Before peeking ahead, should the value function *increase* or *decrease* the further we progress through the sentence/trajectory? Why? $\\color{red}{\\text{FILL IN}}$"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "2b655fbb-db45-455b-b1ee-7dfbdabf3118",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Let's check for our example sentence, what the value function at every position is:\n",
    "# (we should see it go down to 0)\n",
    "for i in range(len(testsentence)):\n",
    "    hcs = XOLM.hcs_from_context(testsentence[:i])\n",
    "    ncl = len(testsentence)-i\n",
    "    loss = value_function_loss_for_state(\n",
    "        prefix_hcs=hcs,\n",
    "        nchars_left=ncl,\n",
    "        value_function=vfa_overfit,\n",
    "        agent_lm=XOLM\n",
    "    )\n",
    "    print(\"After\", i, \"characters, we have V =\", vfa_overfit(hcs, ncl).item(), \"incurring a loss of\", loss.item())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "4cab81c7-262d-4fc2-8821-8fc882865880",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Well here's a fun oddity. Most of the states have quite decent value function judgements, it seems, but some of them incur a very large loss. Why didn't we learn something better?\n",
    "\n",
    "The answer is simple: we almost never visited this state, so we never updated our value function here!  This sort of thing often happens in RL (and we're not going to try to fix it).  We can see that that is true when plotting the *stationary distribution* over states: run the agent a bunch of times and figure our which states were actually visited:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "ceef6535-bdc7-411a-8fcc-fd0fabadfa41",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "stationary = Counter()\n",
    "\n",
    "for _ in range(50):\n",
    "    env = TypistState(lm=XOLM, string=testsentence, start_index=0)\n",
    "    _, agent = env.evaluate_agent(\n",
    "        agentclass=ValueFunctionExpectedReturnAgent,\n",
    "        lm=XOLM,\n",
    "        return_agent=True,\n",
    "        value_function=vfa_overfit,\n",
    "        exploration_policy=eps_greedy_policy\n",
    "    )\n",
    "    for d in agent.visited_cache:\n",
    "        stationary[int(d[\"nchars_left\"])] += 1\n",
    "\n",
    "for i in range(len(testsentence)):\n",
    "    print(f\"The state after {i:2} characters:\", '#' * stationary[len(testsentence) - i])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "54626635-871a-45eb-9990-7b35c6d3aed1",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We can see that there are some states that were visited very often, and some that are visited very rarely. Why is that?\n",
    "\n",
    "$\\color{red}{\\text{FILL IN}}$"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "e1049940-b600-4292-ae35-f5058a6cb2bc",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "All right, enough with this simple overfitting. It's time to train our agent on the real data distribution: every time it enters the environment, it has to guess a new sentence! Can we still do well?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "426c2b80-5216-4995-b99b-a64a7c51ff55",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def sample_generator(lm, temperature=0.2):\n",
    "    while True:\n",
    "        yield ''.join(itertools.islice(lm.greedy_sample(temperature=temperature), 30))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "335bf1e8-8df3-4086-9d81-408bac67ea40",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "We will only train on 500 sentences, that should make us perform *okay* -- but feel free to train for longer to see just how far this agent gets!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "ca0d5498-c7c7-41f6-b8de-84473a651c17",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Now train on some sentences from that generator\n",
    "vfa_all = train_vf(XOLM, itertools.islice(sample_generator(XOLM), 500))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "4a1a0a7e-2ee4-4313-8e21-b35f646df135",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Looks like it trained well! Now let's compare this new great agent and our old GreedyDecisionAgent on a multitude of sentences as well:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "136eace5-0cce-414d-895b-2e79bc868917",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def random_vs_vf_generator():\n",
    "    for sentence in sample_generator(XOLM):\n",
    "        reward_random = TypistState(lm=XOLM, string=sentence, start_index=0).evaluate_agent(\n",
    "            agentclass=RandomLengthAgent,\n",
    "            lm=XOLM   \n",
    "        )\n",
    "        reward_vf = TypistState(lm=XOLM, string=sentence, start_index=0).evaluate_agent(\n",
    "            agentclass=ValueFunctionExpectedReturnAgent,\n",
    "            lm=XOLM,\n",
    "            value_function=vfa_all,\n",
    "            exploration_policy=lambda t: torch.argmax(t)\n",
    "        )\n",
    "        if abs(reward_vf - reward_random) > 4:\n",
    "            print(\"Remarkable difference: on\", sentence, \"we have random:\", reward_random, \"value functions:\", reward_vf)\n",
    "        yield (int(reward_random), int(reward_vf))\n",
    "\n",
    "def plot_agent_difference(reward_pair_generator, name1, name2):\n",
    "    list1, list2, list_diff = [], [], []\n",
    "    for i, (reward1, reward2) in enumerate(itertools.islice(reward_pair_generator, 100)):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            c1 = Counter(list1)\n",
    "            c2 = Counter(list2)\n",
    "            plt.bar(range(-30,0), [c1[i] for i in range(-30,0)], alpha=0.3)\n",
    "            plt.bar(range(-30,0), [c2[i] for i in range(-30,0)], alpha=0.3)\n",
    "            plt.legend([name1, name2])\n",
    "            plt.show()\n",
    "            cd = Counter(list_diff)\n",
    "            plt.bar([i - 10 for i in range(21)], [cd[i-10] for i in range(21)])\n",
    "            plt.legend([name2 + \" - \" + name1])\n",
    "            plt.show()\n",
    "        \n",
    "        list1.append(reward1)\n",
    "        list2.append(reward2)\n",
    "        list_diff.append(reward2 - reward1)\n",
    "    print(\"\\nsums:\", sum(list1), \"vs.\", sum(list2))\n",
    "\n",
    "plot_agent_difference(random_vs_vf_generator(), \"random\", \"value function\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "e452f17c-addd-45d3-b5b0-348ae07a4e89",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Looks like the value functions help over the random agent! The improvement is about the same as the greedy search algorithm was able to get, back when the reward function was nice. The value function approach is able to get this same improvement over random from this new reward function, in the total absence of short-term rewards. And it does so without needing to do exponentially-slow planning.\n",
    "\n",
    "The trade-off, of course, is the long training phase. The greedy planner and the multi-step planner could both be used immediately. The value function took a long time to train before we could actually run the agent. One way to think about this value function trade-off is we are able to *distill* our planning into our value function. Each Bellman update can be thought of as a tiny, one-step plan. When learning a value function, we invest a lot of computation up front, doing thousands of tiny plans and \"storing\" the outcomes of those plans - albeit in a very compressed form - by updating our value function. But once we are done training, we can query the value function and get the results of all that planning in constant time!\n",
    "\n",
    "However, the value function approach still requires a good model of the true environment: the LM that gives the probabilities of acceptance and rejection for each proposal. In the final section, we will ask the question: what if we don't know anything about the environment we are in? But before, some small notes on how you could make this value function learning better and faster.\n",
    "\n",
    "### Possible improvements\n",
    "\n",
    "1. **Update on minibatches** As with gradient descent, relying on single samples means instability and slow convergence -- so usually we would sample many trajectories before performing an update on our value function.\n",
    "2. **Unroll the Bellman equation** Instead of taking the value of the successor states as given by $V^\\pi$ at face value, we could instead try to compute them too as the expectation over these states' successors (i.e., brute-force lookahead for another step). This yield more precise results at the cost of more computation.\n",
    "3. **Sample for the Bellman equation** We have been exploiting the fact that we have a model $p$ to take an expectation over next states that our actions could get to.  This expectation may be slow to compute exactly, especially when we unroll the equation to multiple timesteps as above.  So we can instead approximate the expectation under the model by sampling.  (Note that this still uses the model of the environment rather than actual experience with the environment; it's just a speedup.)  This leads to an interesting tradeoff between unrolling depth (which helps exactness) and a smaller number of samples for each decision on this unrolled depth (if we want to keep computation constant)."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "ddfd29f1-4718-4a82-99d3-bb474b07a089",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "## Model-free RL\n",
    "\n",
    "Let's recap this assignment so far. First, we found that a simple greedy agent was able to take very good actions -- so long as the reward function was amenable to planning over short-term horizons. When the the reward function became more difficult, we needed to use either slow long-term planning or complicated value function approximation of the entire game tree in order to achieve the same level of performance. However, in all of these cases, we actually made the task too easy: we supplied our agent with both knowledge of the reward function and the language model that the environment actually used!\n",
    "\n",
    "We basically handed our agent a complete (and very nearly correct!) model of the world -- a model in which, truly, search was all that was ever needed. So, really, instead of learning something about the environment (as one usually tries in RL), we just learned to search our existing perfect model of it. The fact that there are real trajectories with real rewards didn't matter to us at all.\n",
    "\n",
    "This is a rather uncommon scenario in RL. One does not typically have access to models of this sort. If we had the data, we could try to *learn* a language model - but of course the learned model wouldn't be perfect. It should be easy to see that if the language model that the agent has and the language model that the environment uses to choose $\\mathbf{w}$ differ significantly, agents trained using our current methods would have a tough time scoring well in the environment.\n",
    "\n",
    "So, in this last section we will take away this perfect model and thus the capability of the agent to *reason* about what rewards it will get for any action. It will truly have nothing but the rewards it got from its rollouts to go on. Let's see how far this will get us. Can we still do better than the random agent?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "258dd7f3-d02b-4fab-b854-d0313af61069",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "### Learning $Q$ without modeling $p$\n",
    "\n",
    "Say that we don't know $p$.  Where does our approach break if we no longer have this explicit environment model? We can identify two places:\n",
    "\n",
    "1. During the Bellman update: we need to find out what rewards $r$ and states $s'$ an action *could get us into*, in order to compute the right-hand side of the Bellman update. If we can't make these inferences anymore, what signal can we rely on? Only the transitions $s \\overset{a}{\\rightarrow} s'$ that we actually observed in our roll-out phase!\n",
    "\n",
    "Note that in order to compute the right-hand side of the Bellman update for value functions, it's not enough to have the outcome of one action: we need to have the outcomes of *all* actions. Since we can't \"go back in time\" and undo an action in the real environment, and our state-space is large enough that we'll probably never get into the same state more than once, we will almost never see the outcomes for all actions. There is a way around this: if our interactions in the real world are done according to our current policy, then we can, in fact, guarantee that the distribution of next-states that we reach after taking an action is the same as the one we want for the Bellman update.  As a result, we can use each transition to improve the match of the left-hand side to the right-hand side, by getting a stochastic gradient of the squared difference between them.\n",
    "\n",
    "(One classic version of this algorithm, called [TD-Lambda](https://en.wikipedia.org/wiki/Temporal_difference_learning#TD-Lambda), was sucessfully used back in 1992 to [solve Backgammon in an early win for RL in 1992](https://en.wikipedia.org/wiki/TD-Gammon).)\n",
    "\n",
    "But of course, we need to sample according to an exploration policy that tries all the actions.  This would mean that our exploration policy would then be \"baked into\" the estimates for $V(s')$. That is undesirable, since we only want to update $V$ with our optimal `argmax` policy  -- we want the value function to tell us what expected return we would get when we always take the best actions, not when we take actions according to an exploration policy like the $\\epsilon$-greedy policy. \n",
    "\n",
    "The method we will introduce, Q-learning, will solve that problem nicely: with Q-learning, you can take *any* policy for exploration.  No matter what exploration policy $\\pi$ you pick, you will converge to the same results, namely the Q function for the *optimal* policy $\\pi^*$ ... at least in the tabular MDP case and provided that your exploration policy manages to eventually visit all the states (hence visits them over and over if you keep learning).\n",
    "\n",
    "2. Both in the Bellman update, and during the actual exploration, we still need to decide what action our policy $\\pi$ tells us to take! In the previous section, we did this by using our model to make a one-step prediction, and comparing the outcomes of the various actions. We clearly can't do that now, without a model.\n",
    "\n",
    "Here's a solution to both problems. We will learn a *Q-function* that given a state *and action* tells us the expected return, i.e., instead of querying $V(s)$ for a state $s$ we will now query $Q(s, a)$ for some state-action-pair $(s, a)$.\n",
    "If, again, we had this function, a policy is trivial to derive:\n",
    "$\\pi^*(s) = \\mathrm{argmax}_{a \\in \\mathcal{A}} Q^*(s, a)$.  (No need to query a given environment model $p$ anymore.) \n",
    "\n",
    "Most things stay the same as before, though. We again approximate $Q$ with a neural network, we will still use the hidden states of our pretrained LM as a convenient feature representation for this function approximator, and for the eventual proposal, we will still extract the 1-best strings for the proposed length from the pretrained language model (to keep things simple for this assignment)."
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "7087092f-9a87-4d26-881c-4a5adb37827d",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "So let's go through the only thing that really changes: the Bellman update for this Q-function. We already stated that we are in a situation where instead of being able to expand the game tree, we only observe one trajectory:\n",
    "\n",
    "<img width=\"100%\" src=\"state_to_state_without_model.png\" />\n",
    "\n",
    "The image hints at the solution: for a given state $s$ we can always define $V(s)$ using the Q-function: the value of the state is the maximum value you can get from taking any action $a$ in this state, where the value of taking $a$ in $s$ is precisely what $Q$ gives us.  \n",
    "\n",
    "Once we reach a state $s$, we sample an action such as $a_1$ according to the exploration policy.  As the new Bellman update we would then want a new estimate for $Q(s, a_1)$:\n",
    "\n",
    "$$\n",
    "    Q(s, a_1) = \\mathbb{E}_{r,s'}[r + \\gamma V(s')] = \\mathbb{E}_{r,s'}[r + \\gamma \\max_{a'} Q(s', a')]\n",
    "$$\n",
    "\n",
    "where the expectation is over the stochastic response $r,s'$ of the environment to action $a_1$ (crucially, note that this doesn't even mention the *exploration* policy).  The next action $a'$ ranges over the actions that are available in the new state $s'$.  For example, if $s' = s_{1,3}$ then we will maximize over $a' \\in \\{a_{1,3,1}, a_{1,3,2}\\}$.  \n",
    "\n",
    "1. The first insight needed for Q-learning is that repeating these Bellman updates on *all* the $(s,a)$ pairs over and over again -- no matter how we explore them -- would make the $Q$ function converge to the desired $Q^{\\pi^*}$ function for the optimal policy.\n",
    "\n",
    "2. The second insight is that we can't explicitly compute the expectation on the right-hand side, because we only see the environment give us a single sample of $r,s'$.  So again, we consider the squared error between the left and the right hand sides, compute its gradient, and use our single sample to give us a stochastic estimate of the gradient of that squared error with respect to $Q(s,a_1)$.  We then take a step along that stochastic gradient.  That's Q-learning.\n",
    "\n",
    "3. The third insight is that we are no longer in the tabular case, so we can't take a stochastic gradient step on $Q(s,a_1)$ directly to reduce the squared error.  Rather, $Q(s,a_1)$ is the output of a neural net.  So we use backprop to take a stochastic gradient step on the parameters of the neural net.  That's deep Q-learning.\n",
    "\n",
    "All right! Let's implement it!"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "4146f39d-b58f-4db6-8533-695d3dccad4d",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class QFunctionApproximator(torch.nn.Module):\n",
    "    def __init__(self, h_dims):\n",
    "        super().__init__()\n",
    "        # The value network\n",
    "        self.v_linear1 = torch.nn.Linear(h_dims+1, 16)\n",
    "        self.v_linear2 = torch.nn.Linear(16+1, 1)\n",
    "        self.v_linear2.weight.data = torch.FloatTensor(1, 16+1).uniform_(-.01, .01)\n",
    "        self.v_linear2.bias.data.fill_(0.00)\n",
    "        # The more stable, since only slowly and indirectly updated target network\n",
    "        self.v_linear1_target = torch.nn.Linear(h_dims+1, 16)\n",
    "        self.v_linear1_target.weight.data = self.v_linear1.weight.data\n",
    "        self.v_linear1_target.bias.data = self.v_linear1.bias.data\n",
    "        self.v_linear2_target = torch.nn.Linear(16+1, 1)\n",
    "        self.v_linear2_target.weight.data = self.v_linear2.weight.data\n",
    "        self.v_linear2_target.bias.data = self.v_linear2.bias.data\n",
    "        # The action-value network\n",
    "        self.a_linear1 = torch.nn.Linear(h_dims+1, 16)\n",
    "        self.a_linear2 = torch.nn.Linear(16+1, 11)\n",
    "        self.a_linear2.weight.data = torch.FloatTensor(11, 16+1).uniform_(-.01, .01)\n",
    "        self.a_linear2.bias.data.fill_(0.00)\n",
    "        # The more stable, since only slowly and indirectly updated target network\n",
    "        self.a_linear1_target = torch.nn.Linear(h_dims+1, 16)\n",
    "        self.a_linear1_target.weight.data = self.a_linear1.weight.data\n",
    "        self.a_linear1_target.bias.data = self.a_linear1.bias.data\n",
    "        self.a_linear2_target = torch.nn.Linear(16+1, 11)\n",
    "        self.a_linear2_target.weight.data = self.a_linear2.weight.data\n",
    "        self.a_linear2_target.bias.data = self.a_linear2.bias.data\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Set the target network weights to a new moving average.\n",
    "        \"\"\"\n",
    "        self.v_linear1_target.weight.data = 0.05 * self.v_linear1.weight.data + 0.95 * self.v_linear1_target.weight.data\n",
    "        self.v_linear2_target.weight.data = 0.05 * self.v_linear2.weight.data + 0.95 * self.v_linear2_target.weight.data\n",
    "        self.v_linear1_target.bias.data = 0.05 * self.v_linear1.bias.data + 0.95 * self.v_linear1_target.bias.data\n",
    "        self.v_linear2_target.bias.data = 0.05 * self.v_linear2.bias.data + 0.95 * self.v_linear2_target.bias.data\n",
    "        self.a_linear1_target.weight.data = 0.05 * self.a_linear1.weight.data + 0.95 * self.a_linear1_target.weight.data\n",
    "        self.a_linear2_target.weight.data = 0.05 * self.a_linear2.weight.data + 0.95 * self.a_linear2_target.weight.data\n",
    "        self.a_linear1_target.bias.data = 0.05 * self.a_linear1.bias.data + 0.95 * self.a_linear1_target.bias.data\n",
    "        self.a_linear2_target.bias.data = 0.05 * self.a_linear2.bias.data + 0.95 * self.a_linear2_target.bias.data\n",
    "\n",
    "    def forward(self, hcs, nchars_left, target=False):\n",
    "        \"\"\"\n",
    "        Using PyTorch syntax, we define `forward()` to give us the scalar from\n",
    "        the state representation we feed in: the LM hidden state and the number\n",
    "        of characters left.\n",
    "        The `target` parameter tells us whether to use the (moving average)\n",
    "        target network weights.\n",
    "        \"\"\"\n",
    "        if nchars_left <= 0: return torch.tensor([0.]*11)\n",
    "        lmrep = hcs[-1][0][0]\n",
    "        inp = torch.cat([lmrep, torch.tensor([float(nchars_left)])])\n",
    "        hid = (self.v_linear1_target if target else self.v_linear1)(inp).tanh()\n",
    "        hid = torch.cat([hid, torch.tensor([float(nchars_left)])])\n",
    "        v = (self.v_linear2_target if target else self.v_linear2)(hid)\n",
    "        \n",
    "        hid = (self.a_linear1_target if target else self.a_linear1)(inp).tanh()\n",
    "        hid = torch.cat([hid, torch.tensor([float(nchars_left)])])\n",
    "        a = (self.a_linear2_target if target else self.a_linear2)(hid)\n",
    "        a = a / torch.mean(a[1:])\n",
    "        a[0] = torch.tensor(-float('inf'))\n",
    "        \n",
    "        return v + a\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7f98e533-78ba-46f9-91eb-22dfeb06c990",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# This is how we would use it:\n",
    "prefix_hcs, nchars_left = LM.hcs_from_context(\"sequence\u2423modeling\u2423is\u2423\"), 8\n",
    "Q = QFunctionApproximator(LM.lstm_layers[-1].hidden_size)\n",
    "print(Q(\n",
    "    hcs=prefix_hcs,\n",
    "    nchars_left=nchars_left,\n",
    "))\n",
    "# To get the value of predicting an 8-prefix:\n",
    "print(Q(\n",
    "    hcs=prefix_hcs,\n",
    "    nchars_left=nchars_left,\n",
    ")[8])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "fa68f4e3-69db-482d-ac6e-0bdbc578460f",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def q_function_loss_for_state_action_reward_state(*, hcs, nchars_left, action, reward, next_hcs, next_nchars_left, q_function, agent_lm=None, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Both `hcs_state` and `nchars_left` have versions 1 and 2 for the two states\n",
    "    we need here. Since we chose to encode the action by its final LM hidden\n",
    "    state and its length, there is no need to feed in any more information about\n",
    "    it, as the former is simple `hcs_state2` and the latter can be calculated as\n",
    "    `nchars_left1 - nchars_left2`.\n",
    "    We do, however, need to pass in the immediate reward that we received.\n",
    "    Note that the `agent_lm` is only used to calculate all possible actions from\n",
    "    state2, not to do model-based calculations!\n",
    "    \"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "6c1f0034-3d24-473d-863f-7bc5f3a4014a",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# That is how we would use the function:\n",
    "env = TypistState(lm=LM, string=\"sequence\u2423modeling\u2423is\u2423the\u2423best\", start_index=20)\n",
    "_, hcs, nchars_left = env.current_state\n",
    "reward, (_, next_hcs, next_nchars_left) = env.execute_action(action=4)\n",
    "q_function_loss_for_state_action_reward_state(\n",
    "    hcs=hcs,\n",
    "    nchars_left=nchars_left,\n",
    "    action=4,\n",
    "    reward=reward,\n",
    "    next_hcs=next_hcs,\n",
    "    next_nchars_left=next_nchars_left,\n",
    "    q_function=Q\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "7c26d6b5-41a1-4c0e-808c-696a0abd4169",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "class QFunctionExpectedReturnAgent(TextProposalAgent):\n",
    "    def __init__(self, q_function, exploration_policy, lm=None):\n",
    "        self.q_function = q_function\n",
    "        self.exploration_policy = exploration_policy\n",
    "        self.visited_cache = []\n",
    "\n",
    "    def receive_response(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        This time we can only append a completed dict to the `visisted_cache` list!\n",
    "        \"\"\"\n",
    "        # Assemble tuple\n",
    "        _, hcs, nchars_left = state\n",
    "        _, next_hcs, next_nchars_left = next_state\n",
    "        # Append\n",
    "        self.visited_cache.append(\n",
    "            {\n",
    "                \"hcs\": hcs,\n",
    "                \"nchars_left\": nchars_left,\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"next_hcs\": next_hcs,\n",
    "                \"next_nchars_left\": next_nchars_left\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def decision(self, *, state):\n",
    "        \"\"\"\n",
    "        No need to append to our cache here (unlike the VF agent), but do\n",
    "        remember to use the `self.exploration_policy`!\n",
    "        \"\"\"\n",
    "        _, hcs, nchars_left = state\n",
    "        hcs = tuple((hc[0].detach(), hc[1].detach()) for hc in hcs)\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "8580b37c-ec3d-4ca3-b657-34a433d6281d",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def train_qf(base_lm, sentences, cache_gradients=1):\n",
    "    qf = QFunctionApproximator(base_lm.lstm_layers[-1].hidden_size)\n",
    "    train(\n",
    "        agentclass=QFunctionExpectedReturnAgent,\n",
    "        params_to_optimize=list(qf.v_linear1.parameters()) + list(qf.v_linear2.parameters()) + list(qf.a_linear1.parameters()) + list(qf.a_linear2.parameters()),\n",
    "        agent_lm=None,\n",
    "        exploration_policy=eps_greedy_policy,\n",
    "        q_function=qf,\n",
    "        loss_function=q_function_loss_for_state_action_reward_state,\n",
    "        target_network_updater=qf.update_target_network,\n",
    "        sentences=sentences,\n",
    "        log_interval=50,\n",
    "        cache_gradients=cache_gradients\n",
    "    )\n",
    "    return qf"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "51c8e2eb-a14b-41ed-9a4e-62cd7960c4de",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Again, test if we can overfit this one sentence. This time, we will probably need a lot more samples to get a reasonable estimate because we only learn from our trajectories. In real settings, \"more samples\" means millions and billions -- our code in this notebook already takes a long time for thousands. But maybe the simplicity of our problem makes this still feasible?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "5592ac01-22f6-4e1c-a8a1-5c72c6f776bc",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "testsentence = '\u2423xxo\u2423x\u2423ooxo\u2423xxxooxx\u2423oxoxx\u2423xoxo'\n",
    "qf_overfit = train_qf(XOLM, [testsentence] * 1000)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b990705f-af81-4924-92fb-9d9b2d7c08f9",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "# Let's check for our example sentence, what the maximum Q value at every position is:\n",
    "# (we should see it go up to 0)\n",
    "for i in range(len(testsentence)):\n",
    "    hcs = XOLM.hcs_from_context(testsentence[:i])\n",
    "    ncl = len(testsentence)-i\n",
    "    hyps = XOLM.best_strings_from_hcs(hcs=hcs, max_length=ncl)\n",
    "    maxq = qf_overfit(\n",
    "                hcs=hcs,\n",
    "                nchars_left=ncl\n",
    "            ).max().item()\n",
    "    print(\"After\", i, \"characters, we have a maximum Q of\", maxq)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "fecb114c-8f66-4aa0-88f2-a49d74564fcc",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "print(\"Random gets:\", TypistState(lm=XOLM, string=testsentence, start_index=0).evaluate_agent(agentclass=RandomLengthAgent, lm=XOLM))\n",
    "\n",
    "print(\n",
    "    \"After training, we get this reward using our argmax policy:\",\n",
    "    TypistState(lm=XOLM, string=testsentence, start_index=0).evaluate_agent(\n",
    "        agentclass=QFunctionExpectedReturnAgent,\n",
    "        lm=None,\n",
    "        q_function=qf_overfit,\n",
    "        exploration_policy=lambda t: torch.argmax(t)\n",
    "    )\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b9127f52-266a-4d69-83d0-2ff180242959",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Also for the final test, we can again try to train on the true distribution:"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "00e2cb43-16bd-461e-8f94-700df80dc2c3",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "qf_all = train_qf(XOLM, itertools.islice(sample_generator(XOLM), 2000))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "b61a002b-b316-4f98-8d42-6bd1e0cd47bb",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "Time for the final test: how well does it perform against the random agent overall?"
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "3abb4aa5-016b-4c1f-9cc0-0b809f8a72b6",
    "deletable": false
   },
   "cell_type": "code",
   "source": [
    "def random_vs_q_generator():\n",
    "    for sentence in sample_generator(XOLM):\n",
    "        reward_random = TypistState(lm=XOLM, string=sentence, start_index=0).evaluate_agent(\n",
    "            agentclass=RandomLengthAgent,\n",
    "            lm=XOLM\n",
    "        )\n",
    "        reward_qf = TypistState(lm=XOLM, string=sentence, start_index=0).evaluate_agent(\n",
    "            agentclass=QFunctionExpectedReturnAgent,\n",
    "            lm=None,\n",
    "            q_function=qf_all,\n",
    "            exploration_policy=lambda t: torch.argmax(t)\n",
    "        )\n",
    "        if abs(reward_qf - reward_random) > 4:\n",
    "            print(\"Remarkable difference: on\", sentence,\n",
    "                  \"we have random:\", reward_random, \"vs. Q-functions:\", reward_qf)\n",
    "        yield (reward_random, reward_qf)\n",
    "\n",
    "%matplotlib inline\n",
    "plot_agent_difference(random_vs_q_generator(), \"random\", \"q function\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "g.cell_uuid": "f92768c5-b9d2-460d-a5de-1c4210fcbe00",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "You should hopefully see some decent numbers: the agent outperforms the random baseline by about the same amount as the value function agent did (which is also around the same amount as the greedy agent with nice rewards outperformed the random agent). But this time, we were able to reach that score without any model at all!\n",
    "\n",
    "Hopefully, you can see why Q-learning would be much more general than the previous approaches. Consider a task like teaching a robot to wade through a river. In order to get a \"perfect\" model of the world, we would need to solve an incredibly complicated system involving the fluid dynamics of the river, the torque of the joints, the varying effects of pressure at different depths, the chance the robot gets hit by a passing fish, and a million more things. The chance that we could perfectly model this scenario are slim. But if we use model-free Q-learning, we can sidestep all of those thorny issues and just learn a policy directly!\n",
    "\n",
    "Of course, Q-learning is not without its own issues. It is very sample inefficient, and deep Q-learning specifically is very unstable and difficult to get working. (Hence the 2 releases of this homework.) So, there is still a lot of research that needs to be done before we can actually learn practical robot gaits via deep Q-learning. But since it is so general, the method has a ton of potential! "
   ]
  },
  {
   "metadata": {
    "g.cell_uuid": "0326b145-2ffc-4487-9137-9b3a7e4a9877",
    "deletable": false
   },
   "cell_type": "markdown",
   "source": [
    "# Congrats! You've won the game.\n",
    "\n",
    "That's it for this assignment! If you want, play around with the reinforcement agent a bit more: try other policies and see how this has a bigger effect on $V$ than on $Q$, try to use more complicated networks, try to speed things up by batching... But probably, if you are to attempt to use RL \"in the wild\", you will -- like with most things in this class -- choose to write more task-specific and optimized code, but hopefully you have gotten a little glimpse of how reinforcement learning isn't all about robots moving -- but can be instructive to think about in other settings, too!"
   ]
  }
 ]
}